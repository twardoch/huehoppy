# Context for LLM

## https://github.gg/pengbo-learn/python-color-transfer (7400 tokens)

Token Usage:
GitHub Tokens: 7423
LLM Input Tokens: 0
LLM Output Tokens: 0
Total Tokens: 7423

FileTree:
.gitignore
.github/workflows/poetry.yml
README.md
demo.py
python_color_transfer/color_transfer.py
pyproject.toml
requirements.txt
python_color_transfer/utils.py

Analysis:
.gitignore

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
.idea/

# Poetry
poetry.lock

# Other
.DS_Store
*.log
logs/
.*.swp
env*/

.github/workflows/poetry.yml

name: CI
on:
  push:
  pull_request:

jobs:
  ci:
    strategy:
      fail-fast: false
      matrix:
        python-version: [3.6, 3.7, 3.8, 3.9]
        poetry-version: [1.1.13]
        os: [ubuntu-latest, macos-latest, windows-latest]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      - name: Run image
        uses: abatilo/actions-poetry@v2.0.0
        with:
          poetry-version: ${{ matrix.poetry-version }}
      - name: Build module
        run: poetry build

      - name: 'Upload Artifact'
        uses: actions/upload-artifact@v3
        with:
          name: builds
          path: |
            dist/
          retention-days: 90

README.md

Color Transfer in Python

Three methods of color transfer implemented in Python.

Output Examples
Input image	Reference image	Mean std transfer	Lab mean transfer	Pdf transfer + Regrain

Methods

Let input image be $I$, reference image be $R$ and output image be $O$.
Let $f{I}(r, g, b)$, $f{R}(r, g, b)$ be probability density functions of $I$ and $R$'s rgb values.

Mean std transfer

$$O = (I - mean(I)) / std(I) * std(R) + mean(R).$$

Lab mean transfer[^1]

$$I' = rgb2lab(I)$$
$$R' = rgb2lab(R)$$
$$O' = (I' - mean(I')) / std(I') * std(R') + mean(R')$$
$$O = lab2rgb(O')$$

Pdf transfer[^2]

$O = t(I)$, where $t: R^3\rightarrow R^3$ is a continous mapping so that $f{t(I)}(r, g, b) = f{R}(r, g, b)$.

Requirements
ðŸ python>=3.6
Installation
From PyPi
pip install python-color-transfer

From source
git clone https://github.com/pengbo-learn/python-color-transfer.git
cd python-color-transfer

pip install -r requirements.txt

Demo
To replicate the results in Output Examples, run:
python demo.py 

Output
Usage
from pathlib import Path

import cv2
from python_color_transfer.color_transfer import ColorTransfer

# Using demo images
input_image = 'demo_images/house.jpeg'
ref_image = 'demo_images/hats.png'

# input image and reference image
img_arr_in = cv2.imread(input_image)
img_arr_ref = cv2.imread(ref_image)

# Initialize the class
PT = ColorTransfer()

# Pdf transfer
img_arr_pdf_reg = PT.pdf_transfer(img_arr_in=img_arr_in,
                                  img_arr_ref=img_arr_ref,
                                  regrain=True)
# Mean std transfer
img_arr_mt = PT.mean_std_transfer(img_arr_in=img_arr_in,
                                  img_arr_ref=img_arr_ref)
# Lab mean transfer
img_arr_lt = PT.lab_transfer(img_arr_in=img_arr_in, img_arr_ref=img_arr_ref)

# Save the example results
img_name = Path(input_image).stem
for method, img in [('pdf-reg', img_arr_pdf_reg), ('mt', img_arr_mt),
                   ('lt', img_arr_lt)]:
    cv2.imwrite(f'{img_name}_{method}.jpg', img)

[^1]: Lab mean transfer: Color Transfer between Images by Erik Reinhard, Michael Ashikhmin, Bruce Gooch and Peter Shirley.
Open source's python implementation

[^2]: Pdf transfer: Automated colour grading using colour distribution transfer by F. Pitie , A. Kokaram and R. Dahyot.
Author's matlab implementation

demo.py

# -*- coding: utf-8 -*-

import os
import time

import cv2
import numpy as np

from python_color_transfer.color_transfer import ColorTransfer

def demo():
    cur_dir = os.path.abspath(os.path.dirname(__file__))
    img_folder = os.path.join(cur_dir, "imgs")
    img_names = [
        "scotland_house.png",
        "house.jpeg",
        "fallingwater.png",
        "tower.jpeg",
    ]
    ref_names = [
        "scotland_plain.png",
        "hats.png",
        "autumn.jpg",
        "sunset.jpg",
    ]
    out_names = [
        "scotland_display.png",
        "house_display.png",
        "fallingwater_display.png",
        "tower_display.png",
    ]
    img_paths = [os.path.join(img_folder, x) for x in img_names]
    ref_paths = [os.path.join(img_folder, x) for x in ref_names]
    out_paths = [os.path.join(img_folder, x) for x in out_names]

    # cls init
    PT = ColorTransfer()

    for img_path, ref_path, out_path in zip(img_paths, ref_paths, out_paths):
        # read input img
        img_arr_in = cv2.imread(img_path)
        [h, w, c] = img_arr_in.shape
        print(f"{img_path}: {h}x{w}x{c}")
        # read reference img
        img_arr_ref = cv2.imread(ref_path)
        [h, w, c] = img_arr_ref.shape
        print(f"{ref_path}: {h}x{w}x{c}")
        # pdf transfer
        t0 = time.time()
        img_arr_reg = PT.pdf_transfer(img_arr_in=img_arr_in,
                                      img_arr_ref=img_arr_ref,
                                      regrain=True)
        print(f"Pdf transfer time: {time.time() - t0:.2f}s")
        # mean transfer
        t0 = time.time()
        img_arr_mt = PT.mean_std_transfer(img_arr_in=img_arr_in,
                                          img_arr_ref=img_arr_ref)
        print(f"Mean std transfer time: {time.time() - t0:.2f}s")
        # lab transfer
        t0 = time.time()
        img_arr_lt = PT.lab_transfer(img_arr_in=img_arr_in,
                                     img_arr_ref=img_arr_ref)
        print(f"Lab mean std transfer time: {time.time() - t0:.2f}s")
        # display
        img_arr_out = np.concatenate(
            (img_arr_in, img_arr_ref, img_arr_mt, img_arr_lt, img_arr_reg),
            axis=1)
        cv2.imwrite(out_path, img_arr_out)
        print(f"Saved to {out_path}\n")

if __name__ == "__main__":
    demo()

python_color_transfer/color_transfer.py

# -*- coding: utf-8 -*-
""" Implementation of color transfer in python.

Papers: 
    Color Transfer between Images. (2001)
    Automated colour grading using colour distribution transfer. (2007) 
Referenced Implementations:
    https://github.com/chia56028/Color-Transfer-between-Images
    https://github.com/frcs/colour-transfer
"""

import cv2
import numpy as np
from python_color_transfer.utils import Rotations

class ColorTransfer:
    """ Methods for color transfer of images. """

    def __init__(self, eps=1e-6, m=6, c=3):
        """Hyper parameters.

        Attributes:
            c: dim of rotation matrix, 3 for oridnary img.
            m: num of random orthogonal rotation matrices.
            eps: prevents from zero dividing.
        """
        self.eps = eps
        if c == 3:
            self.rotation_matrices = Rotations.optimal_rotations()
        else:
            self.rotation_matrices = Rotations.random_rotations(m, c=c)
        self.RG = Regrain()

    def lab_transfer(self, img_arr_in=None, img_arr_ref=None):
        """Convert img from rgb space to lab space, apply mean std transfer,
        then convert back.
        Args:
            img_arr_in: bgr numpy array of input image.
            img_arr_ref: bgr numpy array of reference image.
        Returns:
            img_arr_out: transfered bgr numpy array of input image.
        """
        lab_in = cv2.cvtColor(img_arr_in, cv2.COLOR_BGR2LAB)
        lab_ref = cv2.cvtColor(img_arr_ref, cv2.COLOR_BGR2LAB)
        lab_out = self.mean_std_transfer(img_arr_in=lab_in,
                                         img_arr_ref=lab_ref)
        img_arr_out = cv2.cvtColor(lab_out, cv2.COLOR_LAB2BGR)
        return img_arr_out

    def mean_std_transfer(self, img_arr_in=None, img_arr_ref=None):
        """Adapt img_arr_in's (mean, std) to img_arr_ref's (mean, std).

        img_o = (img_i - mean(img_i)) / std(img_i) * std(img_r) + mean(img_r).
        Args:
            img_arr_in: bgr numpy array of input image.
            img_arr_ref: bgr numpy array of reference image.
        Returns:
            img_arr_out: transfered bgr numpy array of input image.
        """
        mean_in = np.mean(img_arr_in, axis=(0, 1), keepdims=True)
        mean_ref = np.mean(img_arr_ref, axis=(0, 1), keepdims=True)
        std_in = np.std(img_arr_in, axis=(0, 1), keepdims=True)
        std_ref = np.std(img_arr_ref, axis=(0, 1), keepdims=True)
        img_arr_out = (img_arr_in - mean_in) / std_in * std_ref + mean_ref
        img_arr_out[img_arr_out < 0] = 0
        img_arr_out[img_arr_out > 255] = 255
        return img_arr_out.astype("uint8")

    def pdf_transfer(self, img_arr_in=None, img_arr_ref=None, regrain=False):
        """Apply probability density function transfer.

        img_o = t(img_i) so that f_{t(img_i)}(r, g, b) = f_{img_r}(r, g, b),
        where f_{img}(r, g, b) is the probability density function of img's rgb values.
        Args:
            img_arr_in: bgr numpy array of input image.
            img_arr_ref: bgr numpy array of reference image.
        Returns:
            img_arr_out: transfered bgr numpy array of input image.
        """

        # reshape (h, w, c) to (c, h*w)
        [h, w, c] = img_arr_in.shape
        reshape_arr_in = img_arr_in.reshape(-1, c).transpose() / 255.0
        reshape_arr_ref = img_arr_ref.reshape(-1, c).transpose() / 255.0
        # pdf transfer
        reshape_arr_out = self.pdf_transfer_nd(arr_in=reshape_arr_in,
                                               arr_ref=reshape_arr_ref)
        # reshape (c, h*w) to (h, w, c)
        reshape_arr_out[reshape_arr_out < 0] = 0
        reshape_arr_out[reshape_arr_out > 1] = 1
        reshape_arr_out = (255.0 * reshape_arr_out).astype("uint8")
        img_arr_out = reshape_arr_out.transpose().reshape(h, w, c)
        if regrain:
            img_arr_out = self.RG.regrain(img_arr_in=img_arr_in,
                                          img_arr_col=img_arr_out)
        return img_arr_out

    def pdf_transfer_nd(self, arr_in=None, arr_ref=None, step_size=1):
        """Apply n-dim probability density function transfer.

        Args:
            arr_in: shape=(n, x).
            arr_ref: shape=(n, x).
            step_size: arr = arr + step_size * delta_arr.
        Returns:
            arr_out: shape=(n, x).
        """
        # n times of 1d-pdf-transfer
        arr_out = np.array(arr_in)
        for rotation_matrix in self.rotation_matrices:
            rot_arr_in = np.matmul(rotation_matrix, arr_out)
            rot_arr_ref = np.matmul(rotation_matrix, arr_ref)
            rot_arr_out = np.zeros(rot_arr_in.shape)
            for i in range(rot_arr_out.shape[0]):
                rot_arr_out[i] = self._pdf_transfer_1d(rot_arr_in[i],
                                                       rot_arr_ref[i])
            # func = lambda x, n : self._pdf_transfer_1d(x[:n], x[n:])
            # rot_arr = np.concatenate((rot_arr_in, rot_arr_ref), axis=1)
            # rot_arr_out = np.apply_along_axis(func, 1, rot_arr, rot_arr_in.shape[1])
            rot_delta_arr = rot_arr_out - rot_arr_in
            delta_arr = np.matmul(
                rotation_matrix.transpose(), rot_delta_arr
            )  # np.linalg.solve(rotation_matrix, rot_delta_arr)
            arr_out = step_size * delta_arr + arr_out
        return arr_out

    def _pdf_transfer_1d(self, arr_in=None, arr_ref=None, n=300):
        """Apply 1-dim probability density function transfer.

        Args:
            arr_in: 1d numpy input array.
            arr_ref: 1d numpy reference array.
            n: discretization num of distribution of image's pixels.
        Returns:
            arr_out: transfered input array.
        """

        arr = np.concatenate((arr_in, arr_ref))
        # discretization as histogram
        min_v = arr.min() - self.eps
        max_v = arr.max() + self.eps
        xs = np.array(
            [min_v + (max_v - min_v) * i / n for i in range(n + 1)])
        hist_in, _ = np.histogram(arr_in, xs)
        hist_ref, _ = np.histogram(arr_ref, xs)
        xs = xs[:-1]
        # compute probability distribution
        cum_in = np.cumsum(hist_in)
        cum_ref = np.cumsum(hist_ref)
        d_in = cum_in / cum_in[-1]
        d_ref = cum_ref / cum_ref[-1]
        # transfer
        t_d_in = np.interp(d_in, d_ref, xs)
        t_d_in[d_in <= d_ref[0]] = min_v
        t_d_in[d_in >= d_ref[-1]] = max_v
        arr_out = np.interp(arr_in, xs, t_d_in)
        return arr_out

class Regrain:

    def __init__(self, smoothness=1):
        """To understand the meaning of these params, refer to paper07."""
        self.nbits = [4, 16, 32, 64, 64, 64]
        self.smoothness = smoothness
        self.level = 0

    def regrain(self, img_arr_in=None, img_arr_col=None):
        """keep gradient of img_arr_in and color of img_arr_col. """

        img_arr_in = img_arr_in / 255.0
        img_arr_col = img_arr_col / 255.0
        img_arr_out = np.array(img_arr_in)
        img_arr_out = self.regrain_rec(img_arr_out, img_arr_in, img_arr_col,
                                       self.nbits, self.level)
        img_arr_out[img_arr_out < 0] = 0
        img_arr_out[img_arr_out > 1] = 1
        img_arr_out = (255.0 * img_arr_out).astype("uint8")
        return img_arr_out

    def regrain_rec(self, img_arr_out, img_arr_in, img_arr_col, nbits, level):
        """direct translation of matlab code. """

        [h, w, _] = img_arr_in.shape
        h2 = (h + 1) // 2
        w2 = (w + 1) // 2
        if len(nbits) > 1 and h2 > 20 and w2 > 20:
            resize_arr_in = cv2.resize(img_arr_in, (w2, h2),
                                       interpolation=cv2.INTER_LINEAR)
            resize_arr_col = cv2.resize(img_arr_col, (w2, h2),
                                        interpolation=cv2.INTER_LINEAR)
            resize_arr_out = cv2.resize(img_arr_out, (w2, h2),
                                        interpolation=cv2.INTER_LINEAR)
            resize_arr_out = self.regrain_rec(resize_arr_out, resize_arr_in,
                                              resize_arr_col, nbits[1:],
                                              level + 1)
            img_arr_out = cv2.resize(resize_arr_out, (w, h),
                                     interpolation=cv2.INTER_LINEAR)
        img_arr_out = self.solve(img_arr_out, img_arr_in, img_arr_col,
                                 nbits[0], level)
        return img_arr_out

    def solve(self,
              img_arr_out,
              img_arr_in,
              img_arr_col,
              nbit,
              level,
              eps=1e-6):
        """direct translation of matlab code. """

        [width, height, c] = img_arr_in.shape
        first_pad_0 = lambda arr: np.concatenate(
            (arr[:1, :], arr[:-1, :]), axis=0)
        first_pad_1 = lambda arr: np.concatenate(
            (arr[:, :1], arr[:, :-1]), axis=1)
        last_pad_0 = lambda arr: np.concatenate(
            (arr[1:, :], arr[-1:, :]), axis=0)
        last_pad_1 = lambda arr: np.concatenate(
            (arr[:, 1:], arr[:, -1:]), axis=1)

        delta_x = last_pad_1(img_arr_in) - first_pad_1(img_arr_in)
        delta_y = last_pad_0(img_arr_in) - first_pad_0(img_arr_in)
        delta = np.sqrt((delta_x**2 + delta_y**2).sum(axis=2, keepdims=True))

        psi = 256 * delta / 5
        psi[psi > 1] = 1
        phi = 30 * 2**(-level) / (1 + 10 * delta / self.smoothness)

        phi1 = (last_pad_1(phi) + phi) / 2
        phi2 = (last_pad_0(phi) + phi) / 2
        phi3 = (first_pad_1(phi) + phi) / 2
        phi4 = (first_pad_0(phi) + phi) / 2

        rho = 1 / 5.0
        for i in range(nbit):
            den = psi + phi1 + phi2 + phi3 + phi4
            num = (
                np.tile(psi, [1, 1, c]) * img_arr_col +
                np.tile(phi1, [1, 1, c]) *
                (last_pad_1(img_arr_out) - last_pad_1(img_arr_in) + img_arr_in)
                + np.tile(phi2, [1, 1, c]) *
                (last_pad_0(img_arr_out) - last_pad_0(img_arr_in) + img_arr_in)
                + np.tile(phi3, [1, 1, c]) *
                (first_pad_1(img_arr_out) - first_pad_1(img_arr_in) +
                 img_arr_in) + np.tile(phi4, [1, 1, c]) *
                (first_pad_0(img_arr_out) - first_pad_0(img_arr_in) +
                 img_arr_in))
            img_arr_out = (num / np.tile(den + eps, [1, 1, c]) * (1 - rho) +
                           rho * img_arr_out)
        return img_arr_out

pyproject.toml

[tool.poetry]
name = "python_color_transfer"
version = "0.1.1"
description = "Three methods of color transfer implemented in python."
authors = ["pengbo-learn"]
license = "MIT"
readme = "README.md"
include=['imgs']

[tool.poetry.dependencies]
python = "^3.6"
opencv-python = "^4.2"
numpy = "^1.19"

[tool.poetry.dev-dependencies]

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

requirements.txt

opencv-python>=4.2
numpy>=1.19

python_color_transfer/utils.py

# -*- coding: utf-8 -*-

import numpy as np

def rvs(dim=3):
    """generate orthogonal matrices with dimension=dim.

    This is the rvs method pulled from the https://github.com/scipy/scipy/pull/5622/files,
    with minimal change - just enough to run as a stand alone numpy function.
    """
    random_state = np.random
    H = np.eye(dim)
    D = np.ones((dim, ))
    for n in range(1, dim):
        x = random_state.normal(size=(dim - n + 1, ))
        D[n - 1] = np.sign(x[0])
        x[0] -= D[n - 1] * np.sqrt((x * x).sum())
        # Householder transformation
        Hx = np.eye(dim - n + 1) - 2.0 * np.outer(x, x) / (x * x).sum()
        mat = np.eye(dim)
        mat[n - 1:, n - 1:] = Hx
        H = np.dot(H, mat)
        # Fix the last sign such that the determinant is 1
    D[-1] = (-1)**(1 - (dim % 2)) * D.prod()
    # Equivalent to np.dot(np.diag(D), H) but faster, apparently
    H = (D * H.T).T
    return H

class Rotations:
    """ generate orthogonal matrices for pdf transfer."""

    @classmethod
    def random_rotations(cls, m, c=3):
        """ Random rotation. """

        assert m > 0
        rotation_matrices = [np.eye(c)]
        rotation_matrices.extend([
            np.matmul(rotation_matrices[0], rvs(dim=c)) for _ in range(m - 1)
        ])
        return rotation_matrices

    @classmethod
    def optimal_rotations(cls):
        """Optimal rotation.

        Copy from Automated colour grading using colour distribution transfer.
        F. PitiÃ© , A. Kokaram and R. Dahyot (2007) Journal of Computer Vision and Image Understanding.
        """

        rotation_matrices = [
            [
                [1.000000, 0.000000, 0.000000],
                [0.000000, 1.000000, 0.000000],
                [0.000000, 0.000000, 1.000000],
            ],
            [
                [0.333333, 0.666667, 0.666667],
                [0.666667, 0.333333, -0.666667],
                [-0.666667, 0.666667, -0.333333],
            ],
            [
                [0.577350, 0.211297, 0.788682],
                [-0.577350, 0.788668, 0.211352],
                [0.577350, 0.577370, -0.577330],
            ],
            [
                [0.577350, 0.408273, 0.707092],
                [-0.577350, -0.408224, 0.707121],
                [0.577350, -0.816497, 0.000029],
            ],
            [
                [0.332572, 0.910758, 0.244778],
                [-0.910887, 0.242977, 0.333536],
                [-0.244295, 0.333890, -0.910405],
            ],
            [
                [0.243799, 0.910726, 0.333376],
                [0.910699, -0.333174, 0.244177],
                [-0.333450, -0.244075, 0.910625],
            ],
            # [[-0.109199, 0.810241, 0.575834], [0.645399, 0.498377, -0.578862], [0.756000, -0.308432, 0.577351]],
            # [[0.759262, 0.649435, -0.041906], [0.143443, -0.104197, 0.984158], [0.634780, -0.753245, -0.172269]],
            # [[0.862298, 0.503331, -0.055679], [-0.490221, 0.802113, -0.341026], [-0.126988, 0.321361, 0.938404]],
            # [[0.982488, 0.149181, 0.111631], [0.186103, -0.756525, -0.626926], [-0.009074, 0.636722, -0.771040]],
            # [[0.687077, -0.577557, -0.440855], [0.592440, 0.796586, -0.120272], [-0.420643, 0.178544, -0.889484]],
            # [[0.463791, 0.822404, 0.329470], [0.030607, -0.386537, 0.921766], [-0.885416, 0.417422, 0.204444]],
        ]
        rotation_matrices = [np.array(x) for x in rotation_matrices]
        # for x in rotation_matrices:
        #    print(np.matmul(x.transpose(), x))
        #    import pdb
        #    pdb.set_trace()
        return rotation_matrices

## https://github.gg/dstein64/colortrans (4600 tokens)

Token Usage:
GitHub Tokens: 4538
LLM Input Tokens: 0
LLM Output Tokens: 0
Total Tokens: 4538

FileTree:
setup.py
colortrans/main.py
README.md
.gitignore
.github/workflows/packages.yml
colortrans/colortrans.py
.github/workflows/build.yml
colortrans/init.py
colortrans/version.txt
tests/test_colortrans.py

Analysis:
setup.py

import os
from setuptools import setup

version_txt = os.path.join(os.path.dirname(__file__), 'colortrans', 'version.txt')
with open(version_txt, 'r') as f:
    version = f.read().strip()

with open('README.md') as f:
    long_description = f.read()

setup(
    author='Daniel Steinberg',
    author_email='ds@dannyadam.com',
    classifiers=[
        'Development Status :: 4 - Beta',
        'Intended Audience :: Developers',
        'Intended Audience :: Science/Research',
        'Topic :: Scientific/Engineering',
        'Topic :: Artistic Software',
        'License :: OSI Approved :: MIT License',
        'Operating System :: Unix',
        'Operating System :: POSIX :: Linux',
        'Operating System :: MacOS',
        'Operating System :: Microsoft :: Windows',
        'Programming Language :: Python :: 3',
    ],
    description='An implementation of various color transfer algorithms',
    entry_points={
        'console_scripts': ['colortrans=colortrans.colortrans:main'],
    },
    keywords=['color', 'color-transfer'],
    license='MIT',
    long_description=long_description,
    long_description_content_type='text/markdown',
    name='colortrans',
    package_data={'colortrans': ['version.txt']},
    packages=['colortrans'],
    python_requires='>=3.6',
    install_requires=['numpy', 'pillow'],
    url='https://github.com/dstein64/colortrans',
    version=version,
)

colortrans/main.py

import sys

from colortrans.colortrans import main

if __name__ == "__main__":
    sys.exit(main())

README.md

colortrans

An implementation of various algorithms for transferring the colors from a reference image to a
content image while preserving the qualitative appearance of the content image (i.e., color
transfer).

Installation
Requirements
Python 3.8 or greater (earlier versions may work, but are not tested)
Install
$ pip3 install colortrans

Update
$ pip3 install --upgrade colortrans

Command-Line Usage

The program can be used from the command line.

The general command line usage is shown below.

$ colortrans [--method METHOD] CONTENT REFERENCE OUTPUT

CONTENT is the path to the content image, REFERENCE is the path to the style image, and OUTPUT
is the path to save the output image.

METHOD specifies the color transfer algorithm. The following methods are supported:

lhm Linear Histogram Matching [1] (default)
pccm Principal Components Color Matching [2, 3]
reinhard Reinhard et al. [4]

If the launcher script was not installed within a directory on your PATH, colortrans can be launched by
passing its module name to Python.

$ python3 -m colortrans [--method METHOD] CONTENT REFERENCE OUTPUT

Library Usage

The algorithms can also be used directly from Python programs. Each of the methods listed above has
a corresponding function, transfer_METHOD, taking two NumPy arrays corresponding to the content
and reference image, respectively. The arrays have HxWxC data ordering (channels-last).

Example
import colortrans
import numpy as np
from PIL import Image

# Load data
with Image.open('/path/to/content.jpg') as img:
    content = np.array(img.convert('RGB'))
with Image.open('/path/to/reference.jpg') as img:
    reference = np.array(img.convert('RGB'))

# Transfer colors using different algorithms
output_lhm = colortrans.transfer_lhm(content, reference)
output_pccm = colortrans.transfer_pccm(content, reference)
output_reinhard = colortrans.transfer_reinhard(content, reference)

# Save outputs
Image.fromarray(output_lhm).save('/path/to/output_lhm.jpg')
Image.fromarray(output_pccm).save('/path/to/output_pccm.jpg')
Image.fromarray(output_reinhard).save('/path/to/output_reinhard.jpg')

References

[1] Hertzmann, Aaron. "Algorithms for Rendering in Artistic Styles." Ph.D., New York University,
2001.

[2] Kotera, Hiroaki, Hung-Shing Chen, and Tetsuro Morimoto. "Object-to-Object Color Mapping by Image
Segmentation." In Color Imaging: Device-Independent Color, Color Hardcopy, and Graphic Arts IV,
3648:148â€“57. SPIE, 1998.

[3] Kotera, Hiroaki. "A Scene-Referred Color Transfer for Pleasant Imaging on Display." In IEEE
International Conference on Image Processing 2005, 2:IIâ€“5, 2005.

[4] Reinhard, Erik, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. "Color Transfer between
Images." IEEE Computer Graphics and Applications 21, no. 5 (July 2001): 34â€“41.

.gitignore

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

.github/workflows/packages.yml

name: packages
# When the 'permissions' key is specified, unspecified permission scopes (e.g.,
# actions, checks, etc.) are set to no access (none).
permissions:
  contents: read
on:
  workflow_dispatch:
    inputs:
      # When git-ref is empty, HEAD will be checked out.
      git-ref:
        description: Optional git ref (branch, tag, or full SHA)
        required: false

jobs:
  packages:
    runs-on: ubuntu-latest

    steps:
    - name: Clone
      uses: actions/checkout@v4
      with:
        # When the ref is empty, HEAD will be checked out.
        ref: ${{ github.event.inputs.git-ref }}

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    - name: Dependencies
      run: python -m pip install --upgrade pip setuptools wheel

    - name: Build
      run: |
        python setup.py sdist
        python setup.py bdist_wheel

    - name: Upload
      uses: actions/upload-artifact@v4
      with:
        name: packages
        path: ./dist

colortrans/colortrans.py

import argparse
import sys

import numpy as np
import os
from PIL import Image

METHODS = ('lhm', 'pccm', 'reinhard')

version_txt = os.path.join(os.path.dirname(__file__), 'version.txt')
with open(version_txt, 'r') as f:
    __version__ = f.read().strip()

def transfer_lhm(content, reference):
    """Transfers colors from a reference image to a content image using the
    Linear Histogram Matching.

    content: NumPy array (HxWxC)
    reference: NumPy array (HxWxC)
    """
    # Convert HxWxC image to a (H*W)xC matrix.
    shape = content.shape
    assert len(shape) == 3
    content = content.reshape(-1, shape[-1]).astype(np.float32)
    reference = reference.reshape(-1, shape[-1]).astype(np.float32)

    def matrix_sqrt(X):
        eig_val, eig_vec = np.linalg.eig(X)
        return eig_vec.dot(np.diag(np.sqrt(eig_val))).dot(eig_vec.T)

    mu_content = np.mean(content, axis=0)
    mu_reference = np.mean(reference, axis=0)

    cov_content = np.cov(content, rowvar=False)
    cov_reference = np.cov(reference, rowvar=False)

    result = matrix_sqrt(cov_reference)
    result = result.dot(np.linalg.inv(matrix_sqrt(cov_content)))
    result = result.dot((content - mu_content).T).T
    result = result + mu_reference
    # Restore image dimensions.
    result = result.reshape(shape).clip(0, 255).round().astype(np.uint8)

    return result

def transfer_pccm(content, reference):
    """Transfers colors from a reference image to a content image using
    Principal Component Color Matching.

    content: NumPy array (HxWxC)
    reference: NumPy array (HxWxC)
    """
    # Convert HxWxC image to a (H*W)xC matrix.
    shape = content.shape
    assert len(shape) == 3
    content = content.reshape(-1, shape[-1]).astype(np.float32)
    reference = reference.reshape(-1, shape[-1]).astype(np.float32)

    mu_content = np.mean(content, axis=0)
    mu_reference = np.mean(reference, axis=0)

    cov_content = np.cov(content, rowvar=False)
    cov_reference = np.cov(reference, rowvar=False)

    eigval_content, eigvec_content = np.linalg.eig(cov_content)
    eigval_reference, eigvec_reference = np.linalg.eig(cov_reference)

    scaling = np.diag(np.sqrt(eigval_reference / eigval_content))
    transform = eigvec_reference.dot(scaling).dot(eigvec_content.T)
    result = (content - mu_content).dot(transform.T) + mu_reference
    # Restore image dimensions.
    result = result.reshape(shape).clip(0, 255).round().astype(np.uint8)

    return result

def transfer_reinhard(content, reference):
    """Transfers colors from a reference image to a content image using the
    technique from Reinhard et al.

    content: NumPy array (HxWxC)
    reference: NumPy array (HxWxC)
    """
    # Convert HxWxC image to a (H*W)xC matrix.
    shape = content.shape
    assert len(shape) == 3
    content = content.reshape(-1, shape[-1]).astype(np.float32)
    reference = reference.reshape(-1, shape[-1]).astype(np.float32)

    m1 = np.array([
        [0.3811, 0.1967, 0.0241],
        [0.5783, 0.7244, 0.1288],
        [0.0402, 0.0782, 0.8444],
    ])

    m2 = np.array([
        [0.5774, 0.4082, 0.7071],
        [0.5774, 0.4082, -0.7071],
        [0.5774, -0.8165, 0.0000],
    ])

    m3 = np.array([
        [0.5774, 0.5774, 0.5774],
        [0.4082, 0.4082, -0.8165],
        [0.7071, -0.7071, 0.0000],
    ])

    m4 = np.array([
        [4.4679, -1.2186, 0.0497],
        [-3.5873, 2.3809, -0.2439],
        [0.1193, -0.1624, 1.2045],
    ])

    # Avoid log of 0. Clipping is used instead of adding epsilon, to avoid
    # taking a log of a small number whose very low output distorts the results.
    # WARN: This differs from the Reinhard paper, where no adjustment is made.
    lab_content = np.log10(np.maximum(1.0, content.dot(m1))).dot(m2)
    lab_reference = np.log10(np.maximum(1.0, reference.dot(m1))).dot(m2)

    mu_content = lab_content.mean(axis=0)
    mu_reference = lab_reference.mean(axis=0)

    std_source = np.std(content, axis=0)
    std_target = np.std(reference, axis=0)

    result = lab_content - mu_content
    result *= std_target
    result /= std_source
    result += mu_reference
    result = (10 ** result.dot(m3)).dot(m4)
    # Restore image dimensions.
    result = result.reshape(shape).clip(0, 255).round().astype(np.uint8)

    return result

# ************************************************************
# * Command Line Interface
# ************************************************************

def parse_args(argv):
    parser = argparse.ArgumentParser(
        prog='colortrans',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # Optional arguments
    parser.add_argument(
        '--method', default='lhm', choices=METHODS,
        help='Algorithm to use for color transfer.')

    # Required arguments
    parser.add_argument('content', help='Path to content image (qualitative appearance).')
    parser.add_argument('reference', help='Path to reference image (desired colors).')
    parser.add_argument('output', help='Path to output image.')

    args = parser.parse_args(argv[1:])

    return args

def main(argv=sys.argv):
    args = parse_args(argv)
    content_img = Image.open(args.content).convert('RGB')
    # The slicing is to remove transparency channels if they exist.
    content = np.array(content_img)[:, :, :3]
    reference_img = Image.open(args.reference).convert('RGB')
    reference = np.array(reference_img)[:, :, :3]
    transfer = globals()[f'transfer_{args.method}']
    output = transfer(content, reference)
    Image.fromarray(output).save(args.output)

if __name__ == '__main__':
    sys.exit(main(sys.argv))

.github/workflows/build.yml

name: build
# When the 'permissions' key is specified, unspecified permission scopes (e.g.,
# actions, checks, etc.) are set to no access (none).
permissions:
  contents: read
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run weekly (* is a special character in YAML, so quote the string)
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      # When git-ref is empty, HEAD will be checked out.
      git-ref:
        description: Optional git ref (branch, tag, or full SHA)
        required: false

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12', '3.13']

    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        # When the ref is empty, HEAD will be checked out.
        ref: ${{ github.event.inputs.git-ref }}

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Upgrade Pip
      run: python -m pip install --upgrade pip

    - name: Lint
      run: |
        pip install flake8
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Install
      run: |
        pip install .

    - name: Test
      run: |
        colortrans --help
        cd tests  # so package is imported from site-packages instead of working directory
        python -m unittest discover . -v

colortrans/init.py

from colortrans.colortrans import (
    METHODS,
    transfer_lhm,
    transfer_pccm,
    transfer_reinhard,
    __version__
)

colortrans/version.txt

1.0.0

tests/test_colortrans.py

import os
import tempfile
import unittest

import numpy as np
from PIL import Image

import colortrans

np.random.seed(0)

class TestColorTrans(unittest.TestCase):
    """colortrans tests"""
    def test_colortrans(self):
        content = np.random.randint(256, size=(20, 30, 3), dtype=np.uint8)
        reference = np.random.randint(256, size=(40, 50, 3), dtype=np.uint8)

        for method in colortrans.METHODS:
            func = getattr(colortrans, f'transfer_{method}')
            output = func(content, reference)
            self.assertEqual(output.shape, content.shape)
            self.assertEqual(output.dtype, np.uint8)

            with tempfile.TemporaryDirectory() as tmp:
                content_path = os.path.join(tmp, 'content.png')
                reference_path = os.path.join(tmp, 'reference.png')
                output_path = os.path.join(tmp, 'output.png')
                Image.fromarray(content).save(content_path)
                Image.fromarray(reference).save(reference_path)
                argv = ['colortrans', content_path, reference_path, output_path, '--method', method]
                colortrans.colortrans.main(argv)
                with Image.open(output_path) as img:
                    self.assertTrue(np.array_equal(np.array(img), output))
                argv = ['colortrans', content_path, reference_path, output_path]
                colortrans.colortrans.main(argv)
                assert_func = self.assertTrue if method == 'lhm' else self.assertFalse
                with Image.open(output_path) as img:
                    assert_func(np.array_equal(np.array(img), output))

if __name__ == '__main__':
    unittest.main()
    
## https://github.gg/ImmersiveMediaLaboratory/ColorTransferLib (290k tokens)

Token Usage:
GitHub Tokens: 291201
LLM Input Tokens: 0
LLM Output Tokens: 0
Total Tokens: 291201

FileTree:
ColorTransferLib/Algorithms/BCC/init.py
.gitignore
ColorTransferLib/Algorithms/CCS/CCS.py
ColorTransferLib/Algorithms/CAM/color_aware_st.py
ColorTransferLib/Algorithms/CAM/init.py
ColorTransferLib/Algorithms/BCC/FaissKNeighbors.py
ColorTransferLib/Algorithms/BCC/BCC.py
ColorTransferLib/Algorithms/CCS/init.py
ColorTransferLib/Algorithms/DPT/DPT.py
ColorTransferLib/Algorithms/DPT/init.py
ColorTransferLib/Algorithms/DPT/deep_photostyle.py
ColorTransferLib/Algorithms/DPT/photo_style.py
ColorTransferLib/Algorithms/DPT/smooth_local_affine.py
ColorTransferLib/Algorithms/DPT/closed_form_matting.py
ColorTransferLib/Algorithms/CAM/CAM.py
ColorTransferLib/Algorithms/DPT/vgg19/vgg.py
ColorTransferLib/Algorithms/DPT/vgg19/init.py
ColorTransferLib/Algorithms/EB3/init.py
ColorTransferLib/Algorithms/FUZ/init.py
ColorTransferLib/Algorithms/GPC/init.py
ColorTransferLib/Algorithms/GPC/GPC.py
ColorTransferLib/Algorithms/GLO/GLO.py
ColorTransferLib/Algorithms/FUZ/FUZ.py
ColorTransferLib/Algorithms/HIS/HIS.py
ColorTransferLib/Algorithms/HIS/LICENSE.txt
ColorTransferLib/Algorithms/HIS/data/init.py
ColorTransferLib/Algorithms/HIS/data/aligned_dataset_rand_seg_onlymap.py
ColorTransferLib/Algorithms/HIS/data/custom_dataset_data_loader.py
ColorTransferLib/Algorithms/EB3/EB3.py
ColorTransferLib/Algorithms/HIS/data/base_dataset.py
ColorTransferLib/Algorithms/HIS/data/base_data_loader.py
ColorTransferLib/Algorithms/GLO/init.py
ColorTransferLib/Algorithms/HIS/data/data_loader.py
ColorTransferLib/Algorithms/HIS/data/image_folder.py
ColorTransferLib/Algorithms/HIS/data/single_dataset.py
ColorTransferLib/Algorithms/HIS/init.py
ColorTransferLib/Algorithms/HIS/data/unaligned_dataset.py
ColorTransferLib/Algorithms/HIS/models/base_model.py
ColorTransferLib/Algorithms/HIS/models/init.py
ColorTransferLib/Algorithms/HIS/models/colorhistogram_model.py
ColorTransferLib/Algorithms/HIS/models/models.py
ColorTransferLib/Algorithms/HIS/models/modules/architecture.py
ColorTransferLib/Algorithms/HIS/models/modules/iccv_model.py
ColorTransferLib/Algorithms/HIS/models/modules/block.py
ColorTransferLib/Algorithms/HIS/models/modules/sft_arch.py
ColorTransferLib/Algorithms/HIS/util/init.py
ColorTransferLib/Algorithms/HIS/models/networks.py
ColorTransferLib/Algorithms/HIS/util/html.py
ColorTransferLib/Algorithms/HIS/models/modules/stdunet_woIN.py
ColorTransferLib/Algorithms/HIS/util/get_data.py
ColorTransferLib/Algorithms/HIS/util/image_pool.py
ColorTransferLib/Algorithms/HIS/util/visualizer.py
ColorTransferLib/Algorithms/MKL/MKL.py
ColorTransferLib/Algorithms/HIS/util/util.py
ColorTransferLib/Algorithms/MKL/init.py
ColorTransferLib/Algorithms/HIS/models/modules/init.py
ColorTransferLib/Algorithms/NST/LICENSE.txt
ColorTransferLib/Algorithms/NST/NST.py
ColorTransferLib/Algorithms/NST/Model.py
ColorTransferLib/Algorithms/NST/init.py
ColorTransferLib/Algorithms/PSN/PSN.py
ColorTransferLib/Algorithms/PDF/init.py
ColorTransferLib/Algorithms/PSN/tf_ops.py
ColorTransferLib/Algorithms/PSN/LICENSE.txt
ColorTransferLib/Algorithms/PSN/psnet.py
ColorTransferLib/Algorithms/PSN/init.py
ColorTransferLib/Algorithms/PSN/utils.py
ColorTransferLib/Algorithms/RHG/ReHistoGAN/init.py
ColorTransferLib/Algorithms/RHG/LICENSE.md
ColorTransferLib/Algorithms/RHG/RHG.py
ColorTransferLib/Algorithms/PDF/PDF.py
ColorTransferLib/Algorithms/RHG/init.py
ColorTransferLib/Algorithms/RHG/histoGAN/init.py
ColorTransferLib/Algorithms/RHG/ReHistoGAN/rehistoGAN.py
ColorTransferLib/Algorithms/RHG/histoGAN/histoGAN.py
ColorTransferLib/Algorithms/RHG/histogram_classes/LabHistBlock.py
ColorTransferLib/Algorithms/RHG/histogram_classes/init.py
ColorTransferLib/Algorithms/RHG/histogram_classes/RGBuvHistBlock.py
ColorTransferLib/Algorithms/RHG/histoGAN.py
ColorTransferLib/Algorithms/RHG/histogram_classes/rgChromaHistBlock.py
ColorTransferLib/Algorithms/RHG/projection_gaussian.py
ColorTransferLib/Algorithms/RHG/upsampling/bguSlice.m
ColorTransferLib/Algorithms/RHG/projection_to_latent.py
ColorTransferLib/Algorithms/RHG/rehistoGAN.py
ColorTransferLib/Algorithms/RHG/upsampling/buildApplyAffineModelMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/buildDerivXMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/buildAffineSliceMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/buildDerivYMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/buildDerivZMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/buildSecondDerivZMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/buildSliceMatrix.m
ColorTransferLib/Algorithms/RHG/upsampling/columnize.m
ColorTransferLib/Algorithms/RHG/upsampling/demo_temp.m
ColorTransferLib/Algorithms/RHG/upsampling/BGU.m
ColorTransferLib/Algorithms/RHG/upsampling/getDefaultAffineGridSize.m
ColorTransferLib/Algorithms/RHG/upsampling/heightWidth.m
ColorTransferLib/Algorithms/RHG/upsampling/maxn.m
ColorTransferLib/Algorithms/RHG/upsampling/mse.m
ColorTransferLib/Algorithms/RHG/upsampling/rgb2luminance.m
ColorTransferLib/Algorithms/RHG/upsampling/testBGU.m
ColorTransferLib/Algorithms/RHG/upsampling/showTestResults.m
ColorTransferLib/Algorithms/RHG/upsampling/runOnFilenames.m
ColorTransferLib/Algorithms/RHG/upsampling/bguFit.m
ColorTransferLib/Algorithms/RHG/upsampling/testBGU_modified.m
ColorTransferLib/Algorithms/RHG/utils/diff_augment.py
ColorTransferLib/Algorithms/RHG/utils/color_transfer_MKL.py
ColorTransferLib/Algorithms/RHG/utils/init.py
ColorTransferLib/Algorithms/RHG/utils/face_preprocessing.py
ColorTransferLib/Algorithms/RHG/utils/imresize.py
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/CJianCost.c
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/GaussTransform.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/memory_layout_note.h
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/GaussTransformCorr.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/mex_GaussTransform.c
ColorTransferLib/Algorithms/RHG/utils/pyramid_upsampling.py
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/mex_CJianCost.c
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/mg_initialiseMexFilesGT.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/Recolour/mg_recolourTarget.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/auxilliary/set_bounds.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/Recolour/mg_recolourTargetM.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/auxilliary/determine_border.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/auxilliary/set_ctrl_pts.m
ColorTransferLib/Algorithms/RHG/utils/vggloss.py
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/clustering/mg_applyKMeans.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/GaussTransform.c
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/gmmreg_L2_corr.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/clustering/mg_quantImage.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/gmmreg_rbf_L2.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/mg_initialize_config_corr.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/compute_rbf_kernel.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/compute_kernel.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_costfunc_corr.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_costfunc.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_tps_costfunc.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/quaternion2rotation.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_affine2d.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_rbf_costfunc.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_affine3d.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_tps_costfunc_corr.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/mg_initialize_config.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/tps_compute_kernel.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_rigid2d.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_rigid3d.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_tps.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_pointset.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mex_mgRecolourParallel_1.cpp
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mex_mgRecolourParallel_Mask.cpp
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mex_mgRecolourParallelTPS.cpp
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mgRecolourPixels.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mgRecolourPixels_Mask.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mgRecolourPixelsRBF.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mg_initialiseMexFilesOMP.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/README.txt
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/candelete.txt
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/ctfunction.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/ct_apply.m
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/ctfunction_corr.m
ColorTransferLib/Algorithms/TPS/init.py
ColorTransferLib/Algorithms/TPS/TPS.py
ColorTransferLib/ColorTransfer.py
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mg_transform_tps_parallel.m
ColorTransferLib/Algorithms/init.py
ColorTransferLib/Evaluation/BA/BA.py
ColorTransferLib/Evaluation/BA/init.py
ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/demo_test.m
ColorTransferLib/Evaluation/BRISQUE/init.py
ColorTransferLib/Evaluation/CF/CF.py
ColorTransferLib/Evaluation/CSS/init.py
ColorTransferLib/Evaluation/CSS/CSS.py
ColorTransferLib/Evaluation/CF/init.py
ColorTransferLib/Evaluation/CTQM/CTQM.py
ColorTransferLib/Evaluation/Corr/Corr.py
ColorTransferLib/Evaluation/CTQM/init.py
ColorTransferLib/Evaluation/FSIM/init.py
ColorTransferLib/Evaluation/BRISQUE/BRISQUE.py
ColorTransferLib/Evaluation/FSIM/FSIM.py
ColorTransferLib/Evaluation/GSSIM/init.py
ColorTransferLib/Evaluation/IVEGSSIM/IVEGSSIM.py
ColorTransferLib/Evaluation/HI/HI.py
ColorTransferLib/Evaluation/HI/init.py
ColorTransferLib/Evaluation/IVEGSSIM/init.py
ColorTransferLib/Evaluation/LPIPS/LPIPS.py
ColorTransferLib/Evaluation/IVSSIM/IVSSIM.py
ColorTransferLib/Evaluation/MSE/MSE.py
ColorTransferLib/Evaluation/Corr/init.py
ColorTransferLib/Evaluation/IVSSIM/init.py
ColorTransferLib/Evaluation/MSE/init.py
ColorTransferLib/Evaluation/GSSIM/GSSIM.py
ColorTransferLib/Evaluation/MSSSIM/init.py
ColorTransferLib/Evaluation/MSSSIM/MSSSIM.py
ColorTransferLib/Evaluation/NIMA/init.py
ColorTransferLib/Evaluation/NIMA/LICENSE.txt
ColorTransferLib/Evaluation/LPIPS/init.py
ColorTransferLib/Evaluation/NIMA/NIMA.py
ColorTransferLib/Evaluation/NIMA/handlers/data_generator.py
ColorTransferLib/Evaluation/NIMA/handlers/config_loader.py
ColorTransferLib/Evaluation/NIMA/handlers/model_builder.py
ColorTransferLib/Evaluation/NIMA/handlers/init.py
ColorTransferLib/Evaluation/NIMA/handlers/samples_loader.py
ColorTransferLib/Evaluation/NIMA/predict.py
ColorTransferLib/Evaluation/NIMA/tests/test_augmentation_utils.py
ColorTransferLib/Evaluation/NIMA/tests/test_data_generator.py
ColorTransferLib/Evaluation/NIMA/trainer/train.py
ColorTransferLib/Evaluation/NIMA/utils/init.py
ColorTransferLib/Evaluation/NIMA/utils/losses.py
ColorTransferLib/Evaluation/NIMA/utils/utils.py
ColorTransferLib/Evaluation/NIQE/NIQE.py
ColorTransferLib/Evaluation/RMSE/RMSE.py
ColorTransferLib/Evaluation/NIQE/init.py
ColorTransferLib/Evaluation/RMSE/init.py
ColorTransferLib/Evaluation/SSIM/SSIM.py
ColorTransferLib/Evaluation/PSNR/PSNR.py
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/connectMatrix.m
ColorTransferLib/Evaluation/VSI/VSI.py
ColorTransferLib/Evaluation/SSIM/init.py
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/distanceMatrix.m
ColorTransferLib/Evaluation/VSI/init.py
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/formMapPyramid.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/getDims.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/graphsalapply.m
ColorTransferLib/Evaluation/PSNR/init.py
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/makeLocationMap.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/initGBVS.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/indexmatrix.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/graphsalinit.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexArrangeLinear.cc
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexAssignWeights.cc
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexColumnNormalize.cc
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexSumOverScales.cc
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexVectorToMap.cc
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/partitionindex.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/principalEigenvectorRaw.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/namenodes.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/simpledistance.m
ColorTransferLib/Evaluation/VSI/gbvs/compile/cleanmex.m
ColorTransferLib/Evaluation/VSI/gbvs/compile/gbvs_compile.m
ColorTransferLib/Evaluation/VSI/gbvs/algsrc/sparseness.m
ColorTransferLib/Evaluation/VSI/gbvs/compile/gbvs_compile2.m
ColorTransferLib/Evaluation/VSI/gbvs/demo/flicker_motion_demo.m
ColorTransferLib/Evaluation/VSI/gbvs/demo/demonstration.m
ColorTransferLib/Evaluation/VSI/gbvs/demo/simplest_demonstration.m
ColorTransferLib/Evaluation/VSI/gbvs/gbvs_install.m
ColorTransferLib/Evaluation/VSI/gbvs/gbvs_fast.m
ColorTransferLib/Evaluation/VSI/gbvs/gbvs.m
ColorTransferLib/Evaluation/VSI/gbvs/makeGBVSParams.m
ColorTransferLib/Evaluation/VSI/gbvs/ittikochmap.m
ColorTransferLib/Evaluation/VSI/gbvs/readme.txt
ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/attenuateBordersGBVS.m
ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/maxNormalizeStdGBVS.m
ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/mexLocalMaximaGBVS.cc
ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/makeGaborFilterGBVS.m
ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/mySubsample.cc
ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/safeDivideGBVS.m
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/C_color.m
ColorTransferLib/Evaluation/VSI/gbvs/util/areaROC.m
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/I_intensity.m
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/D_dklcolor.m
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/M_motion.m
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/O_orientation.m
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/F_flicker.m
ColorTransferLib/Evaluation/VSI/gbvs/util/getBestRows.m
ColorTransferLib/Evaluation/VSI/gbvs/util/getFeatureMaps.m
ColorTransferLib/Evaluation/VSI/gbvs/util/getIntelligentThresholds.m
ColorTransferLib/Evaluation/VSI/gbvs/util/heatmap_overlay.m
ColorTransferLib/Evaluation/VSI/gbvs/util/makeFixationMask.m
ColorTransferLib/Evaluation/VSI/gbvs/util/linearmap.m
ColorTransferLib/Evaluation/VSI/gbvs/util/myContrast.cc
ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/R_contrast.m
ColorTransferLib/Evaluation/VSI/gbvs/util/mycombnk.m
ColorTransferLib/Evaluation/VSI/gbvs/util/mygausskernel.m
ColorTransferLib/Evaluation/VSI/gbvs/util/mymessage.m
ColorTransferLib/Evaluation/VSI/gbvs/util/mygetrgb.m
ColorTransferLib/Evaluation/VSI/gbvs/util/padImageOld.m
ColorTransferLib/Evaluation/VSI/gbvs/util/padImage.m
ColorTransferLib/Evaluation/VSI/gbvs/util/rankimg.m
ColorTransferLib/Evaluation/VSI/gbvs/util/myconv2.m
ColorTransferLib/Evaluation/VSI/gbvs/util/rgb2dkl.m
ColorTransferLib/Evaluation/VSI/gbvs/util/rocScoreSaliencyVsFixations.m
ColorTransferLib/Evaluation/VSI/gbvs/util/shiftImage.m
ColorTransferLib/Evaluation/VSI/gbvs/util/show_imgnmap.m
ColorTransferLib/Evaluation/init.py
ColorTransferLib/MeshProcessing/init.py
ColorTransferLib/ImageProcessing/Image.py
ColorTransferLib/ImageProcessing/Video.py
ColorTransferLib/MeshProcessing/Mesh.py
ColorTransferLib/ImageProcessing/ColorSpaces.py
ColorTransferLib/Options/BCC.json
ColorTransferLib/Evaluation/VSI/gbvs/util/rocSal.m
ColorTransferLib/ImageProcessing/init.py
ColorTransferLib/MeshProcessing/VolumetricVideo.py
ColorTransferLib/Options/CAM.json
ColorTransferLib/Options/EB3.json
ColorTransferLib/Options/DPT.json
ColorTransferLib/Options/GPC.json
ColorTransferLib/Options/GLO.json
ColorTransferLib/Options/HIS.json
ColorTransferLib/Options/CCS.json
ColorTransferLib/Options/MKL.json
ColorTransferLib/Options/PDF.json
ColorTransferLib/Options/NST.json
ColorTransferLib/Options/PSN.json
ColorTransferLib/Utils/BaseOptions.py
ColorTransferLib/Options/TPS.json
ColorTransferLib/Options/RHG.json
ColorTransferLib/Utils/Helper.py
ColorTransferLib/Options/FUZ.json
ColorTransferLib/Utils/init.py
ColorTransferLib/init.py
ColorTransferLib/Utils/Math.py
README.md
main.py
init.py
requirements/requirements.txt
setup.py
requirements/constraints.txt
ColorTransferLib/Evaluation/VSI/gbvs/util/show_imgnmap2.m

Analysis:
ColorTransferLib/Algorithms/BCC/init.py

from . import BCC

.gitignore

/env/
/data/
/Models/
/build/
/colortransfer.egg-info/
/ColorTransferLib.egg-info/
/dist/
__pycache__/
**.pyc
.DS_Store

ColorTransferLib/Algorithms/CCS/CCS.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import time
from copy import deepcopy

from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Color Transfer in Correlated Color Space
#   Author: Xuezhong Xiao, Lizhuang Ma
#   Published in: Proceedings of the 2006 ACM international conference on Virtual reality continuum and its applications
#   Year of Publication: 2006
#
# Abstract:
#   In this paper we present a process called color transfer which can borrow one image's color characteristics from 
#   another. Recently Reinhard and his colleagues reported a pioneering work of color transfer. Their technology can 
#   produce very believable results, but has to transform pixel values from RGB to lab . Inspired by their work, we 
#   advise an approach which can directly deal with the color transfer in any 3D space. From the view of statistics, 
#   we consider pixel's value as a threedimension stochastic variable and an image as a set of samples, so the 
#   correlations between three components can be measured by covariance. Our method imports covariance between three 
#   components of pixel values while calculate the mean along each of the three axes. Then we decompose the covariance 
#   matrix using SVD algorithm and get a rotation matrix. Finally we can scale, rotate and shift pixel data of target 
#   image to fit data points' cluster of source image in the current color space and get resultant image which takes on 
#   source image's look and feel. Besides the global processing, a swatch-based method is introduced in order to 
#   manipulate images' color more elaborately. Experimental results confirm the validity and usefulness of our method.
#
# Info:
#   Name: CorrelatedColorSpaceTransfer
#   Identifier: CSS
#   Link: https://doi.org/10.1145/1128923.1128974
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class CCS:
    compatibility = {
        "src": ["Image", "Mesh", "PointCloud"],
        "ref": ["Image", "Mesh", "PointCloud"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "CCS",
            "title": "Color Transfer in Correlated Color Space",
            "year": 2006,
            "abstract": "In this paper we present a process called color transfer which can borrow one images color "
                        "characteristics from another. Recently Reinhard and his colleagues reported a pioneering work "
                        "of color transfer. Their technology can produce very believable results, but has to transform "
                        "pixel values from RGB to lab. Inspired by their work, we advise an approach which can directly "
                        "deal with the color transfer in any 3D space. From the view of statistics, we consider pixels "
                        "value as a threedimension stochastic variable and an image as a set of samples, so the "
                        "correlations between three components can be measured by covariance. Our method imports "
                        "covariance between three components of pixel values while calculate the mean along each of the "
                        "three axes. Then we decompose the covariance matrix using SVD algorithm and get a rotation "
                        "matrix. Finally we can scale, rotate and shift pixel data of target image to fit data points "
                        "cluster of source image in the current color space and get resultant image which takes on "
                        "source images look and feel. Besides the global processing, a swatch-based method is introduced "
                        "in order to manipulate images color more elaborately. Experimental results confirm the validity "
                        "and usefulness of our method.",
            "types": ["Image", "Mesh", "PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()

        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, CCS.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        src_color = src.get_colors()
        ref_color = ref.get_colors()

        # original src size
        #size_src = (src.get_height(), src.get_width(), 3)

        out_img = deepcopy(src)
        out = out_img.get_colors()

        # 
        reshaped_src = np.reshape(src_color, (-1,3))
        reshaped_ref = np.reshape(ref_color, (-1,3))

        num_pts_src = reshaped_src.shape[0]
        num_pts_ref = reshaped_ref.shape[0]

        # [] Calculate mean of each channel (for src and ref)
        mean_src = np.mean(src_color, axis=(0, 1))
        mean_ref = np.mean(ref_color, axis=(0, 1))

        # [] Calculate covariance matrix between the three components (for src and ref)
        cov_src = np.cov(reshaped_src, rowvar=False)
        cov_ref = np.cov(reshaped_ref, rowvar=False)

        # [] SVD of covariance matrices
        U_src, L_src, _ = np.linalg.svd(cov_src)
        U_ref, L_ref, _ = np.linalg.svd(cov_ref)

        T_ref = np.eye(4)
        T_ref[:3,3] = mean_ref

        R_ref = np.eye(4)
        R_ref[:3,:3] = U_ref

        S_ref = np.array([[np.sqrt(L_ref[0]), 0, 0, 0],
                          [0, np.sqrt(L_ref[1]), 0, 0],
                          [0, 0, np.sqrt(L_ref[2]), 0],
                          [0, 0, 0, 1]])

        T_src = np.eye(4)
        T_src[:3,3] = -mean_src

        R_src = np.eye(4)
        R_src[:3,:3] = np.linalg.inv(U_src)

        S_src = np.array([[1/np.sqrt(L_src[0]), 0, 0, 0],
                          [0, 1/np.sqrt(L_src[1]), 0, 0],
                          [0, 0, 1/np.sqrt(L_src[2]), 0],
                          [0, 0, 0, 1]])

        # [] turn euclidean points into homogeneous points
        ones = np.ones((num_pts_src, 1))
        homogeneous_src = np.hstack((reshaped_src, ones))

        # [] Apply Transformation: out = T_ref * R_ref * S_ref * S_src * R_src * T_src * src
        transformation_matrix = T_ref @ R_ref @ S_ref @ S_src @ R_src @ T_src
        out = (transformation_matrix @ homogeneous_src.T).T

        # turn homogeneous points into euclidean points 
        out_colors = out[:,:3]
        out_colors = np.clip(out_colors, 0, 1)
        out_img.set_colors(out_colors)

        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/CAM/color_aware_st.py

"""
PyTorch implementation of the paper:
M. Afifi, A. Abuolaim, M. Korashy,  M. A. Brubaker, and M. S. Brown. Color-Aware Style Transfer. arXiv preprint 2021.
# Libraries
"""

#from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import torchvision.models as models
import numpy as np
import copy
import cv2
import math

"""# Color palette 
Ref: https://github.com/tody411/PaletteSelection

Helper functions
"""

# ----------------------------------------------------------------------------------------------------------------------
# Convert image into float32 type.
# ----------------------------------------------------------------------------------------------------------------------
def to32F(img):
    if img.dtype == np.float32:
        return img
    return (1.0 / 255.0) * np.float32(img)

# ----------------------------------------------------------------------------------------------------------------------
# RGB channels of the image.
# ----------------------------------------------------------------------------------------------------------------------
def rgb(img):
    if len(img.shape) == 2:
        h, w = img.shape
        img_rgb = np.zeros((h, w, 3), dtype=img.dtype)
        for ci in range(3):
            img_rgb[:, :, ci] = img
        return img_rgb

    h, w, cs = img.shape
    if cs == 3:
        return img

    img_rgb = np.zeros((h, w, 3), dtype=img.dtype)

    cs = min(3, cs)

    for ci in range(cs):
        img_rgb[:, :, ci] = img[:, :, ci]
    return img_rgb

# ----------------------------------------------------------------------------------------------------------------------
# RGB to Lab.
# ----------------------------------------------------------------------------------------------------------------------
def rgb2Lab(img):
    img_rgb = rgb(img)
    Lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)
    return Lab

# ----------------------------------------------------------------------------------------------------------------------
# Lab to RGB.
# ----------------------------------------------------------------------------------------------------------------------
def Lab2rgb(img):
    rgb = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)
    return rgb

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def _isGray(image):
    return len(image.shape) == 2

# ----------------------------------------------------------------------------------------------------------------------
# True if x is a vector.
# ----------------------------------------------------------------------------------------------------------------------
def isVector(x):
    return x.size == x.shape[0]

# ----------------------------------------------------------------------------------------------------------------------
# True if x is a matrix.
# ----------------------------------------------------------------------------------------------------------------------
def isMatrix(x):
    return not isVector(x)

# ----------------------------------------------------------------------------------------------------------------------
# Norm of vectors (n x m matrix).
# ----------------------------------------------------------------------------------------------------------------------
def normVectors(x):
    return np.sqrt(l2NormVectors(x))

# ----------------------------------------------------------------------------------------------------------------------
# L2 norm of vectors (n x m matrix).
#  n x 1 vector: call np.square.
#  n x m vectors: call np.einsum.
# ----------------------------------------------------------------------------------------------------------------------
def l2NormVectors(x):
    if isVector(x):
        return np.square(x)
    else:
        return np.einsum('...i,...i', x, x)

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def colorCoordinates(color_ids, num_bins, color_range):
    color_ids = np.array(color_ids).T
    c_min, c_max = color_range
    color_coordinates = c_min + (
        color_ids * (c_max - c_min)) / float(num_bins - 1.0)
    return color_coordinates

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def colorDensities(hist_bins):
    hist_positive = hist_bins > 0.0
    color_densities = np.float32(hist_bins[hist_positive])

    density_max = np.max(color_densities)
    color_densities = color_densities / density_max

    return color_densities

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def rgbColors(hist_bins, color_bins):
    hist_positive = hist_bins > 0.0

    colors = color_bins[hist_positive, :]
    colors = np.clip(colors, 0.0, 1.0)
    return colors

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def clipLowDensity(hist_bins, color_bins, alpha):
    density_mean = np.mean(hist_bins)
    low_density = hist_bins < density_mean * alpha
    hist_bins[low_density] = 0.0

    for ci in range(3):
        color_bins[low_density, ci] = 0.0

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def densitySizes(color_densities, density_size_range):
    density_size_min, density_size_max = density_size_range
    density_size_factor = density_size_max / density_size_min
    density_sizes = density_size_min * np.power(
        density_size_factor, color_densities)
    return density_sizes

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def range2ticks(tick_range, decimals=1):
    ticks = np.around(tick_range, decimals=decimals)
    ticks[ticks > 10] = np.rint(ticks[ticks > 10])
    return ticks

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def range2lims(tick_range):
    unit = 0.1 * (tick_range[:, 1] - tick_range[:, 0])
    lim = np.array(tick_range)
    lim[:, 0] += -unit
    lim[:, 1] += unit

    return lim

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Color pixels class:
# input image is automatically converted into np.float32 format.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class ColorPixels:
    def __init__(self, image, num_pixels=1000):
        self._image = to32F(image)
        self._num_pixels = num_pixels
        self._rgb_pixels = None
        self._Lab = None
        self._hsv = None

    # ------------------------------------------------------------------------------------------------------------------
    # RGB pixels.
    # ------------------------------------------------------------------------------------------------------------------
    def rgb(self):
        if self._rgb_pixels is None:
            self._rgb_pixels = self.pixels("rgb")
        return self._rgb_pixels

    # ------------------------------------------------------------------------------------------------------------------
    # Lab pixels.
    # ------------------------------------------------------------------------------------------------------------------
    def Lab(self):
        if self._Lab is None:
            self._Lab = self.pixels("Lab")
        return self._Lab

    # ------------------------------------------------------------------------------------------------------------------
    # Pixels of the given color space.
    # ------------------------------------------------------------------------------------------------------------------
    def pixels(self, color_space="rgb"):
        image = np.array(self._image)

        if color_space == "Lab":
            image = rgb2Lab(self._image)

        return self._image2pixels(image)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _image2pixels(self, image):
        if _isGray(image):
            h, w = image.shape
            step = int(h * w / self._num_pixels)
            return image.reshape((h * w))[::step]

        h, w, cs = image.shape
        step = int(h * w / self._num_pixels)
        return image.reshape((-1, cs))[::step]

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# 3D color histograms:
# Implementation of 3D color histograms.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Hist3D:
    def __init__(self, image,
                 num_bins=16, alpha=0.1, color_space='rgb'):
        self._computeTargetPixels(image, color_space)
        self._num_bins = num_bins
        self._alpha = alpha
        self._color_space = color_space
        self._computeColorRange()
        self._computeHistogram()

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def colorSpace(self):
        return self._color_space

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def colorIDs(self):
        color_ids = np.where(self._histPositive())
        return color_ids

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def colorCoordinates(self):
        color_ids = self.colorIDs()
        num_bins = self._num_bins
        color_range = self._color_range
        return colorCoordinates(color_ids, num_bins, color_range)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def colorDensities(self):
        return colorDensities(self._hist_bins)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def rgbColors(self):
        return rgbColors(self._hist_bins, self._color_bins)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def colorRange(self):
        return self._color_range

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _computeTargetPixels(self, image, color_space):
        color_pixels = ColorPixels(image)
        self._pixels = color_pixels.pixels(color_space)
        self._rgb_pixels = color_pixels.rgb()

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _computeColorRange(self):
        pixels = self._pixels
        cs = pixels.shape[1]

        c_min = np.zeros(cs)
        c_max = np.zeros(cs)
        for ci in range(cs):
            c_min[ci] = np.min(pixels[:, ci])
            c_max[ci] = np.max(pixels[:, ci])

        self._color_range = [c_min, c_max]

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _computeHistogram(self):
        pixels = self._pixels
        num_bins = self._num_bins
        c_min, c_max = self._color_range

        hist_bins = np.zeros((num_bins, num_bins, num_bins), dtype=np.float32)
        color_bins = np.zeros((num_bins, num_bins, num_bins, 3), dtype=np.float32)

        # ADDED
        # if range is 0, add small number for division
        rangee = c_max - c_min
        for i in range(rangee.shape[0]):
            if rangee[i] == 0:
                rangee[i] += 1e-5

        color_ids = (num_bins - 1) * (pixels - c_min) / rangee
        color_ids = np.int32(color_ids)

        #print(rangee)
        #exit()

        for pi, color_id in enumerate(color_ids):
            hist_bins[color_id[0], color_id[1], color_id[2]] += 1
            color_bins[color_id[0], color_id[1], color_id[2]] += self._rgb_pixels[pi]

        self._hist_bins = hist_bins
        hist_positive = self._hist_bins > 0.0

        for ci in range(3):
            color_bins[hist_positive, ci] /= self._hist_bins[hist_positive]

        self._color_bins = color_bins

        self._clipLowDensity()

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _clipLowDensity(self):
        clipLowDensity(self._hist_bins, self._color_bins, self._alpha)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _histPositive(self):
        return self._hist_bins > 0.0

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Auto palette selection:
# Implementation of automatic palette selection.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class PaletteSelection:
    def __init__(self, color_coordinates, color_densities, rgb_colors, num_colors=7, sigma=70.0):
        self._color_coordinates = color_coordinates
        self._color_densities = color_densities
        self._rgb_colors = rgb_colors
        self._num_colors = num_colors
        self._sigma = sigma
        self._palette_coordinates = []
        self._palette_colors = []
        self._computeDarkBrightColors()
        self._computeInitialWeight()
        self._compute()

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def paletteCoordinates(self):
        return self._palette_coordinates

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def paletteColors(self):
        return self._palette_colors

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _compute(self):
        for i in range(self._num_colors):
            palette_coordinate = self._updatePalette()
            self._updateWeight(palette_coordinate)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _computeDarkBrightColors(self):
        rgb_colors = self._rgb_colors
        intensities = normVectors(rgb_colors)
        c_dark = self._color_coordinates[np.argmin(intensities)]
        c_bright = self._color_coordinates[np.argmax(intensities)]
        self._dark_bright = [c_dark, c_bright]

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _computeInitialWeight(self):
        self._color_weights = np.array(self._color_densities)
        self._updateWeight(self._dark_bright[0])
        self._updateWeight(self._dark_bright[1])

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _updatePalette(self):
        color_id = np.argmax(self._color_weights)
        palette_coordinate = self._color_coordinates[color_id]
        self._palette_coordinates.append(palette_coordinate)
        palette_color = self._rgb_colors[color_id]
        self._palette_colors.append(palette_color)
        return palette_coordinate

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def _updateWeight(self, palette_coordinate):
        dists = normVectors(self._color_coordinates - palette_coordinate)
        factors = 1.0 - np.exp(- dists ** 2 / (self._sigma ** 2))
        self._color_weights = factors * self._color_weights

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Mask generation
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class CreateMask(nn.Module):
    def __init__(self, opt, color_palette=None):
        """ Computes masks of the image based on a given color palette
        Args:
          insz: maximum size of the input image; if it is larger than this size, the
            image will be resized (scalar). Default value is imsize (i.e.,
            imsize x imsize pixels).
          color_palette: kx3 tensor of color palette
          sigma: this is the sigma parameter of the kernel function.
            The default value is 0.02.
          smooth: boolean flag to apply a Gaussian blur after creating the mask.
          distance: it can be one of the following options: 'chroma_L2' or 'L2'

        Methods:
          forward: accepts input image and returns its masks based on the input
          color palette
        """
        super(CreateMask, self).__init__()
        self.opt = opt
        self.color_palette = color_palette
        self.insz = opt.img_size
        self.device = opt.device
        self.sigma = opt.sigma
        self.distance = opt.color_distance
        self.smooth = opt.smooth
        self.gaussian_kernel = opt.gaussian_kernel

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        if self.color_palette is None:
            raise NameError('No color palette is given')
        x = torch.clamp(x, 0, 1)
        I = F.interpolate(x, size=(self.insz, self.insz), mode='bilinear', align_corners=False)
        masks = torch.zeros(1, self.color_palette.shape[0], self.insz, self.insz, device=self.device)
        if I.shape[1] > 3:
            I = I[:, :3, :, :]
        if self.distance == 'chroma_L2':
            I = I / (torch.unsqueeze(torch.sum(I, dim=1), dim=1) + self.opt.eps)

        for c in range(self.color_palette.shape[0]):
            color = self.color_palette[c, :].view(1, 3, 1, 1)
            if self.distance == 'chroma_L2':
                color = color / (torch.unsqueeze(torch.sum(color, dim=1), dim=1) + self.opt.eps)
            dist = torch.sqrt(torch.sum((I - color) ** 2, dim=1))
            weight = torch.exp(-1 * (dist / self.sigma) ** 2)
            if self.smooth:
                weight = nn.functional.conv2d(torch.unsqueeze(weight, dim=0),
                                              self.gaussian_kernel,
                                              bias=None, stride=1, padding=7)
            masks[0, c, :, :] = weight
        return masks

# ----------------------------------------------------------------------------------------------------------------------
# Loss Functions
# Gram matrix
# ----------------------------------------------------------------------------------------------------------------------
def gram_matrix(input):
    a, b, c, d = input.size() 
    features = input.view(a * b, c * d)  
    G = torch.mm(features, features.t())  # compute the gram product
    return G.div(a * b * c * d)

# ----------------------------------------------------------------------------------------------------------------------
# Masked Gram matrix
# ----------------------------------------------------------------------------------------------------------------------
def masked_gram_matrix(input, masks, opt):
    k = masks.shape[1]
    a, b, c, d = input.size()
    masks = F.interpolate(masks, size=(c, d), mode='bilinear', 
                          align_corners=False)
    G = torch.zeros(k, a * b * a * b, device=opt.device)
    features = input.view(a * b, c * d)
    for i in range(k):
      mask_values = masks[:, i, :, :].view(a, c * d)
      mask_values = (mask_values - torch.min(mask_values)) / (
          torch.max(mask_values) - torch.min(mask_values))
      #num_elements = torch.sum(mask_values > 0.05)
      num_elements = torch.sum(mask_values)
      #compute the gram product
      weighted_features = features * mask_values
      g = torch.mm(weighted_features, weighted_features.t())
      G[i, :] = g.div(num_elements).view(1, a * b * a * b)
    return G / k

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Content loss
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class ContentLoss(nn.Module):
    def __init__(self, target, opt):
        super(ContentLoss, self).__init__()
        self.target = target.detach()
        self.opt = opt

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def forward(self, input):
        if self.opt.content_feature_distance == 'L2':
            self.loss = F.mse_loss(input, self.target)
        elif self.opt.content_feature_distance == 'COSINE':
            self.loss = cosine_similarity(input, self.target, self.opt)
        else:
            raise NotImplementedError

        return input

# ----------------------------------------------------------------------------------------------------------------------
# Cosine similarity
# ----------------------------------------------------------------------------------------------------------------------
def cosine_similarity(x, y, opt):
    x = x.view(1, -1)
    y = y.view(1, -1)
    return 1 - (torch.sum(x * y)/(x.norm(2) * y.norm(2) + opt.eps))

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def set_requires_grad(model, bool):
    for p in model.parameters():
        p.requires_grad = bool

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Color-aware loss
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class ColorAwareLoss(nn.Module):
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, target_feature, target_masks, opt):
        super(ColorAwareLoss, self).__init__()
        self.target = masked_gram_matrix(target_feature, target_masks, opt).detach()
        self.input_masks = target_masks
        self.opt = opt

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def set_input_masks(self, input_masks):
        self.input_masks = input_masks

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def forward(self, input):
        G = masked_gram_matrix(input, self.input_masks, self.opt)
        if self.opt.style_feature_distance == 'L2':
            self.loss = F.mse_loss(G, self.target)
        elif self.opt.style_feature_distance == 'COSINE':
            self.loss = cosine_similarity(G, self.target)
        else:
            raise NotImplementedError

        return input

# ----------------------------------------------------------------------------------------------------------------------
# Image loader and visualization
# Image loader
# ----------------------------------------------------------------------------------------------------------------------
#def image_loader(image_name, opt, K=16):
def image_loader(img, opt, K=16):
    img = img.astype(np.float64) / 255
    #image = Image.open(image_name)
    # compute color palette
    img_array = img#np.array(img)
    # 16 bins, Lab color space
    hist3D = Hist3D(img_array, num_bins=16, color_space='Lab')

    color_coordinates = hist3D.colorCoordinates()
    color_densities = hist3D.colorDensities()
    rgb_colors = hist3D.rgbColors()

    palette_selection = PaletteSelection(color_coordinates, color_densities, 
                                         rgb_colors, num_colors=K, sigma=70.0)
    colors = palette_selection._palette_colors

    # fake batch dimension required to fit network's input dimensions
    pil_img = img * 255
    pil_img = Image.fromarray(pil_img.astype(np.uint8))
    image = opt.loader(pil_img).unsqueeze(0)
    #return image.to(opt.device, torch.float), torch.tensor(colors).to(opt.device, torch.float)
    return image.to(opt.device, torch.float), torch.tensor(np.array(colors)).to(opt.device, torch.float)

# ----------------------------------------------------------------------------------------------------------------------
# Visualization
# ----------------------------------------------------------------------------------------------------------------------
def imshow(tensor, unloader, title=None):
    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it
    image = image.squeeze(0)      # remove the fake batch dimension
    image = unloader(image)
    plt.imshow(image)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Other classes and functions
# Normalization
# create a module to normalize input image so we can easily put it in a nn.Sequential
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Normalization(nn.Module):
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, mean, std):
        super(Normalization, self).__init__()
        # .view the mean and std to make them [C x 1 x 1] so that they can
        # directly work with image Tensor of shape [B x C x H x W].
        # B is batch size. C is number of channels. H is height and W is width.
        #self.mean = torch.tensor(mean).view(-1, 1, 1)
        #self.std = torch.tensor(std).view(-1, 1, 1)
        self.mean = (mean).clone().detach().view(-1, 1, 1)
        self.std = (std).clone().detach().view(-1, 1, 1)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def forward(self, img):
        # normalize img
        return (img - self.mean) / self.std

# ----------------------------------------------------------------------------------------------------------------------
# Get optimizer
# ----------------------------------------------------------------------------------------------------------------------
def get_input_optimizer(input_img, opt):
    # this line to show that input is a parameter that requires a gradient
    if opt.optimizer == 'LBFGS':
        optimizer = optim.LBFGS([input_img.requires_grad_()], lr=opt.lr)
    elif opt.optimizer == 'Adam':
        optimizer = optim.Adam([input_img.requires_grad_()], lr=opt.lr)
    elif opt.optimizer == 'Adagrad':
        optimizer = optim.Adagrad([input_img.requires_grad_()], lr=opt.lr)
    else:
        raise NotImplementedError

    return optimizer

# ----------------------------------------------------------------------------------------------------------------------
# Get style model and losses
# ----------------------------------------------------------------------------------------------------------------------
def get_style_model_and_losses(cnn,
                               normalization_mean,
                               normalization_std,
                               style_img,
                               content_img,
                               style_img_masks,
                               opt):
    cnn = copy.deepcopy(cnn)

    # normalization module
    normalization = Normalization(normalization_mean, normalization_std).to(opt.device)

    # just in order to have an iterable access to or list of content/syle
    # losses
    content_losses = []
    color_aware_style_losses = []

    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential
    # to put in modules that are supposed to be activated sequentially
    model = nn.Sequential(normalization)

    i = 0  # increment every time we see a conv

    for layer in cnn.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = 'conv_{}'.format(i)
        elif isinstance(layer, nn.ReLU):
            name = 'relu_{}'.format(i)
            # The in-place version doesn't play very nicely with the ContentLoss
            # and StyleLoss we insert below. So we replace with out-of-place
            # ones here.
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = 'pool_{}'.format(i)
        elif isinstance(layer, nn.BatchNorm2d):
            name = 'bn_{}'.format(i)
        else:
            raise RuntimeError('Unrecognized layer: {}'.format(
                layer.__class__.__name__))

        model.add_module(name, layer)

        if name in opt.content_layers_default:
            # add content loss:
            target = model(content_img).detach()
            content_loss = ContentLoss(target, opt)
            model.add_module("content_loss_{}".format(i), content_loss)
            content_losses.append(content_loss)

        if name in opt.color_aware_layers_default:
            # add style loss:
            target_feature = model(style_img).detach()
            color_aware_loss = ColorAwareLoss(target_feature, style_img_masks, opt)
            model.add_module("color_aware_loss_{}".format(i), color_aware_loss)
            color_aware_style_losses.append(color_aware_loss)

    for i in range(len(model) - 1, -1, -1):
        if (isinstance(model[i], ContentLoss) or isinstance(model[i], ColorAwareLoss)):
            break
    model = model[:(i + 1)]

    return model, content_losses, color_aware_style_losses

# ----------------------------------------------------------------------------------------------------------------------
# If SMOOTH is true, create a Gaussian blur kernel.
# ----------------------------------------------------------------------------------------------------------------------
def get_gaussian_kernel(opt):
    if opt.smooth:
        """# Gaussian blur kernel"""

        # Set these to whatever you want for your gaussian filter
        kernel_size = 15
        sigma = 5

        # Create a x, y coordinate grid of shape (kernel_size, kernel_size, 2)
        x_cord = torch.arange(kernel_size)
        x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size)
        y_grid = x_grid.t()
        xy_grid = torch.stack([x_grid, y_grid], dim=-1)

        mean = (kernel_size - 1)/2.
        variance = sigma**2.

        # Calculate the 2-dimensional gaussian kernel which is
        # the product of two gaussian distributions for two different
        # variables (in this case called x and y)
        gaussian_kernel = (1./(2.*math.pi*variance)) *\
                          torch.exp(
                              -torch.sum((xy_grid - mean)**2., dim=-1) /\
                              (2*variance))
        # Make sure sum of values in gaussian kernel equals 1.
        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)

        # Reshape to 2d depthwise convolutional weight
        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)
        gaussian_kernel = gaussian_kernel.to(device=opt.device)
        return gaussian_kernel
    else:
        return None

# ----------------------------------------------------------------------------------------------------------------------
# Run style transfer
# Main function to run style transfer
# ----------------------------------------------------------------------------------------------------------------------
def run_style_transfer(cnn, normalization_mean, normalization_std,
                       content_img, style_img, style_img_masks, final_palette, input_img, opt,
                       update_masks=True,
                       ):
    """Run the style transfer."""
    print('Building the style transfer model..')
    model, content_losses, color_aware_losses = ( 
    get_style_model_and_losses(cnn, normalization_mean, normalization_std, 
                               style_img, content_img, style_img_masks, opt))

    optimizer = get_input_optimizer(input_img, opt)

    mask_generator = CreateMask(opt, color_palette=final_palette)

    if not update_masks:
      input_masks = mask_generator(input_img).detach()
      for i in range(len(model) - 1, -1, -1):
                if isinstance(model[i], ColorAwareLoss):
                  model[i].set_input_masks(input_masks)

    print('Optimizing..')
    run = [0]

    while run[0] <= opt.iterations:
        def closure():
            # correct the values of updated input image
            input_img.data.clamp_(0, 1)
            optimizer.zero_grad()

            if update_masks:
              input_masks = mask_generator(input_img).detach()

            """
            if opt.show_masks and run[0] % 50 == 0:
              for i in range(final_palette.shape[0]):
                plt.figure()
                imshow(style_img_masks[:, i, :, :], opt.unloader, title=f'Style Mask of Color # {i}')
                plt.figure()
                imshow(input_masks[:, i, :, :], opt.unloader, title=f'Input Mask of Color # {i}')
            """

            if update_masks:
              for i in range(len(model) - 1, -1, -1):
                if isinstance(model[i], ColorAwareLoss):
                  model[i].set_input_masks(input_masks)

            model(input_img)

            content_score = 0
            color_aware_score = 0

            for cl in content_losses:
                content_score += cl.loss
            for cal in color_aware_losses:
                color_aware_score += cal.loss

            content_score *= opt.content_loss_weight
            color_aware_score *= opt.style_loss_weight

            loss = content_score + color_aware_score 

            loss.backward()

            run[0] += 1
            if run[0] % 50 == 0:
                print("run {}:".format(run))              
                print('Content Loss: {:4f} Color-Aware Loss: {:4f}'.format(
                    content_score.item(), color_aware_score.item()))

            return content_score + color_aware_score 

        optimizer.step(closure)

    input_img.data.clamp_(0, 1)

    return input_img

# ----------------------------------------------------------------------------------------------------------------------
# Main steps
# Image Loader
# ----------------------------------------------------------------------------------------------------------------------
def apply(src, ref, opt):
    """# Settings

    Input images
    """

    # rescale ref to same size of src
    #ref = 

    STYLE_IMAGE = "data/images/2020_Lee_Example-18_Source.png"
    CONTENT_IMAGE = "data/images/2020_Lee_Example-18_Reference.png"

    if opt.device == "cuda" and torch.cuda.is_available():
        opt.device = torch.device("cuda")
    else:
        opt.device = torch.device("cpu")
        opt.img_size = 128

    opt.gaussian_kernel = get_gaussian_kernel(opt)

    # ADDED
    if opt.enable_img_scale:
        opt.loader = transforms.Compose([
            transforms.Resize((opt.img_size, opt.img_size)),  # scale imported image
            transforms.ToTensor()])  # transform it into a torch tensor
    else:
        opt.loader = transforms.Compose([
            transforms.ToTensor()])  # transform it into a torch tensor

    opt.unloader = transforms.ToPILImage()  # reconvert into PIL image

    content_img, content_palette = image_loader(src, opt, opt.palette_size)
    style_img, style_palette = image_loader(ref, opt, opt.palette_size)

    assert style_img.size() == content_img.size(), \
        "we need to import style and content images of the same size"

    """ Image visualization"""

    plt.ion()

    style_img_color_palette_vis = torch.ones((1, 3, 20, 50 * opt.palette_size),
                                             device=opt.device)
    content_img_color_palette_vis = torch.ones((1, 3, 20,50 * opt.palette_size),
                                               device=opt.device)

    for c in range(opt.palette_size):
        style_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] = (
            style_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] *
            style_palette[c, :].view(1, 3, 1, 1))
        content_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] = (
            content_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] *
            content_palette[c, :].view(1, 3, 1, 1))

    #plt.figure()
    #imshow(style_img, opt.unloader, title='Style Image')
    #plt.figure()
    #imshow(style_img_color_palette_vis, opt.unloader, title='Style Color Palette')
    #plt.figure()
    #imshow(content_img, opt.unloader, title='Content Image')
    #plt.figure()
    #imshow(content_img_color_palette_vis, opt.unloader, title='Content Color Palette')

    """ Mask generation"""

    if opt.select_matches:
      print('Please enter color matching order. ',
      'For example to link the first color of content palette to the third color',
      ' in style palette, please enter: 0, 2\n This will be repeated until you ',
      'enter -1.')
      user_input = input()
      matching_order_content = []
      matching_order_style = []
      while user_input != '-1':
        parts = str.split(user_input, ',')
        c = int(parts[0])
        s = int(parts[1])
        assert style_palette.shape[0] > s and content_palette.shape[0] > c
        matching_order_content.append(c)
        matching_order_style.append(s)
        user_input = input()
      sorted_style_order = np.sort(matching_order_style)
      style_palette = style_palette[sorted_style_order, :]
      sorting_inds = list(np.argsort(matching_order_style).astype(int))
      sorted_content_order = [matching_order_content[i] for i in sorting_inds]
      content_palette = content_palette[sorted_content_order, :]
      opt.palette_size = len(sorted_content_order)
      style_img_color_palette_vis = torch.ones((1, 3, 20, 50 * opt.palette_size),
                                             device=opt.device)
      content_img_color_palette_vis = torch.ones((1, 3, 20,50 * opt.palette_size),
                                               device=opt.device)

      for c in range(opt.palette_size):
        style_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] = (
            style_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] *
            style_palette[c, :].view(1, 3, 1, 1))
        content_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] = (
            content_img_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] *
            content_palette[c, :].view(1, 3, 1, 1))

        """
      plt.figure()
      imshow(style_img_color_palette_vis, opt.unloader, title='Final Style Color Palette')
      plt.figure()
      imshow(content_img_color_palette_vis, opt.unloader, title='Matched Content Color Palette')
      """

      final_palette = content_palette.clone()
      mask_generator_style = CreateMask(opt, color_palette=style_palette)
      mask_generator_content = CreateMask(opt, color_palette=content_palette)

    else:
      final_palette = torch.cat([style_palette, content_palette], dim=0)
      if opt.add_black_white:
        black_white = torch.tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]],
                               dtype=torch.float32, device=opt.device)
        final_palette = torch.cat([final_palette, black_white], dim=0)

      final_palette = torch.unique(final_palette, dim=0)
      final_color_palette_vis = torch.ones((1, 3, 50, 50 * final_palette.shape[0]),
                                         device=opt.device)
      for c in range(final_palette.shape[0]):
        final_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] = (
            final_color_palette_vis[0, :, :, c * 50 : (c + 1) * 50 - 1] *
            final_palette[c, :].view(1, 3, 1, 1))
      #plt.figure()
      #imshow(final_color_palette_vis, opt.unloader, title='Final Color Palette')
      mask_generator_style = CreateMask(opt, color_palette=final_palette)
      mask_generator_content = CreateMask(opt, color_palette=final_palette)

    style_masks = mask_generator_style(style_img)

    content_masks = mask_generator_content(content_img)

    """
    if opt.show_masks:
        for i in range(final_palette.shape[0]):
            plt.figure()
            imshow(style_masks[:, i, :, :], opt.unloader, title=f'Style Mask of Color # {i}')
            plt.figure()
            imshow(content_masks[:, i, :, :], opt.unloader, title=f'Content Mask of Color # {i}')
    """

    """ Loading VGG model"""

    cnn = models.vgg19(pretrained=True).features.to(opt.device).eval()

    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(opt.device)
    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(opt.device)

    input_img = content_img.clone()
    # if you want to use white noise instead uncomment the below line:
    # input_img = torch.randn(content_img.data.size(), device=opt.device)

    """ Run style transfer"""

    output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,
                                content_img, style_img, style_masks, final_palette, input_img, opt,
                                update_masks=not opt.select_matches)

    #plt.figure()
    #imshow(output, opt.unloader, title='Output Image')
    #plt.ioff()
    #plt.show()

    #print(output.detach().shape)
    output = output.squeeze(0)
    output = np.swapaxes(output, 0, 1)
    output = np.swapaxes(output, 1, 2)
    output = output.cpu().detach().numpy()
    output = output * 255
    output = output.astype(np.uint8)
    return output

ColorTransferLib/Algorithms/CAM/init.py

from . import CAM

ColorTransferLib/Algorithms/BCC/FaissKNeighbors.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.

Source: https://towardsdatascience.com/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb
"""

import numpy as np
import faiss

class FaissKNeighbors:
    def __init__(self, k=5):
        self.index = None
        self.y = None
        self.k = k

    def fit(self, X, y):
        self.index = faiss.IndexFlatL2(X.shape[1])
        self.index.add(X.astype(np.float32))
        self.y = y

    def predict(self, X):
        distances, indices = self.index.search(X.astype(np.float32), k=self.k)
        votes = self.y[indices]
        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])
        return predictions

ColorTransferLib/Algorithms/BCC/BCC.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
from copy import deepcopy
import csv
import cv2
import open3d as o3d
import time

from ColorTransferLib.Utils.Helper import check_compatibility
from pyhull.convex_hull import ConvexHull
from .FaissKNeighbors import FaissKNeighbors

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: A framework for transfer colors based on the basic color categories
#   Author: Youngha Chang, Suguru Saito, Masayuki Nakajima
#   Published in: Proceedings Computer Graphics International
#   Year of Publication: 2003
#
# Abstract:
#   Usually, paintings are more appealing than photographic images. This is because paintings have styles. This style 
#   can be distinguished by looking at elements such as motif, color, shape deformation and brush texture. We focus on 
#   the effect of "color" element and devise a method for transforming the color of an input photograph according to a 
#   reference painting. To do this, we consider basic color category concepts in the color transformation process. By 
#   doing so, we achieve large but natural color transformations of an image.
#
# Info:
#   Name: BasicColorCategoryTransfer
#   Identifier: BCC
#   Link: https://doi.org/10.1109/CGI.2003.1214463
#
# Misc:
#   RayCasting: http://www.open3d.org/docs/latest/tutorial/geometry/ray_casting.html
#
# Implementation Details:
#   The number of colors per category has to be at least 4 with unique position in order to generate a convex hull.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class BCC:
    compatibility = {
        "src": ["Image", "Mesh", "PointCloud"],
        "ref": ["Image", "Mesh", "PointCloud"]
    }

    color_samples = {
        "Red": np.array([1.0,0.0,0.0]),
        "Yellow":np.array([1.0,1.0,0.0]),
        "Green": np.array([0.0,1.0,0.0]),
        "Blue": np.array([0.0,0.0,1.0]),
        "Black": np.array([0.0,0.0,0.0]),
        "White": np.array([1.0,1.0,1.0]),
        "Grey": np.array([0.5,0.5,0.5]),
        "Orange": np.array([1.0,0.5,0.0]),
        "Brown": np.array([0.4,0.2,0.1]),
        "Pink": np.array([0.85,0.5,0.75]),
        "Purple": np.array([0.4,0.01,0.77]),
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "BCC",
            "title": "A Framework for Transfer Colors Based on the Basic Color Categories",
            "year": 2003,
            "abstract": "Usually, paintings are more appealing than photographic images. This is because paintings have "
                        "styles. This style can be distinguished by looking at elements such as motif, color, shape "
                        "deformation and brush texture. We focus on the effect of color element and devise a method "
                        "for transforming the color of an input photograph according to a reference painting. To do "
                        "this, we consider basic color category concepts in the color transformation process. By doing "
                        "so, we achieve large but natural color transformations of an image.",
            "types": ["Image", "Mesh", "PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, BCC.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        src_color = src.get_colors()
        src_color = cv2.cvtColor(src_color, cv2.COLOR_RGB2Lab)
        ref_color = ref.get_colors()
        ref_color = cv2.cvtColor(ref_color, cv2.COLOR_RGB2Lab)
        out_img = deepcopy(src)

        # Read Color Dataset
        color_terms = np.array(["Red", "Yellow", "Green", "Blue", "Black", "White", "Grey", "Orange", "Brown", "Pink", "Purple"])
        color_mapping = []
        with open("Models/BCC/colormapping.csv") as csv_file:
            csv_reader = csv.reader(csv_file, delimiter=',')
            line_count = 0
            for row in csv_reader:
                if line_count != 0:
                    color_mapping.append([float(row[0]), float(row[1]), float(row[2]), float(np.where(color_terms == row[3])[0][0])])
                line_count += 1

        color_mapping = np.asarray(color_mapping)
        colors = color_mapping[:,:3] / 255
        colors = np.expand_dims(colors, axis=1).astype("float32")
        colors = cv2.cvtColor(colors, cv2.COLOR_RGB2Lab)
        colors = np.squeeze(colors)
        labels = color_mapping[:,3].astype("int64")

        # Train Classifier
        neigh = FaissKNeighbors(k=1)
        neigh.fit(colors, labels)

        # predic src color label
        src_preds = neigh.predict(src_color[:,0,:]) # colors are of size (number of colors, 1, dimension)
        # predic ref color label
        ref_preds = neigh.predict(ref_color[:,0,:])

        color_cats_src= {
            "Red": [],
            "Yellow": [],
            "Green": [],
            "Blue": [],
            "Black": [],
            "White": [],
            "Grey": [],
            "Orange": [],
            "Brown": [],
            "Pink": [],
            "Purple": []
        }

        color_cats_src_ids = {
            "Red": [],
            "Yellow": [],
            "Green": [],
            "Blue": [],
            "Black": [],
            "White": [],
            "Grey": [],
            "Orange": [],
            "Brown": [],
            "Pink": [],
            "Purple": []
        }

        color_cats_ref = {
            "Red": [],
            "Yellow": [],
            "Green": [],
            "Blue": [],
            "Black": [],
            "White": [],
            "Grey": [],
            "Orange": [],
            "Brown": [],
            "Pink": [],
            "Purple": []
        }

        for i, (pred, color) in enumerate(zip(src_preds, src_color[:,0,:])):
            color_cats_src[color_terms[int(pred)]].append(color)
            color_cats_src_ids[color_terms[int(pred)]].append(i)

        for pred, color in zip(ref_preds, ref_color[:,0,:]):
            color_cats_ref[color_terms[int(pred)]].append(color)

        output_colors = np.empty([0, 3])
        output_ids = np.empty([0, 1])
        for color_cat in color_cats_src.keys():
            print(color_cat)
            output_ids = np.concatenate((output_ids, np.asarray(color_cats_src_ids[color_cat])[:, np.newaxis]))
            # Create Convex Hulls
            # Check if color categories are not empty
            if len(color_cats_src[color_cat]) >= 4 and len(color_cats_ref[color_cat]) < 4:
                output_colors = np.concatenate((output_colors, np.asarray(color_cats_src[color_cat])))
                continue
            elif len(color_cats_src[color_cat]) == 0:
                continue
            elif len(color_cats_src[color_cat]) < 4:
                output_colors = np.concatenate((output_colors, np.asarray(color_cats_src[color_cat])))
                continue

            if BCC.__check_identity(np.asarray(color_cats_src[color_cat])) or BCC.__check_identity(np.asarray(color_cats_ref[color_cat])):
                output_colors = np.concatenate((output_colors, np.asarray(color_cats_src[color_cat])))
                continue

            mesh_src = BCC.__calc_convex_hull(color_cats_src[color_cat])
            mesh_ref = BCC.__calc_convex_hull(color_cats_ref[color_cat])

            mass_center_src = BCC.__calc_gravitational_center(mesh_src)
            mass_center_ref = BCC.__calc_gravitational_center(mesh_ref)

            # calculate intersection between convex hull and ray consisting of the center of mass and the given pixel color
            inter_src = BCC.__calc_line_mesh_intersection(mesh_src, color_cats_src[color_cat] - mass_center_src, mass_center_src)
            inter_ref = BCC.__calc_line_mesh_intersection(mesh_ref, color_cats_src[color_cat] - mass_center_src, mass_center_ref)

            # Color Transfer
            output_colors = BCC.__transfer_colors(output_colors=output_colors, 
                                                                         colors=color_cats_src[color_cat], 
                                                                         mass_center_src=mass_center_src, 
                                                                         mass_center_ref=mass_center_ref, 
                                                                         dist_src=inter_src['t_hit'], 
                                                                         dist_ref=inter_ref['t_hit'])

        sort = np.argsort(output_ids, axis=0)
        sorted_colors = output_colors[sort]

        output_colors = cv2.cvtColor(sorted_colors.astype("float32"), cv2.COLOR_Lab2RGB)
        output_colors = np.clip(output_colors, 0, 1)

        out_img.set_colors(output_colors)

        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

    # ------------------------------------------------------------------------------------------------------------------
    # checks if the given data does not lie on a plane -> this would lead to a convex hull with volume = 0
    # ------------------------------------------------------------------------------------------------------------------  
    def __check_coplanarity(data):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    # checks if the given data contains at least four different values for creating a convex hull with volume > 0
    # returns true if the data does not contain at least four different values
    # ------------------------------------------------------------------------------------------------------------------  
    def __check_identity(data):
        unique_val = np.unique(data, axis=0)
        if unique_val.shape[0] < 4:
            return True
        else:
            return False

    # ------------------------------------------------------------------------------------------------------------------
    # Calculates the gravitational center of a mesh
    # ------------------------------------------------------------------------------------------------------------------  
    def __calc_gravitational_center(mesh):
        # calculate gravitational center of convex hull
        # (1) get geometrical center
        coord_center = mesh.get_center()
        # (2) iterate over triangles and calculate tetrahaedon mass and center using the coordinate center of the whole mesh
        vol_center = 0
        vertices = np.asarray(mesh.vertices)
        mesh_volume = 0
        for tri in mesh.triangles:
            # calculate center
            pos0 = vertices[tri[0]]
            pos1 = vertices[tri[1]]
            pos2 = vertices[tri[2]]
            pos3 = coord_center
            geo_center = np.sum([pos0, pos1, pos2, pos3], axis=0) / 4
            # calculate volume using the formula: V = |(a-b) * ((b-d) x (c-d))| / 6
            vol = np.abs(np.dot((pos0 - pos3), np.cross((pos1 - pos3), (pos2-pos3)))) / 6
            vol_center += vol * geo_center
            mesh_volume += vol
        # (3) calculate mesh center based on: mass_center = sum(tetra_volumes*tetra_centers)/sum(volumes)
        mass_center = vol_center / mesh_volume
        return mass_center

    # ------------------------------------------------------------------------------------------------------------------
    # Calculates the convex hull of a given point set
    # ------------------------------------------------------------------------------------------------------------------  
    def __calc_convex_hull(points):
        chull_red_src = ConvexHull(points)
        chull_red_src_p = np.expand_dims(chull_red_src.points, axis=1).astype("float32")
        chull_red_src_p = np.squeeze(chull_red_src_p)

        mesh = o3d.geometry.TriangleMesh(vertices=o3d.utility.Vector3dVector(chull_red_src_p),
                                          triangles=o3d.utility.Vector3iVector(chull_red_src.vertices))
        return mesh

    # ------------------------------------------------------------------------------------------------------------------
    # Calculates the intersection between a line and a triangle mesh
    # ------------------------------------------------------------------------------------------------------------------    
    def __calc_line_mesh_intersection(mesh, directions, mass_center):
        scene = o3d.t.geometry.RaycastingScene()
        mesh = o3d.t.geometry.TriangleMesh.from_legacy(mesh)
        mesh_id = scene.add_triangles(mesh)

        # Note: directions have to be normalized in order to get the correct ray cast distance
        norms = np.linalg.norm(directions, axis=1)[:, np.newaxis]
        norms_ext = np.concatenate((norms, norms, norms), axis= 1)
        norm_directions = directions / norms_ext

        rays_src = np.concatenate((np.full(np.asarray(directions).shape, mass_center), norm_directions), axis=1)

        rays_src_tensor = o3d.core.Tensor(rays_src, dtype=o3d.core.Dtype.Float32)
        ans = scene.cast_rays(rays_src_tensor)
        return ans

    # ------------------------------------------------------------------------------------------------------------------
    # Calculates the convex hull of a given point set and saves it as a triangle mesh
    # ------------------------------------------------------------------------------------------------------------------ 
    def __write_convex_hull_mesh(colors, shape, path, color, color_space="LAB"):
        if color_space == "RGB":
            ex = np.asarray(colors)[:, np.newaxis]
            cex = cv2.cvtColor(ex, cv2.COLOR_Lab2RGB)
            mesh = BCC.__calc_convex_hull(cex.squeeze())
        else:
            mesh = BCC.__calc_convex_hull(colors)

        colors = np.full(shape, color)
        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)
        o3d.io.write_triangle_mesh(filename=path, 
                                   mesh=mesh, 
                                   write_ascii=True,
                                   write_vertex_normals=False,
                                   write_vertex_colors=True,
                                   write_triangle_uvs=False)

    # ------------------------------------------------------------------------------------------------------------------
    # coplanarity
    # ------------------------------------------------------------------------------------------------------------------         
    def __transfer_colors(output_colors, colors, mass_center_src, mass_center_ref, dist_src, dist_ref):
        point_dir = colors - mass_center_src 
        point_dist = np.linalg.norm(point_dir, axis=1)
        intersection_dist_src = dist_src.numpy()
        relative_point_dist = (point_dist / intersection_dist_src)[:, np.newaxis]

        point_dist_ext = point_dist[:, np.newaxis]
        norm_point_dir = point_dir / np.concatenate((point_dist_ext, point_dist_ext, point_dist_ext), axis= 1)
        intersection_dist_ref = dist_ref.numpy()[:, np.newaxis]

        shift = norm_point_dir * np.concatenate((intersection_dist_ref, intersection_dist_ref, intersection_dist_ref), axis= 1) * np.concatenate((relative_point_dist, relative_point_dist, relative_point_dist), axis= 1)
        out = shift + mass_center_ref
        output_colors = np.concatenate((output_colors, out))
        return output_colors

ColorTransferLib/Algorithms/CCS/init.py

from . import CCS

ColorTransferLib/Algorithms/DPT/DPT.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
from PIL import Image
import numpy as np
import os
import time
from copy import deepcopy

from ColorTransferLib.Algorithms.DPT.photo_style import stylize
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Deep Photo Style Transfer
#   Author: Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala
#   Published in: ...
#   Year of Publication: 2017
#
# Abstract:
#   This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image
#   content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly
#   transfer that separates style from the content of an image by considering different layers of a neural network.
#   However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and
#   reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution
#   is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express
#   this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses
#   distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer
#   of the time of day, weather, season, and artistic edits.
#
# Info:
#   Name: Deep Photo Style Transfer
#   Identifier: DPT
#   Link: https://doi.org/10.48550/arXiv.1703.07511
#   Source: https://github.com/LouieYang/deep-photo-styletransfer-tf
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class DPT:
    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # HOST METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "DPT",
            "title": "Deep Photo Style Transfer",
            "year": 2017,
            "abstract": "This paper introduces a deep-learning approach to photographic style transfer that handles a "
                        "large variety of image content while faithfully transferring the reference style. Our "
                        "approach builds upon the recent work on painterly transfer that separates style from the "
                        "content of an image by considering different layers of a neural network. However, as is, this "
                        "approach is not suitable for photorealistic style transfer. Even when both the input and "
                        "reference images are photographs, the output still exhibits distortions reminiscent of a "
                        "painting. Our contribution is to constrain the transformation from the input to the output to "
                        "be locally affine in colorspace, and to express this constraint as a custom fully "
                        "differentiable energy term. We show that this approach successfully suppresses distortion and "
                        "yields satisfying photorealistic style transfers in a broad variety of scenarios, including "
                        "transfer of the time of day, weather, season, and artistic edits.",
            "types": ["Image"]
        }

        return info
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, DPT.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        src_img = src.get_raw() * 255.0
        ref_img = ref.get_raw() * 255.0
        out_img = deepcopy(src)

        if opt.style_option == 0:
            best_image_bgr = stylize(opt, False, src_img, ref_img)
            out = np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0))
        elif opt.style_option == 1:
            best_image_bgr = stylize(opt, True, src_img, ref_img)
            out = np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0))
        elif opt.style_option == 2:
            opt.max_iter = 2 * opt.max_iter
            tmp_image_bgr = stylize(opt, False, src_img, ref_img)
            result = Image.fromarray(np.uint8(np.clip(tmp_image_bgr[:, :, ::-1], 0, 255.0)))
            opt.init_image_path = os.path.join(opt.serial, "tmp_result.png")

            best_image_bgr = stylize(opt, True, src_img, ref_img)
            out = np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0))

        out_img.set_raw(out.astype(np.float32))
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/DPT/init.py

from . import DPT

ColorTransferLib/Algorithms/DPT/deep_photostyle.py

import argparse
from PIL import Image
import numpy as np
import os
from photo_style import stylize

parser = argparse.ArgumentParser()
# Input Options
parser.add_argument("--content_image_path", dest='content_image_path',  nargs='?',
                    help="Path to the content image")
parser.add_argument("--style_image_path",   dest='style_image_path',    nargs='?',
                    help="Path to the style image")
parser.add_argument("--content_seg_path",   dest='content_seg_path',    nargs='?',
                    help="Path to the style segmentation")
parser.add_argument("--style_seg_path",     dest='style_seg_path',      nargs='?',
                    help="Path to the style segmentation")
parser.add_argument("--init_image_path",    dest='init_image_path',     nargs='?',
                    help="Path to init image", default="")
parser.add_argument("--output_image",       dest='output_image',        nargs='?',
                    help='Path to output the stylized image', default="best_stylized.png")
parser.add_argument("--serial",             dest='serial',              nargs='?',
                    help='Path to save the serial out_iter_X.png', default='./')

# Training Optimizer Options
parser.add_argument("--max_iter",           dest='max_iter',            nargs='?', type=int,
                    help='maximum image iteration', default=1000)
parser.add_argument("--learning_rate",      dest='learning_rate',       nargs='?', type=float,
                    help='learning rate for adam optimizer', default=1.0)
parser.add_argument("--print_iter",         dest='print_iter',          nargs='?', type=int,
                    help='print loss per iterations', default=1)
# Note the result might not be smooth enough since not applying smooth for temp result
parser.add_argument("--save_iter",          dest='save_iter',           nargs='?', type=int,
                    help='save temporary result per iterations', default=100)
parser.add_argument("--lbfgs",              dest='lbfgs',               nargs='?',
                    help="True=lbfgs, False=Adam", default=True)

# Weight Options
parser.add_argument("--content_weight",     dest='content_weight',      nargs='?', type=float,
                    help="weight of content loss", default=5e0)
parser.add_argument("--style_weight",       dest='style_weight',        nargs='?', type=float,
                    help="weight of style loss", default=1e2)
parser.add_argument("--tv_weight",          dest='tv_weight',           nargs='?', type=float,
                    help="weight of total variational loss", default=1e-3)
parser.add_argument("--affine_weight",      dest='affine_weight',       nargs='?', type=float,
                    help="weight of affine loss", default=1e4)

# Style Options
parser.add_argument("--style_option",       dest='style_option',        nargs='?', type=int,
                    help="0=non-Matting, 1=only Matting, 2=first non-Matting, then Matting", default=0)
parser.add_argument("--apply_smooth",       dest='apply_smooth',        nargs='?',
                    help="if apply local affine smooth", default=False)

# Smoothing Argument
parser.add_argument("--f_radius",           dest='f_radius',            nargs='?', type=int,
                    help="smooth argument", default=15)
parser.add_argument("--f_edge",             dest='f_edge',              nargs='?', type=float,
                    help="smooth argument", default=1e-1)

args = parser.parse_args()

def main():
    if args.style_option == 0:
        best_image_bgr = stylize(args, False)
        result = Image.fromarray(np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0)))
        result.save(args.output_image)
    elif args.style_option == 1:
        best_image_bgr = stylize(args, True)
        """
        if not args.apply_smooth:
            result = Image.fromarray(np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0)))
            result.save(args.output_image)
        else:
            # Pycuda runtime incompatible with Tensorflow
            from smooth_local_affine import smooth_local_affine
            content_input = np.array(Image.open(args.content_image_path).convert("RGB"), dtype=np.float32)
            # RGB to BGR
            content_input = content_input[:, :, ::-1]
            # H * W * C to C * H * W
            content_input = content_input.transpose((2, 0, 1))
            input_ = np.ascontiguousarray(content_input, dtype=np.float32) / 255.

            _, H, W = np.shape(input_)

            output_ = np.ascontiguousarray(best_image_bgr.transpose((2, 0, 1)), dtype=np.float32) / 255.
            best_ = smooth_local_affine(output_, input_, 1e-7, 3, H, W, args.f_radius, args.f_edge).transpose(1, 2, 0)
            result = Image.fromarray(np.uint8(np.clip(best_ * 255., 0, 255.)))
            result.save(args.output_image)
        """
        result = Image.fromarray(np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0)))
        result.save(args.output_image)
    elif args.style_option == 2:
        args.max_iter = 2 * args.max_iter
        tmp_image_bgr = stylize(args, False)
        result = Image.fromarray(np.uint8(np.clip(tmp_image_bgr[:, :, ::-1], 0, 255.0)))
        args.init_image_path = os.path.join(args.serial, "tmp_result.png")
        result.save(args.init_image_path)

        best_image_bgr = stylize(args, True)
        """
        if not args.apply_smooth:
            result = Image.fromarray(np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0)))
            result.save(args.output_image)
        else:
            from smooth_local_affine import smooth_local_affine
            content_input = np.array(Image.open(args.content_image_path).convert("RGB"), dtype=np.float32)
            # RGB to BGR
            content_input = content_input[:, :, ::-1]
            # H * W * C to C * H * W
            content_input = content_input.transpose((2, 0, 1))
            input_ = np.ascontiguousarray(content_input, dtype=np.float32) / 255.

            _, H, W = np.shape(input_)

            output_ = np.ascontiguousarray(best_image_bgr.transpose((2, 0, 1)), dtype=np.float32) / 255.
            best_ = smooth_local_affine(output_, input_, 1e-7, 3, H, W, args.f_radius, args.f_edge).transpose(1, 2, 0)
            result = Image.fromarray(np.uint8(np.clip(best_ * 255., 0, 255.)))
            result.save(args.output_image)
        """

        result = Image.fromarray(np.uint8(np.clip(best_image_bgr[:, :, ::-1], 0, 255.0)))
        result.save(args.output_image)

if __name__ == "__main__":
    main()

ColorTransferLib/Algorithms/DPT/photo_style.py

from __future__ import division, print_function

import numpy as np
import tensorflow as tf
from ColorTransferLib.Algorithms.DPT.vgg19.vgg import Vgg19
from PIL import Image
import time
from ColorTransferLib.Algorithms.DPT.closed_form_matting import getLaplacian
import math
from functools import partial
import copy
import os
import cv2
import copy

physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)

tf.compat.v1.disable_eager_execution()

xrange = range  # Python 3

VGG_MEAN = [103.939, 116.779, 123.68]

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def rgb2bgr(rgb, vgg_mean=True):
    if vgg_mean:
        return rgb[:, :, ::-1] - VGG_MEAN
    else:
        return rgb[:, :, ::-1]

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def bgr2rgb(bgr, vgg_mean=False):
    if vgg_mean:
        return bgr[:, :, ::-1] + VGG_MEAN
    else:
        return bgr[:, :, ::-1]

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def load_seg(content_seg, style_seg, content_shape, style_shape):
    color_codes = ['BLUE', 'GREEN', 'BLACK', 'WHITE', 'RED', 'YELLOW', 'GREY', 'LIGHT_BLUE', 'PURPLE']
    def _extract_mask(seg, color_str):
        h, w, c = np.shape(seg)
        if color_str == "BLUE":
            mask_r = (seg[:, :, 0] < 0.1).astype(np.uint8)
            mask_g = (seg[:, :, 1] < 0.1).astype(np.uint8)
            mask_b = (seg[:, :, 2] > 0.9).astype(np.uint8)
        elif color_str == "GREEN":
            mask_r = (seg[:, :, 0] < 0.1).astype(np.uint8)
            mask_g = (seg[:, :, 1] > 0.9).astype(np.uint8)
            mask_b = (seg[:, :, 2] < 0.1).astype(np.uint8)
        elif color_str == "BLACK":
            mask_r = (seg[:, :, 0] < 0.1).astype(np.uint8)
            mask_g = (seg[:, :, 1] < 0.1).astype(np.uint8)
            mask_b = (seg[:, :, 2] < 0.1).astype(np.uint8)
        elif color_str == "WHITE":
            mask_r = (seg[:, :, 0] > 0.9).astype(np.uint8)
            mask_g = (seg[:, :, 1] > 0.9).astype(np.uint8)
            mask_b = (seg[:, :, 2] > 0.9).astype(np.uint8)
        elif color_str == "RED":
            mask_r = (seg[:, :, 0] > 0.9).astype(np.uint8)
            mask_g = (seg[:, :, 1] < 0.1).astype(np.uint8)
            mask_b = (seg[:, :, 2] < 0.1).astype(np.uint8)
        elif color_str == "YELLOW":
            mask_r = (seg[:, :, 0] > 0.9).astype(np.uint8)
            mask_g = (seg[:, :, 1] > 0.9).astype(np.uint8)
            mask_b = (seg[:, :, 2] < 0.1).astype(np.uint8)
        elif color_str == "GREY":
            mask_r = np.multiply((seg[:, :, 0] > 0.4).astype(np.uint8),
                                 (seg[:, :, 0] < 0.6).astype(np.uint8))
            mask_g = np.multiply((seg[:, :, 1] > 0.4).astype(np.uint8),
                                 (seg[:, :, 1] < 0.6).astype(np.uint8))
            mask_b = np.multiply((seg[:, :, 2] > 0.4).astype(np.uint8),
                                 (seg[:, :, 2] < 0.6).astype(np.uint8))
        elif color_str == "LIGHT_BLUE":
            mask_r = (seg[:, :, 0] < 0.1).astype(np.uint8)
            mask_g = (seg[:, :, 1] > 0.9).astype(np.uint8)
            mask_b = (seg[:, :, 2] > 0.9).astype(np.uint8)
        elif color_str == "PURPLE":
            mask_r = (seg[:, :, 0] > 0.9).astype(np.uint8)
            mask_g = (seg[:, :, 1] < 0.1).astype(np.uint8)
            mask_b = (seg[:, :, 2] > 0.9).astype(np.uint8)
        return np.multiply(np.multiply(mask_r, mask_g), mask_b).astype(np.float32)

    # PIL resize has different order of np.shape
    content_seg = content_seg
    style_seg = style_seg
    #content_seg = np.array(Image.open(content_seg_path).convert("RGB").resize(content_shape, resample=Image.BILINEAR), dtype=np.float32) / 255.0
    #style_seg = np.array(Image.open(style_seg_path).convert("RGB").resize(style_shape, resample=Image.BILINEAR), dtype=np.float32) / 255.0

    color_content_masks = []
    color_style_masks = []
    for i in xrange(len(color_codes)):
        color_content_masks.append(tf.expand_dims(tf.expand_dims(tf.constant(_extract_mask(content_seg, color_codes[i])), 0), -1))
        color_style_masks.append(tf.expand_dims(tf.expand_dims(tf.constant(_extract_mask(style_seg, color_codes[i])), 0), -1))

    return color_content_masks, color_style_masks

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def gram_matrix(activations):
    height = tf.shape(activations)[1]
    width = tf.shape(activations)[2]
    num_channels = tf.shape(activations)[3]
    gram_matrix = tf.transpose(activations, [0, 3, 1, 2])
    gram_matrix = tf.reshape(gram_matrix, [num_channels, width * height])
    gram_matrix = tf.matmul(gram_matrix, gram_matrix, transpose_b=True)
    return gram_matrix

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def content_loss(const_layer, var_layer, weight):
    return tf.reduce_mean(tf.compat.v1.squared_difference(const_layer, var_layer)) * weight

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def style_loss(CNN_structure, const_layers, var_layers, content_segs, style_segs, weight):
    loss_styles = []
    layer_count = float(len(const_layers))
    layer_index = 0

    _, content_seg_height, content_seg_width, _ = content_segs[0].get_shape().as_list()
    _, style_seg_height, style_seg_width, _ = style_segs[0].get_shape().as_list()
    for layer_name in CNN_structure:
        layer_name = layer_name[layer_name.find("/") + 1:]

        # downsampling segmentation
        if "pool" in layer_name:
            content_seg_width, content_seg_height = int(math.ceil(content_seg_width / 2)), int(math.ceil(content_seg_height / 2))
            style_seg_width, style_seg_height = int(math.ceil(style_seg_width / 2)), int(math.ceil(style_seg_height / 2))

            for i in xrange(len(content_segs)):
                content_segs[i] = tf.compat.v1.image.resize_bilinear(content_segs[i], tf.constant((content_seg_height, content_seg_width)))
                style_segs[i] = tf.compat.v1.image.resize_bilinear(style_segs[i], tf.constant((style_seg_height, style_seg_width)))

        elif "conv" in layer_name:
            for i in xrange(len(content_segs)):
                # have some differences on border with torch
                content_segs[i] = tf.nn.avg_pool(tf.pad(content_segs[i], [[0, 0], [1, 1], [1, 1], [0, 0]], "CONSTANT"), \
                ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='VALID')
                style_segs[i] = tf.nn.avg_pool(tf.pad(style_segs[i], [[0, 0], [1, 1], [1, 1], [0, 0]], "CONSTANT"), \
                ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='VALID')

        if layer_name == var_layers[layer_index].name[var_layers[layer_index].name.find("/") + 1:]:
            print("Setting up style layer: <{}>".format(layer_name))
            const_layer = const_layers[layer_index]
            var_layer = var_layers[layer_index]

            layer_index = layer_index + 1

            layer_style_loss = 0.0
            for content_seg, style_seg in zip(content_segs, style_segs):
                gram_matrix_const = gram_matrix(tf.multiply(const_layer, style_seg))
                style_mask_mean   = tf.reduce_mean(style_seg)
                gram_matrix_const = tf.cond(tf.greater(style_mask_mean, 0.),
                                        lambda: gram_matrix_const / (tf.compat.v1.to_float(tf.size(const_layer)) * style_mask_mean),
                                        lambda: gram_matrix_const
                                    )

                gram_matrix_var   = gram_matrix(tf.multiply(var_layer, content_seg))
                content_mask_mean = tf.reduce_mean(content_seg)
                gram_matrix_var   = tf.cond(tf.greater(content_mask_mean, 0.),
                                        lambda: gram_matrix_var / (tf.compat.v1.to_float(tf.size(var_layer)) * content_mask_mean),
                                        lambda: gram_matrix_var
                                    )

                diff_style_sum    = tf.reduce_mean(tf.compat.v1.squared_difference(gram_matrix_const, gram_matrix_var)) * content_mask_mean

                layer_style_loss += diff_style_sum

            loss_styles.append(layer_style_loss * weight)
    return loss_styles

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def total_variation_loss(output, weight):
    shape = output.get_shape()
    tv_loss = tf.reduce_sum((output[:, :-1, :-1, :] - output[:, :-1, 1:, :]) * (output[:, :-1, :-1, :] - output[:, :-1, 1:, :]) + \
              (output[:, :-1, :-1, :] - output[:, 1:, :-1, :]) * (output[:, :-1, :-1, :] - output[:, 1:, :-1, :])) / 2.0
    return tv_loss * weight

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def affine_loss(output, M, weight):
    loss_affine = 0.0
    output_t = output / 255.
    for Vc in tf.unstack(output_t, axis=-1):
        Vc_ravel = tf.reshape(tf.transpose(Vc), [-1])
        loss_affine += tf.matmul(tf.expand_dims(Vc_ravel, 0), tf.compat.v1.sparse_tensor_dense_matmul(M, tf.expand_dims(Vc_ravel, -1)))

    return loss_affine * weight

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def save_result(img_, str_):
    result = Image.fromarray(np.uint8(np.clip(img_, 0, 255.0)))
    result.save(str_)

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
iter_count = 0
min_loss, best_image = float("inf"), None
def print_loss(args, loss_content, loss_styles_list, loss_tv, loss_affine, overall_loss, output_image):
    global iter_count, min_loss, best_image
    if iter_count % args.print_iter == 0:
        print('Iteration {} / {}\n\tContent loss: {}'.format(iter_count, args.max_iter, loss_content))
        for j, style_loss in enumerate(loss_styles_list):
            print('\tStyle {} loss: {}'.format(j + 1, style_loss))
        print('\tTV loss: {}'.format(loss_tv))
        print('\tAffine loss: {}'.format(loss_affine))
        print('\tTotal loss: {}'.format(overall_loss - loss_affine))

    if overall_loss < min_loss:
        min_loss, best_image = overall_loss, output_image

    if iter_count % args.save_iter == 0 and iter_count != 0:
        save_result(best_image[:, :, ::-1], os.path.join(args.serial, 'out_iter_{}.png'.format(iter_count)))

    iter_count += 1

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def stylize(args, Matting, src, ref):
    config = tf.compat.v1.ConfigProto()
    config.gpu_options.allow_growth = True
    sess = tf.compat.v1.Session(config=config)

    # prepare input images
    content_image = np.array(src, dtype=np.float32)
    #content_image = np.array(Image.open("data/images/2020_Lee_Example-18_Source.png").convert("RGB"), dtype=np.float32)
    content_width, content_height = content_image.shape[1], content_image.shape[0]

    if Matting:
        M = tf.compat.v1.to_float(getLaplacian(content_image / 255.))

    content_image = rgb2bgr(content_image)
    content_image = content_image.reshape((1, content_height, content_width, 3)).astype(np.float32)

    style_image = rgb2bgr(np.array(ref, dtype=np.float32))
    #style_image = rgb2bgr(np.array(Image.open("data/images/2020_Lee_Example-18_Reference.png").convert("RGB"), dtype=np.float32))
    style_width, style_height = style_image.shape[1], style_image.shape[0]
    style_image = style_image.reshape((1, style_height, style_width, 3)).astype(np.float32)

    content_masks = np.zeros((content_height, content_width, 3), dtype=np.float32)
    style_masks = np.zeros((style_height, style_width, 3), dtype=np.float32)
    content_masks, style_masks = load_seg(content_masks, style_masks, [content_width, content_height],[style_width, style_height])
    #content_masks, style_masks = load_seg(args.content_seg_path, args.style_seg_path, [content_width, content_height], [style_width, style_height])

    if not args.init_image_path:
        if Matting:
            print("<WARNING>: Apply Matting with random init")
        init_image = np.random.randn(1, content_height, content_width, 3).astype(np.float32) * 0.0001
    else:
        init_image = np.expand_dims(rgb2bgr(np.array(Image.open(args.init_image_path).convert("RGB"), dtype=np.float32)).astype(np.float32), 0)

    mean_pixel = tf.constant(VGG_MEAN)
    input_image = tf.Variable(init_image)

    with tf.name_scope("constant"):
        vgg_const = Vgg19(args.vgg19_path)
        vgg_const.build(tf.constant(content_image), clear_data=False)

        content_fv = sess.run(vgg_const.conv4_2)
        content_layer_const = tf.constant(content_fv)

        vgg_const.build(tf.constant(style_image))
        style_layers_const = [vgg_const.conv1_1, vgg_const.conv2_1, vgg_const.conv3_1, vgg_const.conv4_1, vgg_const.conv5_1]
        style_fvs = sess.run(style_layers_const)
        style_layers_const = [tf.constant(fv) for fv in style_fvs]

    with tf.name_scope("variable"):
        vgg_var = Vgg19(args.vgg19_path)
        vgg_var.build(input_image)

    # which layers we want to use?
    style_layers_var = [vgg_var.conv1_1, vgg_var.conv2_1, vgg_var.conv3_1, vgg_var.conv4_1, vgg_var.conv5_1]
    content_layer_var = vgg_var.conv4_2

    # The whole CNN structure to downsample mask
    layer_structure_all = [layer.name for layer in vgg_var.get_all_layers()]

    # Content Loss
    loss_content = content_loss(content_layer_const, content_layer_var, float(args.content_weight))

    # Style Loss
    loss_styles_list = style_loss(layer_structure_all, style_layers_const, style_layers_var, content_masks, style_masks, float(args.style_weight))
    loss_style = 0.0
    for loss in loss_styles_list:
        loss_style += loss

    input_image_plus = tf.squeeze(input_image + mean_pixel, [0])

    # Affine Loss
    if Matting:
        loss_affine = affine_loss(input_image_plus, M, args.affine_weight)
    else:
        loss_affine = tf.constant(0.00001)  # junk value

    # Total Variational Loss
    loss_tv = total_variation_loss(input_image, float(args.tv_weight))
    """
    if args.lbfgs:
        if not Matting:
            overall_loss = loss_content + loss_tv + loss_style
        else:
            overall_loss = loss_content + loss_style + loss_tv + loss_affine

        optimizer = tf.contrib.opt.ScipyOptimizerInterface(overall_loss, method='L-BFGS-B', options={'maxiter': args.max_iter, 'disp': 0})
        sess.run(tf.global_variables_initializer())
        print_loss_partial = partial(print_loss, args)
        optimizer.minimize(sess, fetches=[loss_content, loss_styles_list, loss_tv, loss_affine, overall_loss, input_image_plus], loss_callback=print_loss_partial)

        global min_loss, best_image, iter_count
        best_result = copy.deepcopy(best_image)
        min_loss, best_image = float("inf"), None
        return best_result
    else:
        VGGNetLoss = loss_content + loss_tv + loss_style
        optimizer = tf.train.AdamOptimizer(learning_rate=args.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08)
        VGG_grads = optimizer.compute_gradients(VGGNetLoss, [input_image])

        if Matting:
            b, g, r = tf.unstack(input_image_plus / 255., axis=-1)
            b_gradient = tf.transpose(tf.reshape(2 * tf.sparse_tensor_dense_matmul(M, tf.expand_dims(tf.reshape(tf.transpose(b), [-1]), -1)), [content_width, content_height]))
            g_gradient = tf.transpose(tf.reshape(2 * tf.sparse_tensor_dense_matmul(M, tf.expand_dims(tf.reshape(tf.transpose(g), [-1]), -1)), [content_width, content_height]))
            r_gradient = tf.transpose(tf.reshape(2 * tf.sparse_tensor_dense_matmul(M, tf.expand_dims(tf.reshape(tf.transpose(r), [-1]), -1)), [content_width, content_height]))

            Matting_grad = tf.expand_dims(tf.stack([b_gradient, g_gradient, r_gradient], axis=-1), 0) / 255. * args.affine_weight
            VGGMatting_grad = [(VGG_grad[0] + Matting_grad, VGG_grad[1]) for VGG_grad in VGG_grads]

            train_op = optimizer.apply_gradients(VGGMatting_grad)
        else:
            train_op = optimizer.apply_gradients(VGG_grads)

        sess.run(tf.global_variables_initializer())
        min_loss, best_image = float("inf"), None
        for i in xrange(1, args.max_iter):
            _, loss_content_, loss_styles_list_, loss_tv_, loss_affine_, overall_loss_, output_image_ = sess.run([
                train_op, loss_content, loss_styles_list, loss_tv, loss_affine, VGGNetLoss, input_image_plus
            ])
            if i % args.print_iter == 0:
                print('Iteration {} / {}\n\tContent loss: {}'.format(i, args.max_iter, loss_content_))
                for j, style_loss_ in enumerate(loss_styles_list_):
                    print('\tStyle {} loss: {}'.format(j + 1, style_loss_))
                print('\tTV loss: {}'.format(loss_tv_))
                if Matting:
                    print('\tAffine loss: {}'.format(loss_affine_))
                print('\tTotal loss: {}'.format(overall_loss_ - loss_tv_))

            if overall_loss_ < min_loss:
                min_loss, best_image = overall_loss_, output_image_

            if i % args.save_iter == 0 and i != 0:
                save_result(best_image[:, :, ::-1], os.path.join(args.serial, 'out_iter_{}.png'.format(i)))

        return best_image
        """
    VGGNetLoss = loss_content + loss_tv + loss_style
    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-08)
    VGG_grads = optimizer.compute_gradients(VGGNetLoss, [input_image])

    if Matting:
        b, g, r = tf.unstack(input_image_plus / 255., axis=-1)
        b_gradient = tf.transpose(tf.reshape(2 * tf.compat.v1.sparse_tensor_dense_matmul(M, tf.expand_dims(tf.reshape(tf.transpose(b), [-1]), -1)), [content_width, content_height]))
        g_gradient = tf.transpose(tf.reshape(2 * tf.compat.v1.sparse_tensor_dense_matmul(M, tf.expand_dims(tf.reshape(tf.transpose(g), [-1]), -1)), [content_width, content_height]))
        r_gradient = tf.transpose(tf.reshape(2 * tf.compat.v1.sparse_tensor_dense_matmul(M, tf.expand_dims(tf.reshape(tf.transpose(r), [-1]), -1)), [content_width, content_height]))

        Matting_grad = tf.expand_dims(tf.stack([b_gradient, g_gradient, r_gradient], axis=-1), 0) / 255. * args.affine_weight
        VGGMatting_grad = [(VGG_grad[0] + Matting_grad, VGG_grad[1]) for VGG_grad in VGG_grads]

        train_op = optimizer.apply_gradients(VGGMatting_grad)
    else:
        train_op = optimizer.apply_gradients(VGG_grads)

    sess.run(tf.compat.v1.global_variables_initializer())
    min_loss, best_image = float("inf"), None
    for i in xrange(1, args.max_iter):
        _, loss_content_, loss_styles_list_, loss_tv_, loss_affine_, overall_loss_, output_image_ = sess.run([train_op, loss_content, loss_styles_list, loss_tv, loss_affine, VGGNetLoss, input_image_plus])
        if i % args.print_iter == 0:
            print('Iteration {} / {}\n\tContent loss: {}'.format(i, args.max_iter, loss_content_))
            #for j, style_loss_ in enumerate(loss_styles_list_):
            #    print('\tStyle {} loss: {}'.format(j + 1, style_loss_))
            #print('\tTV loss: {}'.format(loss_tv_))
            #if Matting:
            #    print('\tAffine loss: {}'.format(loss_affine_))
            #print('\tTotal loss: {}'.format(overall_loss_ - loss_tv_))

        if overall_loss_ < min_loss:
            min_loss, best_image = overall_loss_, output_image_

        # Saves results after the given number of iterations
        # if i % args.save_iter == 0 and i != 0:
        #     save_result(best_image[:, :, ::-1], os.path.join(args.serial, 'out_iter_{}.png'.format(i)))

    tf.keras.backend.clear_session()
    sess.close()

    return best_image

ColorTransferLib/Algorithms/DPT/smooth_local_affine.py

import numpy as np
from PIL import Image
import pycuda.autoinit
import pycuda.driver as drv
import scipy.io
from pycuda.compiler import SourceModule

def smooth_local_affine(output_, input_, epsilon, patch, h, w, f_r, f_e):
	mod = SourceModule("""

	#include <stdio.h>
	#include <assert.h>
	#include <math_constants.h>
	#include <math_functions.h>
	#include <stdint.h>
	#include <unistd.h>

	#define TB 256
	#define EPS 1e-7

	__device__ bool InverseMat4x4(double m_in[4][4], double inv_out[4][4]) {
		double m[16], inv[16];
		for (int i = 0; i < 4; i++) {
			for (int j = 0; j < 4; j++) {
				m[i * 4 + j] = m_in[i][j];
			}
		}

	    inv[0] = m[5]  * m[10] * m[15] -
	             m[5]  * m[11] * m[14] -
	             m[9]  * m[6]  * m[15] +
	             m[9]  * m[7]  * m[14] +
	             m[13] * m[6]  * m[11] -
	             m[13] * m[7]  * m[10];

	    inv[4] = -m[4]  * m[10] * m[15] +
	              m[4]  * m[11] * m[14] +
	              m[8]  * m[6]  * m[15] -
	              m[8]  * m[7]  * m[14] -
	              m[12] * m[6]  * m[11] +
	              m[12] * m[7]  * m[10];

	    inv[8] = m[4]  * m[9] * m[15] -
	             m[4]  * m[11] * m[13] -
	             m[8]  * m[5] * m[15] +
	             m[8]  * m[7] * m[13] +
	             m[12] * m[5] * m[11] -
	             m[12] * m[7] * m[9];

	    inv[12] = -m[4]  * m[9] * m[14] +
	               m[4]  * m[10] * m[13] +
	               m[8]  * m[5] * m[14] -
	               m[8]  * m[6] * m[13] -
	               m[12] * m[5] * m[10] +
	               m[12] * m[6] * m[9];

	    inv[1] = -m[1]  * m[10] * m[15] +
	              m[1]  * m[11] * m[14] +
	              m[9]  * m[2] * m[15] -
	              m[9]  * m[3] * m[14] -
	              m[13] * m[2] * m[11] +
	              m[13] * m[3] * m[10];

	    inv[5] = m[0]  * m[10] * m[15] -
	             m[0]  * m[11] * m[14] -
	             m[8]  * m[2] * m[15] +
	             m[8]  * m[3] * m[14] +
	             m[12] * m[2] * m[11] -
	             m[12] * m[3] * m[10];

	    inv[9] = -m[0]  * m[9] * m[15] +
	              m[0]  * m[11] * m[13] +
	              m[8]  * m[1] * m[15] -
	              m[8]  * m[3] * m[13] -
	              m[12] * m[1] * m[11] +
	              m[12] * m[3] * m[9];

	    inv[13] = m[0]  * m[9] * m[14] -
	              m[0]  * m[10] * m[13] -
	              m[8]  * m[1] * m[14] +
	              m[8]  * m[2] * m[13] +
	              m[12] * m[1] * m[10] -
	              m[12] * m[2] * m[9];

	    inv[2] = m[1]  * m[6] * m[15] -
	             m[1]  * m[7] * m[14] -
	             m[5]  * m[2] * m[15] +
	             m[5]  * m[3] * m[14] +
	             m[13] * m[2] * m[7] -
	             m[13] * m[3] * m[6];

	    inv[6] = -m[0]  * m[6] * m[15] +
	              m[0]  * m[7] * m[14] +
	              m[4]  * m[2] * m[15] -
	              m[4]  * m[3] * m[14] -
	              m[12] * m[2] * m[7] +
	              m[12] * m[3] * m[6];

	    inv[10] = m[0]  * m[5] * m[15] -
	              m[0]  * m[7] * m[13] -
	              m[4]  * m[1] * m[15] +
	              m[4]  * m[3] * m[13] +
	              m[12] * m[1] * m[7] -
	              m[12] * m[3] * m[5];

	    inv[14] = -m[0]  * m[5] * m[14] +
	               m[0]  * m[6] * m[13] +
	               m[4]  * m[1] * m[14] -
	               m[4]  * m[2] * m[13] -
	               m[12] * m[1] * m[6] +
	               m[12] * m[2] * m[5];

	    inv[3] = -m[1] * m[6] * m[11] +
	              m[1] * m[7] * m[10] +
	              m[5] * m[2] * m[11] -
	              m[5] * m[3] * m[10] -
	              m[9] * m[2] * m[7] +
	              m[9] * m[3] * m[6];

	    inv[7] = m[0] * m[6] * m[11] -
	             m[0] * m[7] * m[10] -
	             m[4] * m[2] * m[11] +
	             m[4] * m[3] * m[10] +
	             m[8] * m[2] * m[7] -
	             m[8] * m[3] * m[6];

	    inv[11] = -m[0] * m[5] * m[11] +
	               m[0] * m[7] * m[9] +
	               m[4] * m[1] * m[11] -
	               m[4] * m[3] * m[9] -
	               m[8] * m[1] * m[7] +
	               m[8] * m[3] * m[5];

	    inv[15] = m[0] * m[5] * m[10] -
	              m[0] * m[6] * m[9] -
	              m[4] * m[1] * m[10] +
	              m[4] * m[2] * m[9] +
	              m[8] * m[1] * m[6] -
	              m[8] * m[2] * m[5];

	    double det = m[0] * inv[0] + m[1] * inv[4] + m[2] * inv[8] + m[3] * inv[12];

	    if (abs(det) < 1e-9) {
	        return false;
	    }

	    det = 1.0 / det;

	    for (int i = 0; i < 4; i++) {
	    	for (int j = 0; j < 4; j++) {
	    		inv_out[i][j] = inv[i * 4 + j] * det;
	    	}
	    }

	    return true;
	}

	__global__ void best_local_affine_kernel(
		float *output, float *input, float *affine_model,
		int h, int w, float epsilon, int kernel_radius
	)
	{
		int size = h * w;
		int id = blockIdx.x * blockDim.x + threadIdx.x;

		if (id < size) {
			int x = id % w, y = id / w;

			double Mt_M[4][4] = {}; // 4x4
			double invMt_M[4][4] = {};
			double Mt_S[3][4] = {}; // RGB -> 1x4
			double A[3][4] = {};
			for (int i = 0; i < 4; i++)
				for (int j = 0; j < 4; j++) {
					Mt_M[i][j] = 0, invMt_M[i][j] = 0;
					if (i != 3) {
						Mt_S[i][j] = 0, A[i][j] = 0;
						if (i == j)
				    		Mt_M[i][j] = 1e-3;
				    }
				}

			for (int dy = -kernel_radius; dy <= kernel_radius; dy++) {
				for (int dx = -kernel_radius; dx <= kernel_radius; dx++) {

					int xx = x + dx, yy = y + dy;
					int id2 = yy * w + xx;

					if (0 <= xx && xx < w && 0 <= yy && yy < h) {

						Mt_M[0][0] += input[id2 + 2*size] * input[id2 + 2*size];
						Mt_M[0][1] += input[id2 + 2*size] * input[id2 + size];
						Mt_M[0][2] += input[id2 + 2*size] * input[id2];
						Mt_M[0][3] += input[id2 + 2*size];

						Mt_M[1][0] += input[id2 + size] * input[id2 + 2*size];
						Mt_M[1][1] += input[id2 + size] * input[id2 + size];
						Mt_M[1][2] += input[id2 + size] * input[id2];
						Mt_M[1][3] += input[id2 + size];

						Mt_M[2][0] += input[id2] * input[id2 + 2*size];
						Mt_M[2][1] += input[id2] * input[id2 + size];
						Mt_M[2][2] += input[id2] * input[id2];
						Mt_M[2][3] += input[id2];

						Mt_M[3][0] += input[id2 + 2*size];
						Mt_M[3][1] += input[id2 + size];
						Mt_M[3][2] += input[id2];
						Mt_M[3][3] += 1;

						Mt_S[0][0] += input[id2 + 2*size] * output[id2 + 2*size];
						Mt_S[0][1] += input[id2 + size] * output[id2 + 2*size];
						Mt_S[0][2] += input[id2] * output[id2 + 2*size];
						Mt_S[0][3] += output[id2 + 2*size];

						Mt_S[1][0] += input[id2 + 2*size] * output[id2 + size];
						Mt_S[1][1] += input[id2 + size] * output[id2 + size];
						Mt_S[1][2] += input[id2] * output[id2 + size];
						Mt_S[1][3] += output[id2 + size];

						Mt_S[2][0] += input[id2 + 2*size] * output[id2];
						Mt_S[2][1] += input[id2 + size] * output[id2];
						Mt_S[2][2] += input[id2] * output[id2];
						Mt_S[2][3] += output[id2];
					}
				}
			}

			bool success = InverseMat4x4(Mt_M, invMt_M);

			for (int i = 0; i < 3; i++) {
				for (int j = 0; j < 4; j++) {
					for (int k = 0; k < 4; k++) {
						A[i][j] += invMt_M[j][k] * Mt_S[i][k];
					}
				}
			}

			for (int i = 0; i < 3; i++) {
				for (int j = 0; j < 4; j++) {
					int affine_id = i * 4 + j;
					affine_model[12 * id + affine_id] = A[i][j];
				}
			}

		}
		return ;
	}

	__global__ void bilateral_smooth_kernel(
		float *affine_model, float *filtered_affine_model, float *guide,
		int h, int w, int kernel_radius, float sigma1, float sigma2
	)
	{
		int id = blockIdx.x * blockDim.x + threadIdx.x;
		int size = h * w;
		if (id < size) {
			int x = id % w;
			int y = id / w;

			double sum_affine[12] = {};
			double sum_weight = 0;
			for (int dx = -kernel_radius; dx <= kernel_radius; dx++) {
				for (int dy = -kernel_radius; dy <= kernel_radius; dy++) {
					int yy = y + dy, xx = x + dx;
					int id2 = yy * w + xx;
					if (0 <= xx && xx < w && 0 <= yy && yy < h) {
						float color_diff1 = guide[yy*w + xx] - guide[y*w + x];
						float color_diff2 = guide[yy*w + xx + size] - guide[y*w + x + size];
						float color_diff3 = guide[yy*w + xx + 2*size] - guide[y*w + x + 2*size];
						float color_diff_sqr =
							(color_diff1*color_diff1 + color_diff2*color_diff2 + color_diff3*color_diff3) / 3;

						float v1 = exp(-(dx * dx + dy * dy) / (2 * sigma1 * sigma1));
						float v2 = exp(-(color_diff_sqr) / (2 * sigma2 * sigma2));
						float weight = v1 * v2;

						for (int i = 0; i < 3; i++) {
							for (int j = 0; j < 4; j++) {
								int affine_id = i * 4 + j;
								sum_affine[affine_id] += weight * affine_model[id2*12 + affine_id];
							}
						}
						sum_weight += weight;
					}
				}
			}

			for (int i = 0; i < 3; i++) {
				for (int j = 0; j < 4; j++) {
					int affine_id = i * 4 + j;
					filtered_affine_model[id*12 + affine_id] = sum_affine[affine_id] / sum_weight;
				}
			}
		}
		return ;
	}

	__global__ void reconstruction_best_kernel(
		float *input, float *filtered_affine_model, float *filtered_best_output,
		int h, int w
	)
	{
		int id = blockIdx.x * blockDim.x + threadIdx.x;
		int size = h * w;
		if (id < size) {
			double out1 =
				input[id + 2*size] * filtered_affine_model[id*12 + 0] + // A[0][0] +
				input[id + size]   * filtered_affine_model[id*12 + 1] + // A[0][1] +
				input[id]          * filtered_affine_model[id*12 + 2] + // A[0][2] +
									 filtered_affine_model[id*12 + 3]; //A[0][3];
			double out2 =
				input[id + 2*size] * filtered_affine_model[id*12 + 4] + //A[1][0] +
				input[id + size]   * filtered_affine_model[id*12 + 5] + //A[1][1] +
				input[id]          * filtered_affine_model[id*12 + 6] + //A[1][2] +
									 filtered_affine_model[id*12 + 7]; //A[1][3];
			double out3 =
				input[id + 2*size] * filtered_affine_model[id*12 + 8] + //A[2][0] +
				input[id + size]   * filtered_affine_model[id*12 + 9] + //A[2][1] +
				input[id]          * filtered_affine_model[id*12 + 10] + //A[2][2] +
									 filtered_affine_model[id*12 + 11]; // A[2][3];

			filtered_best_output[id] = out1;
			filtered_best_output[id + size] = out2;
			filtered_best_output[id + 2*size] = out3;
		}
		return ;
	}
	""")
	_best_local_affine_kernel = mod.get_function("best_local_affine_kernel")
	_bilateral_smooth_kernel = mod.get_function("bilateral_smooth_kernel")
	_reconstruction_best_kernel = mod.get_function("reconstruction_best_kernel")

	filter_radius = f_r
	sigma1, sigma2 = filter_radius / 3, f_e

	filtered_best_output = np.zeros(np.shape(input_), dtype=np.float32)
	affine_model = np.zeros((h * w, 12), dtype=np.float32)
	filtered_affine_model = np.zeros((h * w, 12), dtype=np.float32)

	radius = (patch - 1) / 2

	_best_local_affine_kernel(
        drv.InOut(output_), drv.InOut(input_), drv.InOut(affine_model),
        np.int32(h), np.int32(w), np.float32(epsilon), np.int32(radius), block=(256, 1, 1), grid=(int((h * w) / 256 + 1), 1)
    )

	_bilateral_smooth_kernel(
		drv.InOut(affine_model), drv.InOut(filtered_affine_model),
		drv.InOut(input_), np.int32(h), np.int32(w), np.int32(f_r), np.float32(sigma1), np.float32(sigma2),
		block=(256, 1, 1), grid=(int((h * w) / 256 + 1), 1)
	)
	_reconstruction_best_kernel(
		drv.InOut(input_), drv.InOut(filtered_affine_model), drv.InOut(filtered_best_output),
		np.int32(h), np.int32(w), block=(256, 1, 1), grid=(int((h * w) / 256 + 1), 1)
	)
	return filtered_best_output

if __name__ == "__main__":
	X = scipy.io.loadmat("./best3_t_1000.mat")
	output_ = np.ascontiguousarray(X['output'], dtype=np.float32) / 256.
	# output_ = np.ascontiguousarray(np.array(Image.open("test2.png").convert("RGB"), dtype=np.float32)[:, :, ::-1].transpose((2, 0, 1)), dtype=np.float32) / 256.
	input_ = np.ascontiguousarray(np.array(Image.open("./examples/input/in3.png").convert("RGB"), dtype=np.float32)[:, :, ::-1].transpose((2, 0, 1)), dtype=np.float32)/256.
	c, h, w = np.shape(input_)
	best = smooth_local_affine(output_, input_, 1e-7, 3, h, w, 15, 0.01).transpose(1, 2, 0)
	best_img = Image.fromarray(np.uint8(np.clip(best * 256, 0, 255.0)))
	best_img.save("./best2.png")

ColorTransferLib/Algorithms/DPT/closed_form_matting.py

from __future__ import division
import argparse
import os
import scipy.misc as spm
import scipy.ndimage as spi
import scipy.sparse as sps
import numpy as np
import tensorflow as tf

def getlaplacian1(i_arr, consts, epsilon=1e-5, win_rad=1):
    neb_size = (win_rad * 2 + 1) ** 2
    h, w, c = i_arr.shape
    img_size = w * h
    consts = spi.morphology.grey_erosion(consts, footprint=np.ones(shape=(win_rad * 2 + 1, win_rad * 2 + 1)))

    indsM = np.reshape(np.array(range(img_size)), newshape=(h, w), order='F')
    tlen = int((-consts[win_rad:-win_rad, win_rad:-win_rad] + 1).sum() * (neb_size ** 2))
    row_inds = np.zeros(tlen)
    col_inds = np.zeros(tlen)
    vals = np.zeros(tlen)
    l = 0
    for j in range(win_rad, w - win_rad):
        for i in range(win_rad, h - win_rad):
            if consts[i, j]:
                continue
            win_inds = indsM[i - win_rad:i + win_rad + 1, j - win_rad: j + win_rad + 1]
            win_inds = win_inds.ravel(order='F')
            win_i = i_arr[i - win_rad:i + win_rad + 1, j - win_rad: j + win_rad + 1, :]
            win_i = win_i.reshape((neb_size, c), order='F')
            win_mu = np.mean(win_i, axis=0).reshape(c, 1)
            win_var = np.linalg.inv(
                np.matmul(win_i.T, win_i) / neb_size - np.matmul(win_mu, win_mu.T) + epsilon / neb_size * np.identity(
                    c))

            win_i2 = win_i - np.repeat(win_mu.transpose(), neb_size, 0)
            tvals = (1 + np.matmul(np.matmul(win_i2, win_var), win_i2.T)) / neb_size

            ind_mat = np.broadcast_to(win_inds, (neb_size, neb_size))
            row_inds[l: (neb_size ** 2 + l)] = ind_mat.ravel(order='C')
            col_inds[l: neb_size ** 2 + l] = ind_mat.ravel(order='F')
            vals[l: neb_size ** 2 + l] = tvals.ravel(order='F')
            l += neb_size ** 2

    vals = vals.ravel(order='F')[0: l]
    row_inds = row_inds.ravel(order='F')[0: l]
    col_inds = col_inds.ravel(order='F')[0: l]
    a_sparse = sps.csr_matrix((vals, (row_inds, col_inds)), shape=(img_size, img_size))

    sum_a = a_sparse.sum(axis=1).T.tolist()[0]
    a_sparse = sps.diags([sum_a], [0], shape=(img_size, img_size)) - a_sparse

    return a_sparse

def getLaplacian(img):
    h, w, _ = img.shape
    coo = getlaplacian1(img, np.zeros(shape=(h, w)), 1e-5, 1).tocoo()
    indices = np.mat([coo.row, coo.col]).transpose()
    return tf.SparseTensor(indices, coo.data, coo.shape)

ColorTransferLib/Algorithms/CAM/CAM.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import time
from copy import deepcopy
import torch
import cv2

import ColorTransferLib.Algorithms.CAM.color_aware_st as cwst
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: CAMS: Color-Aware Multi-Style Transfer
#   Author: Mahmoud Afifi, Abdullah Abuolaim, Mostafa Hussien, Marcus A. Brubaker, Michael S. Brown
#   Published in: I...
#   Year of Publication: 2021
#
# Abstract:
#   Image style transfer aims to manipulate the appearance of a source image, or "content" image, to share similar
#   texture and colors of a target "style" image. Ideally, the style transfer manipulation should also preserve the
#   semantic content of the source image. A commonly used approach to assist in transferring styles is based on Gram
#   matrix optimization. One problem of Gram matrix-based optimization is that it does not consider the correlation
#   between colors and their styles. Specifically, certain textures or structures should be associated with specific
#   colors. This is particularly challenging when the target style image exhibits multiple style types. In this work,
#   we propose a color-aware multi-style transfer method that generates aesthetically pleasing results while preserving
#   the style-color correlation between style and generated images. We achieve this desired outcome by introducing a
#   simple but efficient modification to classic Gram matrix-based style transfer optimization. A nice feature of our
#   method is that it enables the users to manually select the color associations between the target style and content
#   image for more transfer flexibility. We validated our method with several qualitative comparisons, including a user
#   study conducted with 30 participants. In comparison with prior work, our method is simple, easy to implement, and
#   achieves visually appealing results when targeting images that have multiple styles. Source code is available at
#   this https URL.
#
# Info:
#   Name: CamsTransfer
#   Identifier: CAM
#   Link: https://doi.org/10.48550/arXiv.2106.13920
#   Source: https://github.com/mahmoudnafifi/color-aware-style-transfer
#
# Implementation Details:
#   in ComputeHistogram add small value to prevent division by zero when using images with small color depth
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class CAM:
    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "CamsTransfer",
            "title": "CAMS: Color-Aware Multi-Style Transfer",
            "year": 2021,
            "abstract": "Image style transfer aims to manipulate the appearance of a source image, or content image, "
                        "to share similar texture and colors of a target style image. Ideally, the style transfer "
                        "manipulation should also preserve the semantic content of the source image. A commonly used "
                        "approach to assist in transferring styles is based on Gram matrix optimization. One problem "
                        "of Gram matrix-based optimization is that it does not consider the correlation between colors "
                        "and their styles. Specifically, certain textures or structures should be associated with "
                        "specific colors. This is particularly challenging when the target style image exhibits "
                        "multiple style types. In this work, we propose a color-aware multi-style transfer method that "
                        "generates aesthetically pleasing results while preserving the style-color correlation between "
                        "style and generated images. We achieve this desired outcome by introducing a simple but "
                        "efficient modification to classic Gram matrix-based style transfer optimization. A nice "
                        "feature of our method is that it enables the users to manually select the color associations "
                        "between the target style and content image for more transfer flexibility. We validated our "
                        "method with several qualitative comparisons, including a user study conducted with 30 "
                        "participants. In comparison with prior work, our method is simple, easy to implement, and "
                        "achieves visually appealing results when targeting images that have multiple styles. Source "
                        "code is available at this https URL.",
            "types": ["Image", "Mesh"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, options):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, CAM.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        if not torch.cuda.is_available():
            options.device = "cpu"

        #ref.resize(src.get_width(), src.get_height())

        # Preprocessing
        src_img = src.get_raw() * 255.0
        ref_img = ref.get_raw() * 255.0
        out_img = deepcopy(src)

        ref_img = cv2.resize(ref_img, dsize=(src_img.shape[0], src_img.shape[1]), interpolation=cv2.INTER_CUBIC)

        out = cwst.apply(src_img, ref_img, options)

        out_img.set_raw(out, normalized=True)
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/DPT/vgg19/vgg.py

import os
import tensorflow as tf

import numpy as np
import time
import inspect

class Vgg19:
    def __init__(self, vgg19_npy_path=None):
        if vgg19_npy_path is None:
            path = inspect.getfile(Vgg19)
            path = os.path.abspath(os.path.join(path, os.pardir))
            path = os.path.join(path, "vgg19.npy")
            vgg19_npy_path = path

        #self.data_dict = np.load(vgg19_npy_path, encoding='latin1').item()

        # save np.load
        np_load_old = np.load
        # modify the default parameters of np.load
        np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)
        # call load_data with allow_pickle implicitly set to true
        #(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
        self.data_dict = np.load(vgg19_npy_path, encoding='latin1').item()
        # restore np.load for future normal usage
        np.load = np_load_old

    def build(self, bgr, clear_data=True):
        """
        load variable from npy to build the VGG
        """
        self.conv1_1 = self.conv_layer(bgr, "conv1_1")
        self.conv1_2 = self.conv_layer(self.conv1_1, "conv1_2")
        self.pool1 = self.max_pool(self.conv1_2, 'pool1')

        self.conv2_1 = self.conv_layer(self.pool1, "conv2_1")
        self.conv2_2 = self.conv_layer(self.conv2_1, "conv2_2")
        self.pool2 = self.max_pool(self.conv2_2, 'pool2')

        self.conv3_1 = self.conv_layer(self.pool2, "conv3_1")
        self.conv3_2 = self.conv_layer(self.conv3_1, "conv3_2")
        self.conv3_3 = self.conv_layer(self.conv3_2, "conv3_3")
        self.conv3_4 = self.conv_layer(self.conv3_3, "conv3_4")
        self.pool3 = self.max_pool(self.conv3_4, 'pool3')

        self.conv4_1 = self.conv_layer(self.pool3, "conv4_1")
        self.conv4_2 = self.conv_layer(self.conv4_1, "conv4_2")
        self.conv4_3 = self.conv_layer(self.conv4_2, "conv4_3")
        self.conv4_4 = self.conv_layer(self.conv4_3, "conv4_4")
        self.pool4 = self.max_pool(self.conv4_4, 'pool4')

        self.conv5_1 = self.conv_layer(self.pool4, "conv5_1")
        #self.conv5_2 = self.conv_layer(self.conv5_1, "conv5_2")
        #self.conv5_3 = self.conv_layer(self.conv5_2, "conv5_3")
        #self.conv5_4 = self.conv_layer(self.conv5_3, "conv5_4")
        #self.pool5 = self.max_pool(self.conv5_4, 'pool5')

        if clear_data:
            self.data_dict = None

    def get_all_layers(self):
        return [self.conv1_1, self.conv1_2, self.pool1,\
                self.conv2_1, self.conv2_2, self.pool2, \
                self.conv3_1, self.conv3_2, self.conv3_3, self.conv3_4, self.pool3, \
                self.conv4_1, self.conv4_2, self.conv4_3, self.conv4_4, self.pool4, \
                self.conv5_1]

    def avg_pool(self, bottom, name):
        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)

    def max_pool(self, bottom, name):
        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)

    def conv_layer(self, bottom, name):
        with tf.compat.v1.variable_scope(name):
            filt = self.get_conv_filter(name)
            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')

            conv_biases = self.get_bias(name)
            bias = tf.nn.bias_add(conv, conv_biases)

            relu = tf.nn.relu(bias)
            return relu

    def get_conv_filter(self, name):
        return tf.constant(self.data_dict[name][0], name="filter")

    def get_bias(self, name):
        return tf.constant(self.data_dict[name][1], name="biases")

ColorTransferLib/Algorithms/DPT/vgg19/init.py

from . import vgg

ColorTransferLib/Algorithms/EB3/init.py

from . import EB3

ColorTransferLib/Algorithms/FUZ/init.py

from . import FUZ

ColorTransferLib/Algorithms/GPC/init.py

from . import GPC

ColorTransferLib/Algorithms/GPC/GPC.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import time
from copy import deepcopy
import pyamg
from scipy.sparse import lil_matrix

from ColorTransferLib.ImageProcessing.Image import Image as Img
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Gradient-Preserving Color Transfer
#   Author: Xuezhong Xiao, Lizhuang Ma
#   Published in: IEEE Computer Graphics and Applications
#   Year of Publication: 2009
#
# Abstract:
#   Color transfer is an image processing technique which can produce a new image combining one source image s contents 
#   with another image s color style. While being able to produce convincing results, however, Reinhard et al. s 
#   pioneering work has two problems-mixing up of colors in different regions and the fidelity problem. Many local color 
#   transfer algorithms have been proposed to resolve the first problem, but the second problem was paid few attentions.
#   In this paper, a novel color transfer algorithm is presented to resolve the fidelity problem of color transfer in 
#   terms of scene details and colors. It s well known that human visual system is more sensitive to local intensity 
#   differences than to intensity itself. We thus consider that preserving the color gradient is necessary for scene 
#   fidelity. We formulate the color transfer problem as an optimization problem and solve it in two steps-histogram 
#   matching and a gradient-preserving optimization. Following the idea of the fidelity in terms of color and gradient, 
#   we also propose a metric for objectively evaluating the performance of example-based color transfer algorithms. The 
#   experimental results show the validity and high fidelity of our algorithm and that it can be used to deal with local 
#   color transfer.
#
# Info:
#   Name: GradientPreservingColorTransfer
#   Identifier: GPC
#   Link: https://doi.org/10.1111/j.1467-8659.2009.01566.x
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class GPC:
    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "GPC",
            "title": "Gradient-Preserving Color Transfer",
            "year": 2009,
            "abstract": "Color transfer is an image processing technique which can produce a new image combining one source imageâ€™s contents with another imageâ€™s color style. While being able to produce convincing results, however, Reinhard et al.â€™s pioneering work has two problemsâ€”mixing up of colors in different regions and the fidelity problem. Many local color transfer algorithms have been proposed to resolve the first problem, but the second problem was paid few attentions. In this paper, a novel color transfer algorithm is presented to resolve the fidelity problem of color transfer in terms of scene details and colors. Itâ€™s well known that human visual system is more sensitive to local intensity differences than to intensity itself. We thus consider that preserving the color gradient is necessary for scene fidelity. We formulate the color transfer problem as an optimization problem and solve it in two stepsâ€”histogram matching and a gradient-preserving optimization. Following the idea of the fidelity in terms of color and gradient, we also propose a metric for objectively evaluating the performance of example-based color transfer algorithms. The experimental results show the validity and high fidelity of our algorithm and that it can be used to deal with local color transfer.",
            "types": ["Image", "Mesh"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gradient_matrices(M, N):
        # Erstellt die Gradientenmatrizen Dx und Dy fÃ¼r ein Bild der GrÃ¶ÃŸe MxN
        size = M * N
        Dx = lil_matrix((size, size))
        Dy = lil_matrix((size, size))

        for i in range(size):
            # For the Sobel filter in x-direction:
            if (i-N-1) >= 0 and (i-N-1) % N != N-1: Dx[i, i-N-1] = 1
            #if i-N >= 0: Dx[i, i-N] = 0
            if (i-N+1) >= 0 and (i-N+1) % N != 0: Dx[i, i-N+1] = -1

            if (i-1) >= 0 and (i-1) % N != N-1: Dx[i, i-1] = 2
            #Dx[i, i] = 0
            if (i+1) < size and (i+1) % N != 0: Dx[i, i+1] = -2

            if (i+N-1) < size and ((i+N) % N)-1 >= 0: Dx[i, i+N-1] = 1
            #if i+N < size: Dx[i, i+N] = 0
            if (i+N+1) < size and ((i+N) % N)+1 < N: Dx[i, i+N+1] = -1

            # For the Sobel filter in y-direction:
            if (i-N-1) >= 0 and (i-N-1) % N != N-1: Dy[i, i-N-1] = 1
            if i-N >= 0: Dy[i, i-N] = 2
            if (i+1) < size and (i-N+1) >= 0 and (i-N+1) % N != 0: Dy[i, i-N+1] = 1

            #if (i-1) >= 0 and (i-1) % N != N-1: Dy[i, i-1] = 0
            #Dy[i, i] = 0
            #if (i+1) % N != 0: Dy[i, i+1] = 0

            if (i+N-1) < size and ((i+N) % N)-1 >= 0: Dy[i, i+N-1] = -1
            if i+N < size: Dy[i, i+N] = -2
            if (i+N+1) < size and ((i+N) % N)+1 < N: Dy[i, i+N+1] = -1

        Dx = Dx.tocsr()
        Dy = Dy.tocsr()

        return Dx, Dy   

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def solve_for_channel(channel_data_f, channel_data_s, M, N, lambda_val, Dx, Dy):
        size = M * N
        I = lil_matrix((size, size))
        I.setdiag(1)
        I = I.tocsr()

        A = I + lambda_val * (Dx.T @ Dx + Dy.T @ Dy)
        b = channel_data_f + (lambda_val * (Dx.T @ Dx + Dy.T @ Dy) @ channel_data_s)

        ml = pyamg.smoothed_aggregation_solver(A)
        o = ml.solve(b, tol=1e-10)

        return o.reshape((M, N))

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def histogram_matching(source, reference):
        matched = np.empty_like(source)
        for channel in range(source.shape[2]):
            matched[:,:,channel] = GPC.match_single_channel(source[:,:,channel], reference[:,:,channel])
        return matched

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def match_single_channel(source, reference):
        s_values, s_counts = np.unique(source, return_counts=True)
        r_values, r_counts = np.unique(reference, return_counts=True)

        s_quants = np.cumsum(s_counts).astype(np.float64)
        s_quants /= s_quants[-1]

        r_quants = np.cumsum(r_counts).astype(np.float64)
        r_quants /= r_quants[-1]

        interp_r_values = np.interp(s_quants, r_quants, r_values)

        return interp_r_values[np.searchsorted(s_values, source)]

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()

        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, GPC.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        #histogram matching
        matched_img = GPC.histogram_matching(src_img, ref_img)

        # original src size
        #size_src = (src.get_height(), src.get_width(), 3)

        out_img = deepcopy(src)
        out = out_img.get_colors()

        pad = 50

        M, N = src_img.shape[0]+2*pad, src_img.shape[1]+2*pad
        lambda_val = 1.0  # Setzen Sie hier den gewÃ¼nschten Wert fÃ¼r Lambda ein
        Dx, Dy = GPC.gradient_matrices(M, N)

        o_rgb = np.zeros((M, N, 3))

        # LÃ¶sen Sie die Gleichung fÃ¼r jeden Kanal separat
        matched_img = np.pad(matched_img, ((pad,pad),(pad,pad),(0,0)), "reflect")
        src_img = np.pad(src_img, ((pad,pad),(pad,pad),(0,0)), "reflect")
        for channel in range(3):
            o_rgb[:,:,channel] = GPC.solve_for_channel(matched_img[:,:,channel].flatten(), src_img[:,:,channel].flatten(), M, N, lambda_val, Dx, Dy)

        o_rgb = np.clip(o_rgb, 0, 1)
        o_rgb = o_rgb[pad:o_rgb.shape[0]-pad,pad:o_rgb.shape[1]-pad,:]

        out_img.set_colors(o_rgb)

        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/GLO/GLO.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import time
from copy import deepcopy

from ColorTransferLib.ImageProcessing.ColorSpaces import ColorSpaces
from ColorTransferLib.Utils.Helper import check_compatibility
from ColorTransferLib.ImageProcessing.Video import Video
from ColorTransferLib.MeshProcessing.VolumetricVideo import VolumetricVideo

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Color Transfer between Images
#   Author: Erik Reinhard, Michael Ashikhmin, Bruce Gooch, Peter Shirley
#   Published in: IEEE Computer Graphics and Applications
#   Year of Publication: 2001
#
# Abstract:
#   We use a simple statistical analysis to impose one image's color characteristics on another. We can achieve color
#   correction by choosing an appropriate source image and apply its characteristic to another image.
#
# Info:
#   Name: GlobalColorTransfer
#   Identifier: GLO
#   Link: https://doi.org/10.1109/38.946629
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class GLO:
    compatibility = {
        "src": ["Image", "Mesh", "PointCloud", "Video", "VolumetricVideo"],
        "ref": ["Image", "Mesh", "PointCloud", "Video", "VolumetricVideo"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    # Returns basic information of the corresponding publication.
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "GLO",
            "title": "Color Transfer between Images",
            "year": 2001,
            "abstract": "We use a simple statistical analysis to impose one images color characteristics on another. "
                        "We can achieve color correction by choosing an appropriate source image and apply its "
                        "characteristic to another image.",
            "types": ["Image", "Mesh", "PointCloud", "Video", "VolumetricVideo"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    # Applies the color transfer algorihtm
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()

        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, GLO.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # check if type is video
        out_colors_arr = []
        if src.get_type() == "Video" or src.get_type() == "VolumetricVideo":
            src_colors = src.get_colors()
        else:
            src_colors = [src.get_colors()]

        for i, src_color in enumerate(src_colors):
            # Preprocessing
            ref_color = ref.get_colors()

            if src.get_type() == "Video":
                out_img = deepcopy(src.get_images()[0])
            elif src.get_type() == "VolumetricVideo":
                out_img = deepcopy(src.get_meshes()[i])
            else:
                out_img = deepcopy(src)

            out_colors = out_img.get_colors()

            # [2] Convert RGB to lab color space
            if opt.colorspace == "lalphabeta":
                lab_src = ColorSpaces.rgb_to_lab_cpu(src_color)
                lab_ref = ColorSpaces.rgb_to_lab_cpu(ref_color)
            elif opt.colorspace == "rgb":
                lab_src = src_color
                lab_ref = ref_color

            # [3] Get mean, standard deviation and ratio of standard deviations
            mean_lab_src = np.mean(lab_src, axis=(0, 1))
            std_lab_src = np.std(lab_src, axis=(0, 1))
            mean_lab_ref = np.mean(lab_ref, axis=(0, 1))
            std_lab_ref = np.std(lab_ref, axis=(0, 1))

            device_div_std = std_lab_ref / std_lab_src

            # [4] Apply Global Color Transfer
            out_colors[:,:,0] = device_div_std[0] * (lab_src[:,:,0] - mean_lab_src[0]) + mean_lab_ref[0]
            out_colors[:,:,1] = device_div_std[1] * (lab_src[:,:,1] - mean_lab_src[1]) + mean_lab_ref[1]
            out_colors[:,:,2] = device_div_std[2] * (lab_src[:,:,2] - mean_lab_src[2]) + mean_lab_ref[2]

            # [5] Convert lab to RGB color space
            if opt.colorspace == "lalphabeta":
                out_colors = ColorSpaces.lab_to_rgb_cpu(out_colors)

            # [6] Clip color to range [0,1]
            out_colors = np.clip(out_colors, 0, 1)

            out_img.set_colors(out_colors)

            out_colors_arr.append(out_img)

        if src.get_type() == "Video":
            outp = Video(imgs=out_colors_arr)
        elif src.get_type() == "VolumetricVideo":
            outp = VolumetricVideo(meshes=out_colors_arr, file_name=src.get_file_name())
        else:
            outp = out_colors_arr[0]

        output = {
            "status_code": 0,
            "response": "",
            "object": outp,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/FUZ/FUZ.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
from fcmeans import FCM
import networkx as nx
import time
from copy import deepcopy

from ColorTransferLib.ImageProcessing.ColorSpaces import ColorSpaces
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: An efficient fuzzy clustering-based color transfer method
#   Author: XiaoYan Qian, BangFeng Wang, Lei Han
#   Published in: Seventh International Conference on Fuzzy Systems and Knowledge Discovery
#   Year of Publication: 2010
#
# Abstract:
#   Each image has its own color content that greatly influences the perception of human observer. Recently, color
#   transfer among different images has been under investigation. In this paper, after a brief review on the few
#   efficient works performed in the field, a novel fuzzy clustering based color transfer method is proposed. The
#   proposed method accomplishes the transformation based on a set of corresponding fuzzy clustering
#   algorithm-selected regions in images along with membership degree factors. Results show the presented algorithm is
#   highly automatically and more effective.
#
# Info:
#   Name: FuzzyColorTransfer
#   Identifier: FUZ
#   Link: https://doi.org/10.1109/FSKD.2010.5569560
#
# Implementation Details:
#   Number of Clusters: 3
#   Fuzzier: 2.0
#   Max Iterations: 100
#   Error: 1e-04
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class FUZ:
    compatibility = {
        "src": ["Image", "Mesh", "PointCloud"],
        "ref": ["Image", "Mesh", "PointCloud"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    # Returns basic information of the corresponding publication.
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "FuzzyColorTransfer",
            "title": "An efficient fuzzy clustering-based color transfer method",
            "year": 2010,
            "abstract": "Each image has its own color content that greatly influences the perception of human "
                        "observer. Recently, color transfer among different images has been under investigation. In "
                        "this paper, after a brief review on the few efficient works performed in the field, a novel "
                        "fuzzy clustering based color transfer method is proposed. The proposed method accomplishes "
                        "the transformation based on a set of corresponding fuzzy clustering algorithm-selected "
                        "regions in images along with membership degree factors. Results show the presented algorithm "
                        "is highly automatically and more effective.",
            "types": ["Image", "Mesh", "PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    # Applies the color transfer algorihtm
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        # Record the start time for performance measurement
        start_time = time.time()

        # Check if the source and reference images are compatible for color transfer
        output = check_compatibility(src, ref, FUZ.compatibility)

        # If not compatible, return the error output
        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Extract colors from the source and reference images
        src_color = src.get_colors()
        ref_color = ref.get_colors()
        out_img = deepcopy(src)

        # [1] Extract parameters needed for the algorithm
        src_pix_num = src_color.shape[0]
        ref_pix_num = ref_color.shape[0]
        dim = src_color.shape[2]
        clusters = opt.cluster_num
        max_iter = opt.max_iterations
        fuzzier = opt.fuzzier
        term_error = opt.error

        # [2] Convert colors from RGB to Lab color space
        src_color = ColorSpaces.rgb_to_lab_cpu(src_color)
        ref_color = ColorSpaces.rgb_to_lab_cpu(ref_color)

        # [3] reshaping to (num_pixels, 3) because input is of size (num_pixels, 1, 3)
        src_reshape = src_color.reshape(src_pix_num, dim)
        ref_reshape = ref_color.reshape(ref_pix_num, dim)

        # [4] Apply Fuzzy C-Means clustering to both source and reference colors
        fcm_src = FCM(n_clusters=clusters, max_iter=max_iter, m=fuzzier, error=term_error)
        fcm_src.fit(src_reshape)

        fcm_ref = FCM(n_clusters=clusters, max_iter=max_iter, m=fuzzier, error=term_error)
        fcm_ref.fit(ref_reshape)

        # [5] Calculate cluster directions
        membership_s = fcm_src.u
        norm_factor_s = np.sum(membership_s, axis=0)
        centers_s = fcm_src.centers

        std_s = np.zeros((clusters, 3))
        weights_s = np.zeros(clusters)
        for c in range(clusters):
            sig_l = np.sqrt(np.sum(membership_s[:,c:c+1] * np.power(src_reshape[:,0:1] - centers_s[c][0], 2) / norm_factor_s[c]))
            sig_a = np.sqrt(np.sum(membership_s[:,c:c+1] * np.power(src_reshape[:,1:2] - centers_s[c][1], 2) / norm_factor_s[c]))
            sig_b = np.sqrt(np.sum(membership_s[:,c:c+1] * np.power(src_reshape[:,2:3] - centers_s[c][2], 2) / norm_factor_s[c]))
            std_s[c] = np.array([sig_l, sig_a, sig_b])
            weights_s[c] = (1/3)*sig_l + (1/3)*sig_a + (1/3)*sig_b

        membership_r = fcm_ref.u
        norm_factor_r = np.sum(membership_r, axis=0)
        centers_r = fcm_ref.centers
        std_r = np.zeros((clusters, 3))
        weights_r = np.zeros(clusters)
        for c in range(clusters):
            sig_l = np.sqrt(np.sum(membership_r[:,c:c+1] * np.power(ref_reshape[:,0:1] - centers_r[c][0], 2) / norm_factor_r[c]))
            sig_a = np.sqrt(np.sum(membership_r[:,c:c+1] * np.power(ref_reshape[:,1:2] - centers_r[c][1], 2) / norm_factor_r[c]))
            sig_b = np.sqrt(np.sum(membership_r[:,c:c+1] * np.power(ref_reshape[:,2:3] - centers_r[c][2], 2) / norm_factor_r[c]))
            std_r[c] = np.array([sig_l, sig_a, sig_b])
            weights_r[c] = (1/3)*sig_l + (1/3)*sig_a + (1/3)*sig_b

        # [6] Calculate cluster directions by bipartite matching
        mapping = np.arange(clusters)
        B = nx.Graph()
        B.add_nodes_from(np.arange(clusters), bipartite=0)
        B.add_nodes_from(np.arange(2*clusters), bipartite=1)

        for ks, ws in enumerate(weights_s):
            for kr, wr in enumerate(weights_r):
                B.add_edge(ks, kr+clusters, weight=np.linalg.norm(ws-wr))

        my_matching = nx.bipartite.matching.minimum_weight_full_matching(B, np.arange(clusters), "weight")

        for i in range(clusters):
            mapping[i] = my_matching[i] - clusters

        # [7] Apply Reinhard's Color Transfer per cluster combination
        lab_new = np.zeros((dim, src_pix_num))

        for c in range(clusters):
            l_c = (std_r[mapping[c]][0]/std_s[c][0]) * (src_reshape[:,0:1] - centers_s[c][0]) + centers_r[mapping[c]][0]
            a_c = (std_r[mapping[c]][1]/std_s[c][1]) * (src_reshape[:,1:2] - centers_s[c][1]) + centers_r[mapping[c]][1]
            b_c = (std_r[mapping[c]][2]/std_s[c][2]) * (src_reshape[:,2:3] - centers_s[c][2]) + centers_r[mapping[c]][2]

            lab_new[0] += np.sum(l_c * membership_s[:,c:c+1], axis=1)
            lab_new[1] += np.sum(a_c * membership_s[:,c:c+1], axis=1)
            lab_new[2] += np.sum(b_c * membership_s[:,c:c+1], axis=1)

        lab_new = lab_new.T.reshape((src_pix_num, dim))

        # [8] Convert the resulting Lab colors back to RGB
        lab_new = lab_new.reshape(src_pix_num, 1, dim)
        lab_new = ColorSpaces.lab_to_rgb_cpu(lab_new)
        lab_new = np.clip(lab_new, 0.0, 1.0)

        # [9] Set the new colors to the output image
        out_img.set_colors(lab_new)

        # Prepare and return the output with performance metrics
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/HIS/HIS.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import time
import torch
from copy import deepcopy

from ColorTransferLib.Algorithms.HIS.models.models import create_model
from ColorTransferLib.Algorithms.HIS.data.data_loader import CreateDataLoader
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Deep Color Transfer using Histogram Analogy
#   Author: Junyong Lee, Hyeongseok Son, Gunhee Lee, Jonghyeop Lee, Sunghyun Cho, Seungyong Lee
#   Published in: The Visual Computer: International Journal of Computer Graphics, Volume 36, Issue 10-12Oct 2020
#   Year of Publication: 2020
#
# Abstract:
#   We propose a novel approach to transferring the color of a reference image to a given source image. Although there
#   can be diverse pairs of source and reference images in terms of content and composition similarity, previous methods
#   are not capable of covering the whole diversity. To resolve this limitation, we propose a deep neural network that
#   leverages color histogram analogy for color transfer. A histogram contains essential color information of an image,
#   and our network utilizes the analogy between the source and reference histograms to modulate the color of the source
#   image with abstract color features of the reference image. In our approach, histogram analogy is exploited basically
#   among the whole images, but it can also be applied to semantically corresponding regions in the case that the source
#   and reference images have similar contents with different compositions. Experimental results show that our approach
#   effectively transfers the reference colors to the source images in a variety of settings. We also demonstrate a few
#   applications of our approach, such as palette-based recolorization, color enhancement, and color editing.
#
# Info:
#   Name: HistogramAnalogy
#   Identifier: HIS
#   Link: https://doi.org/10.1007/s00371-020-01921-6
#   Sources: https://github.com/codeslake/Color_Transfer_Histogram_Analogy
#
# Implementation Details:
#   Restriction of max 700x700px was removed
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class HIS:
    identifier = "HIS"
    title = "Deep Color Transfer using Histogram Analogy"
    year = 2020

    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "HistogramAnalogy",
            "title": "Deep Color Transfer using Histogram Analogy",
            "year": 2020,
            "abstract": "We propose a novel approach to transferring the color of a reference image to a given source "
                        "image. Although there can be diverse pairs of source and reference images in terms of content "
                        "and composition similarity, previous methods are not capable of covering the whole diversity. "
                        "To resolve this limitation, we propose a deep neural network that leverages color histogram "
                        "analogy for color transfer. A histogram contains essential color information of an image, and "
                        "our network utilizes the analogy between the source and reference histograms to modulate the "
                        "color of the source image with abstract color features of the reference image. In our "
                        "approach, histogram analogy is exploited basically among the whole images, but it can also be "
                        "applied to semantically corresponding regions in the case that the source and reference "
                        "images have similar contents with different compositions. Experimental results show that our "
                        "approach effectively transfers the reference colors to the source images in a variety of "
                        "settings. We also demonstrate a few applications of our approach, such as palette-based "
                        "recolorization, color enhancement, and color editing.",
            "types": ["Image"]
        }

        return info
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, HIS.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        if not torch.cuda.is_available():
            opt.gpu_ids = [-1]

        # Preprocessing
        srcT = src.get_raw()
        refT = ref.get_raw()
        out_img = deepcopy(src)

        opt.checkpoints_dir = "Models/HIS"

        data_loader = CreateDataLoader(opt, srcT, refT)
        dataset = data_loader.load_data()

        # set gpu ids
        if len(opt.gpu_ids) > 0:
            torch.cuda.set_device(opt.gpu_ids[0])

        model = create_model(opt)
        opt.is_psnr = True

        model.set_input(dataset[0])
        model.test()

        visuals = model.get_current_visuals()
        ou = visuals["03_output"]
        ou = np.swapaxes(ou, 0, 1)
        ou = np.swapaxes(ou, 1, 2)

        out = ou.cpu().detach().numpy()

        out = out.astype(np.float32)
        out_img.set_raw(out, normalized=True)
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }
        return output

ColorTransferLib/Algorithms/HIS/LICENSE.txt

GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<http://www.gnu.org/licenses/>.

ColorTransferLib/Algorithms/HIS/data/init.py

ColorTransferLib/Algorithms/HIS/data/aligned_dataset_rand_seg_onlymap.py

import os.path
import torchvision.transforms as transforms
from ColorTransferLib.Algorithms.HIS.data.base_dataset import BaseDataset, get_transform, get_transform_lab, no_transform
from ColorTransferLib.Algorithms.HIS.data.image_folder import make_dataset
from PIL import Image
import random
import numpy as np

class AlignedDataset_Rand_Seg_onlymap(BaseDataset):
    def initialize(self, opt, src, ref):
        self.src = src
        self.ref = ref

        self.opt = opt
        self.root = opt.dataroot
        self.img_type = opt.img_type

        """
        self.dir_A =   os.path.join(opt.dataroot, 'input')
        self.dir_B =   os.path.join(opt.dataroot, 'target')
        self.dir_A_Map = os.path.join(opt.dataroot, 'seg_in')
        self.dir_B_Map = os.path.join(opt.dataroot, 'seg_tar')

        self.A_paths = make_dataset(self.dir_A)
        self.A_paths = sorted(self.A_paths)

        self.B_paths = make_dataset(self.dir_B)
        self.B_paths = sorted(self.B_paths)

        self.A_paths_map = make_dataset(self.dir_A_Map)
        self.A_paths_map = sorted(self.A_paths_map)

        self.B_paths_map = make_dataset(self.dir_B_Map)
        self.B_paths_map = sorted(self.B_paths_map)

        self.A_size = len(self.A_paths)
        self.B_size = len(self.B_paths)
        """
        # only one source and one reference image are processed
        self.A_size = 1
        self.B_size = 1

        self.transform_type = get_transform_lab(opt)
        self.transform_no = no_transform(opt)

    def __getitem__(self, index):
        #A_path = self.A_paths[index % self.A_size]
        #B_path = self.B_paths[index % self.B_size]
        #if self.opt.is_SR is True:
        #    A_path_map = self.A_paths_map[index % self.A_size]
        #    B_path_map = self.B_paths_map[index % self.B_size]

        #A_img = Image.open(A_path).convert('RGB')
        #B_img = Image.open(B_path).convert('RGB')
        A_img = self.src
        B_img = self.ref

        A = self.transform_type(A_img)
        B = self.transform_type(B_img)

        if self.opt.is_SR is False:
            A_map = np.zeros_like(np.array(A))
            B_map = np.zeros_like(np.array(B))
        #else:
        #    A_map=self.transform_no(Image.open(A_path_map))
        #    B_map=self.transform_no(Image.open(B_path_map))

        #return {'A': A, 'B': B, 'A_map': A_map, 'B_map': B_map,
        #        'A_paths': A_path, 'B_paths': B_path}

        return {'A': A, 'B': B, 'A_map': A_map, 'B_map': B_map,
                'A_paths': "", 'B_paths': ""}

    def __len__(self):
        return max(self.A_size, self.B_size)

    def name(self):
        return 'AlignedDataset_Rand_Seg_onlymap'

ColorTransferLib/Algorithms/HIS/data/custom_dataset_data_loader.py

import torch.utils.data
from ColorTransferLib.Algorithms.HIS.data.base_data_loader import BaseDataLoader

def CreateDataset(opt, src, ref):
    from ColorTransferLib.Algorithms.HIS.data.aligned_dataset_rand_seg_onlymap import AlignedDataset_Rand_Seg_onlymap
    dataset = AlignedDataset_Rand_Seg_onlymap()
    dataset.initialize(opt, src, ref)

    return dataset

class CustomDatasetDataLoader(BaseDataLoader):
    def name(self):
        return 'CustomDatasetDataLoader'

    def initialize(self, opt, src, ref):
        BaseDataLoader.initialize(self, opt)
        self.dataset = CreateDataset(opt, src, ref)
        self.dataloader = torch.utils.data.DataLoader(
            self.dataset,
            batch_size=1,
            shuffle=True)
        # self.dataloader = torch.utils.data.DataLoader(
        #     self.dataset,
        #     batch_size=opt.batchSize,
        #     shuffle=not opt.serial_batches,
        #     num_workers=int(opt.nThreads))

    def load_data(self):
        return self

    def __len__(self):
        return min(len(self.dataset), self.opt.max_dataset_size)

    def __iter__(self):
        for i, data in enumerate(self.dataloader):
            if i >= self.opt.max_dataset_size:
                break
            yield data

    def __getitem__(self, index):
        for i, data in enumerate(self.dataloader):
            if index == i:
                return data

ColorTransferLib/Algorithms/EB3/EB3.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
from sklearn.decomposition import PCA
from scipy.linalg import fractional_matrix_power
from copy import deepcopy

from ColorTransferLib.ImageProcessing.ColorSpaces import ColorSpaces
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Example-Based Colour Transfer for 3D Point Clouds
#   Author: Ific GoudÃ©, RÃ©mi Cozot, Olivier Le Meur, Kadi Bouatouch
#   Published in: Computer Graphics Forum, Volume 40
#   Year of Publication: 2021
#
# Abstract:
#   Example-based colour transfer between images, which has raised a lot of interest in the past decades, consists of
#   transferring the colour of an image to another one. Many methods based on colour distributions have been proposed,
#   and more recently, the efficiency of neural networks has been demonstrated again for colour transfer problems. In
#   this paper, we propose a new pipeline with methods adapted from the image domain to automatically transfer the
#   colour from a target point cloud to an input point cloud. These colour transfer methods are based on colour
#   distributions and account for the geometry of the point clouds to produce a coherent result. The proposed methods
#   rely on simple statistical analysis, are effective, and succeed in transferring the colour style from one point
#   cloud to another. The qualitative results of the colour transfers are evaluated and compared with existing methods.
#
# Link: https://doi.org/10.1111/cgf.14388
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class EB3:
    compatibility = {
        "src": ["PointCloud"],
        "ref": ["PointCloud"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "EB3",
            "title": "Example-Based Colour Transfer for 3D Point Clouds",
            "year": 2021,
            "abstract": "Example-based colour transfer between images, which has raised a lot of interest in the past "
                        "decades, consists of transferring the colour of an image to another one. Many methods based "
                        "on colour distributions have been proposed, and more recently, the efficiency of neural "
                        "networks has been demonstrated again for colour transfer problems. In this paper, we propose "
                        "a new pipeline with methods adapted from the image domain to automatically transfer the "
                        "colour from a target point cloud to an input point cloud. These colour transfer methods are "
                        "based on colour distributions and account for the geometry of the point clouds to produce a "
                        "coherent result. The proposed methods rely on simple statistical analysis, are effective, and "
                        "succeed in transferring the colour style from one point cloud to another. The qualitative "
                        "results of the colour transfers are evaluated and compared with existing methods.",
            "types": ["PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def IGD(src, ref, pca_enabled):
        # Convert colors from RGB to lalphabeta color space
        src_color_lab = ColorSpaces.rgb_to_lab_cpu(src.get_colors()).reshape(src.get_num_vertices(), 3)
        ref_color_lab = ColorSpaces.rgb_to_lab_cpu(ref.get_colors()).reshape(ref.get_num_vertices(), 3)

        # convert 3D normals to PCA adjusted 6D normals
        norma_src = src.get_normals().reshape(src.get_num_vertices(), 3)
        norma_ref = ref.get_normals().reshape(ref.get_num_vertices(), 3)

        if pca_enabled:
            pca = PCA(n_components=3)
            pca.fit(norma_src)
            src_pca_nx = np.einsum('ij, ij->i', norma_src, np.full((src.get_num_vertices(), 3), pca.components_[0]))
            src_pca_ny = np.einsum('ij, ij->i', norma_src, np.full((src.get_num_vertices(), 3), pca.components_[1]))
            src_pca_nz = np.einsum('ij, ij->i', norma_src, np.full((src.get_num_vertices(), 3), pca.components_[2]))
            norma_src = np.stack((src_pca_nx, src_pca_ny, src_pca_nz), axis=1)

            pca_ref = PCA(n_components=3)
            pca_ref.fit(norma_ref)
            ref_pca_nx = np.einsum('ij, ij->i', norma_ref, np.full((ref.get_num_vertices(), 3), pca_ref.components_[0]))
            ref_pca_ny = np.einsum('ij, ij->i', norma_ref, np.full((ref.get_num_vertices(), 3), pca_ref.components_[1]))
            ref_pca_nz = np.einsum('ij, ij->i', norma_ref, np.full((ref.get_num_vertices(), 3), pca_ref.components_[2]))
            norma_ref = np.stack((ref_pca_nx, ref_pca_ny, ref_pca_nz), axis=1)

        src_normal_6d = np.concatenate(
            (np.where(norma_src > 0.0, 0.0, np.abs(norma_src)), np.where(norma_src < 0.0, 0.0, np.abs(norma_src))),
            axis=1)
        ref_normal_6d = np.concatenate(
            (np.where(norma_ref > 0.0, 0.0, np.abs(norma_ref)), np.where(norma_ref < 0.0, 0.0, np.abs(norma_ref))),
            axis=1)

        # contains color and normal information per vertex -> [[cx, cy, cz, nx, ny, nz], ...]
        src_in_raw = np.concatenate((src_color_lab, src_normal_6d), axis=1).reshape(src.get_num_vertices(), 9)
        ref_in_raw = np.concatenate((ref_color_lab, ref_normal_6d), axis=1).reshape(ref.get_num_vertices(), 9)

        src_sum_normal = np.sum(src_in_raw[:, 3:], axis=0)
        ref_sum_normal = np.sum(ref_in_raw[:, 3:], axis=0)

        # calculate means and standard deviations
        src_mean_nx = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 3:4], src_in_raw[:, :3]), axis=0) / src_sum_normal[0]
        src_mean_ny = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 4:5], src_in_raw[:, :3]), axis=0) / src_sum_normal[1]
        src_mean_nz = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 5:6], src_in_raw[:, :3]), axis=0) / src_sum_normal[2]
        src_mean_px = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 6:7], src_in_raw[:, :3]), axis=0) / src_sum_normal[3]
        src_mean_py = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 7:8], src_in_raw[:, :3]), axis=0) / src_sum_normal[4]
        src_mean_pz = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 8:9], src_in_raw[:, :3]), axis=0) / src_sum_normal[5]

        src_std_nx = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 3:4], np.power(src_in_raw[:, :3] - src_mean_nx, 2)),
                            axis=0) / src_sum_normal[0]
        src_std_ny = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 4:5], np.power(src_in_raw[:, :3] - src_mean_ny, 2)),
                            axis=0) / src_sum_normal[1]
        src_std_nz = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 5:6], np.power(src_in_raw[:, :3] - src_mean_nz, 2)),
                            axis=0) / src_sum_normal[2]
        src_std_px = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 6:7], np.power(src_in_raw[:, :3] - src_mean_px, 2)),
                            axis=0) / src_sum_normal[3]
        src_std_py = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 7:8], np.power(src_in_raw[:, :3] - src_mean_py, 2)),
                            axis=0) / src_sum_normal[4]
        src_std_pz = np.sum(np.einsum('ik, ij->ij', src_in_raw[:, 8:9], np.power(src_in_raw[:, :3] - src_mean_pz, 2)),
                            axis=0) / src_sum_normal[5]

        ref_mean_nx = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 3:4], ref_in_raw[:, :3]), axis=0) / ref_sum_normal[0]
        ref_mean_ny = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 4:5], ref_in_raw[:, :3]), axis=0) / ref_sum_normal[1]
        ref_mean_nz = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 5:6], ref_in_raw[:, :3]), axis=0) / ref_sum_normal[2]
        ref_mean_px = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 6:7], ref_in_raw[:, :3]), axis=0) / ref_sum_normal[3]
        ref_mean_py = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 7:8], ref_in_raw[:, :3]), axis=0) / ref_sum_normal[4]
        ref_mean_pz = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 8:9], ref_in_raw[:, :3]), axis=0) / ref_sum_normal[5]

        ref_std_nx = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 3:4], np.power(ref_in_raw[:, :3] - ref_mean_nx, 2)),
                            axis=0) / ref_sum_normal[0]
        ref_std_ny = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 4:5], np.power(ref_in_raw[:, :3] - ref_mean_ny, 2)),
                            axis=0) / ref_sum_normal[1]
        ref_std_nz = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 5:6], np.power(ref_in_raw[:, :3] - ref_mean_nz, 2)),
                            axis=0) / ref_sum_normal[2]
        ref_std_px = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 6:7], np.power(ref_in_raw[:, :3] - ref_mean_px, 2)),
                            axis=0) / ref_sum_normal[3]
        ref_std_py = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 7:8], np.power(ref_in_raw[:, :3] - ref_mean_py, 2)),
                            axis=0) / ref_sum_normal[4]
        ref_std_pz = np.sum(np.einsum('ik, ij->ij', ref_in_raw[:, 8:9], np.power(ref_in_raw[:, :3] - ref_mean_pz, 2)),
                            axis=0) / ref_sum_normal[5]

        src_std_nx = np.sqrt(src_std_nx)
        src_std_ny = np.sqrt(src_std_ny)
        src_std_nz = np.sqrt(src_std_nz)
        src_std_px = np.sqrt(src_std_px)
        src_std_py = np.sqrt(src_std_py)
        src_std_pz = np.sqrt(src_std_pz)

        ref_std_nx = np.sqrt(ref_std_nx)
        ref_std_ny = np.sqrt(ref_std_ny)
        ref_std_nz = np.sqrt(ref_std_nz)
        ref_std_px = np.sqrt(ref_std_px)
        ref_std_py = np.sqrt(ref_std_py)
        ref_std_pz = np.sqrt(ref_std_pz)

        src_out_nx = (src_in_raw[:, :3] - src_mean_nx) * np.full((src.get_num_vertices(), 3),
                                                                 (ref_std_nx * np.reciprocal(src_std_nx))) + ref_mean_nx
        src_out_ny = (src_in_raw[:, :3] - src_mean_ny) * np.full((src.get_num_vertices(), 3),
                                                                 (ref_std_ny * np.reciprocal(src_std_ny))) + ref_mean_ny
        src_out_nz = (src_in_raw[:, :3] - src_mean_nz) * np.full((src.get_num_vertices(), 3),
                                                                 (ref_std_nz * np.reciprocal(src_std_nz))) + ref_mean_nz
        src_out_px = (src_in_raw[:, :3] - src_mean_px) * np.full((src.get_num_vertices(), 3),
                                                                 (ref_std_px * np.reciprocal(src_std_px))) + ref_mean_px
        src_out_py = (src_in_raw[:, :3] - src_mean_py) * np.full((src.get_num_vertices(), 3),
                                                                 (ref_std_py * np.reciprocal(src_std_py))) + ref_mean_py
        src_out_pz = (src_in_raw[:, :3] - src_mean_pz) * np.full((src.get_num_vertices(), 3),
                                                                 (ref_std_pz * np.reciprocal(src_std_pz))) + ref_mean_pz

        src_out_l_temp = np.concatenate((src_out_nx[:, 0:1], src_out_ny[:, 0:1], src_out_nz[:, 0:1], src_out_px[:, 0:1],
                                         src_out_py[:, 0:1], src_out_pz[:, 0:1]), axis=1)
        src_out_a_temp = np.concatenate((src_out_nx[:, 1:2], src_out_ny[:, 1:2], src_out_nz[:, 1:2], src_out_px[:, 1:2],
                                         src_out_py[:, 1:2], src_out_pz[:, 1:2]), axis=1)
        src_out_b_temp = np.concatenate((src_out_nx[:, 2:3], src_out_ny[:, 2:3], src_out_nz[:, 2:3], src_out_px[:, 2:3],
                                         src_out_py[:, 2:3], src_out_pz[:, 2:3]), axis=1)

        src_out_l = np.einsum('ij, ij-> i', src_normal_6d, src_out_l_temp)
        src_out_a = np.einsum('ij, ij-> i', src_normal_6d, src_out_a_temp)
        src_out_b = np.einsum('ij, ij-> i', src_normal_6d, src_out_b_temp)

        out = np.concatenate((src_out_l.reshape(src.get_num_vertices(), 1),
                              src_out_a.reshape(src.get_num_vertices(), 1),
                              src_out_b.reshape(src.get_num_vertices(), 1)), axis=1)

        # [8] Convert to RGB
        lab_new = out.reshape(src.get_num_vertices(), 1, 3)
        lab_new = ColorSpaces.lab_to_rgb_cpu(lab_new)
        lab_new = np.clip(lab_new, 0.0, 1.0)

        return lab_new

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def MGD(src, ref, pca_enabled):
        # Convert colors from RGB to lalphabeta color space
        src_color_lab = ColorSpaces.rgb_to_lab_cpu(src.get_colors()).reshape(src.get_num_vertices(), 3)
        ref_color_lab = ColorSpaces.rgb_to_lab_cpu(ref.get_colors()).reshape(ref.get_num_vertices(), 3)

        norma_src = src.get_vertex_normals().reshape(src.get_num_vertices(), 3)
        norma_ref = ref.get_vertex_normals().reshape(ref.get_num_vertices(), 3)

        if pca_enabled:
            pca = PCA(n_components=3)
            pca.fit(norma_src)
            src_pca_nx = np.einsum('ij, ij->i', norma_src, np.full((src.get_num_vertices(), 3), pca.components_[0]))
            src_pca_ny = np.einsum('ij, ij->i', norma_src, np.full((src.get_num_vertices(), 3), pca.components_[1]))
            src_pca_nz = np.einsum('ij, ij->i', norma_src, np.full((src.get_num_vertices(), 3), pca.components_[2]))
            norma_src = np.stack((src_pca_nx, src_pca_ny, src_pca_nz), axis=1)

            pca_ref = PCA(n_components=3)
            pca_ref.fit(norma_ref)
            ref_pca_nx = np.einsum('ij, ij->i', norma_ref, np.full((ref.get_num_vertices(), 3), pca_ref.components_[0]))
            ref_pca_ny = np.einsum('ij, ij->i', norma_ref, np.full((ref.get_num_vertices(), 3), pca_ref.components_[1]))
            ref_pca_nz = np.einsum('ij, ij->i', norma_ref, np.full((ref.get_num_vertices(), 3), pca_ref.components_[2]))
            norma_ref = np.stack((ref_pca_nx, ref_pca_ny, ref_pca_nz), axis=1)

        src_normal_6d = np.concatenate((np.where(norma_src > 0.0, 0.0, np.abs(norma_src)),
                                        np.where(norma_src < 0.0, 0.0, np.abs(norma_src))), axis=1)
        ref_normal_6d = np.concatenate((np.where(norma_ref > 0.0, 0.0, np.abs(norma_ref)),
                                        np.where(norma_ref < 0.0, 0.0, np.abs(norma_ref))),axis=1)

        # contains color and normal information per vertex -> [[cx, cy, cz, nx, ny, nz], ...]
        src_in_raw = np.concatenate((src_color_lab, src_normal_6d), axis=1).reshape(src.get_num_vertices(), 9)
        ref_in_raw = np.concatenate((ref_color_lab, ref_normal_6d), axis=1).reshape(ref.get_num_vertices(), 9)

        src_mean = np.mean(src_in_raw, axis=0)
        ref_mean = np.mean(ref_in_raw, axis=0)
        src_cov = np.cov(src_in_raw, rowvar=False)
        ref_cov = np.cov(ref_in_raw, rowvar=False)

        src_covs = fractional_matrix_power(src_cov, 0.5)
        src_covsr = fractional_matrix_power(src_cov, -0.5)

        M = np.dot(src_covsr, np.dot(fractional_matrix_power(np.dot(src_covs, np.dot(ref_cov, src_covs)), 0.5), src_covsr))
        f_out = np.dot(src_in_raw - src_mean, M) + ref_mean

        lab_new = f_out[:,:3]
        lab_new = lab_new.reshape(src.get_num_vertices(), 1, 3)
        lab_new = ColorSpaces.lab_to_rgb_cpu(np.ascontiguousarray(lab_new, dtype=np.float32))
        lab_new = np.clip(lab_new, 0.0, 1.0)

        return lab_new

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, EB3.compatibility)
        if output["status_code"] != 0:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        out_img = deepcopy(src)

        if opt.version == "IGD":
            out = EB3.IGD(src, ref, opt.pca)
        else:
            out = EB3.MGD(src, ref, opt.pca)

        out_img.set_colors(out)

        output = {
            "status_code": 0,
            "response": "",
            "object": out_img
        }

        return output

ColorTransferLib/Algorithms/HIS/data/base_dataset.py

import torch.utils.data as data
from PIL import Image
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
import numpy
from skimage import color

#Added
import time
from math import exp
import random

class BaseDataset(data.Dataset):
    def __init__(self):
        super(BaseDataset, self).__init__()

    def name(self):
        return 'BaseDataset'

    def initialize(self, opt):
        pass

def get_transform_filter_sat(opt):
    transform_list = []
    transform_list.append(transforms.Lambda(lambda img: Filter_syn(numpy.array(img),"random",1,1,1,1,time.time())))
    transform_list.append(transforms.Lambda(lambda img: RGB2LAB(numpy.array(img))))
    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_filter_red(opt):
    transform_list = [transforms.Resize(opt.fineSize),]
    transform_list.append(transforms.Lambda(lambda img: Filter_syn(numpy.array(img),"original",0.8,1.3,1,1,time.time())))
    transform_list.append(transforms.Lambda(lambda img: RGB2LAB(numpy.array(img))))
    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_filter_blue(opt):
    transform_list = [transforms.Resize(opt.fineSize),]
    transform_list.append(transforms.Lambda(lambda img: Filter_syn(numpy.array(img),"original",0.8,1,1,1.3,time.time())))
    transform_list.append(transforms.Lambda(lambda img: RGB2LAB(numpy.array(img))))
    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def Filter_syn(I,mode,w,x,y,z,start_time):

    # Preprocessing
    this_time=time.time()
    elapsed_time = this_time - start_time

    if mode == "random":
        w = ((elapsed_time*1000)%1000)*(1.5)/1000 # above 1
        if w > 1.3:
            w = 1.3
        if w <0.4:
            w = 0.4

    # Change Saturation
    hsv = color.rgb2hsv(I)
    h = (hsv[:, :, 0] / 360.0 ) * 255.0
    s = (hsv[:, :, 1] / 100.0 ) * 255.0 * (w)
    v = (hsv[:, :, 2] / 100.0 ) * 255.0

    r = (h / 255.0 ) * 360.0
    g = (s / 255.0 ) * 100.0
    b = (v / 255.0 ) * 100.0

    rgb = color.hsv2rgb(numpy.dstack([r, g, b]).astype(numpy.float64))

    # Change Color
    rgb[:,:,0] = rgb[:,:,0] * x
    rgb[:,:,1] = rgb[:,:,1] * y
    rgb[:,:,2] = rgb[:,:,2] * z

    # Post-processing
    rgb[:,:,0] = numpy.clip(rgb[:,:,0],0,1)
    rgb[:,:,1] = numpy.clip(rgb[:,:,1],0,1)
    rgb[:,:,2] = numpy.clip(rgb[:,:,2],0,1)

    return rgb

def no_transform(opt):
    transform_list = []
    #osize = [opt.loadSize, opt.loadSize]
    #transform_list.append(transforms.Scale(osize, Image.BICUBIC))
    #transform_list.append(transforms.CenterCrop(opt.fineSize))
    #transform_list.append(transforms.RandomCrop(opt.fineSize))
    #transform_list.append(transforms.Resize(interpolation=0.5))
    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_hsv(opt):
    transform_list = []

    transform_list.append(transforms.Lambda(lambda img: RGB2HSV(numpy.array(img))))
    transform_list.append(transforms.Lambda(lambda img: (numpy.array(img))))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5, 0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_lab(opt): # Now we are using
    transform_list = []
    transform_list.append(transforms.Lambda(lambda img: RGB2LAB(numpy.array(img))))
    #transform_list.append(transforms.Lambda(lambda img: LAB2RGB(numpy.array(img))))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_hueshiftlab(opt): # Now we are using
    start_time = time.time()

    transform_list = []
    transform_list.append(transforms.Lambda(lambda img: RGB2HSV_shift_LAB(numpy.array(img),start_time)))
    #transform_list.append(transforms.Lambda(lambda img: LAB2RGB(numpy.array(img))))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_hueshiftlab2(opt): # Now we are using
    start_time = time.time()

    transform_list = []
    transform_list.append(transforms.Lambda(lambda img: RGB2HSV_shift_LAB2(numpy.array(img),start_time)))
    #transform_list.append(transforms.Lambda(lambda img: LAB2RGB(numpy.array(img))))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_hueshiftlab_randomcrop(opt): # Now we are using
    start_time = time.time()

    transform_list = []
    transform_list.append(transforms.Lambda(lambda img, i, j, h, w: TF.crop(img, i, j, h, w)))
    transform_list.append(transforms.Lambda(lambda img: RGB2HSV_shift_LAB(numpy.array(img),start_time)))
    #transform_list.append(transforms.Lambda(lambda img: LAB2RGB(numpy.array(img))))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_hueshiftlab2_randomcrop(opt): # Now we are using
    start_time = time.time()

    transform_list = []
    transform_list.append(transforms.Lambda(lambda img, i, j, h, w: TF.crop(img, i, j, h, w)))
    transform_list.append(transforms.Lambda(lambda img: RGB2HSV_shift_LAB2(numpy.array(img),start_time)))
    #transform_list.append(transforms.Lambda(lambda img: LAB2RGB(numpy.array(img))))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_saturation(opt):
    transform_list = []

    #transform_list.append(transforms.Resize(interpolation=0.5))
    #osize = [opt.loadSize, opt.loadSize]
    #transform_list.append(transforms.Scale(osize, Image.BICUBIC))
    #transform_list.append(transforms.CenterCrop(opt.fineSize))
    #transform_list.append(transforms.RandomCrop(opt.fineSize))

    start_time = time.time()
    transform_list.append(transforms.Lambda(lambda img: RGB2HSV2RGB(numpy.array(img),"random",start_time)))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5, 0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def get_transform_grayish(opt):
    transform_list = []

    start_time = time.time()
    transform_list.append(transforms.Lambda(lambda img: RGB2HSV2RGB_Gray(numpy.array(img),"random",start_time)))

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5, 0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)
#def get_transform_lab2(opt):
#    transform_list = []
#
#    transform_list.append(transforms.Lambda(lambda img: RGB2HSV(numpy.array(img))))
#    transform_list.append(transforms.Lambda(lambda img: (numpy.array(img))))
#
#    transform_list += [transforms.ToTensor(),
#                       transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5, 0.5),
#                                            (0.5, 0.5, 0.5, 0.5, 0.5, 0.5))]
#    return transforms.Compose(transform_list)

def get_transform(opt):
    transform_list = []
    if opt.resize_or_crop == 'resize_and_crop':
        osize = [opt.loadSize, opt.loadSize]
        transform_list.append(transforms.Scale(osize, Image.BICUBIC))
        transform_list.append(transforms.RandomCrop(opt.fineSize))
    elif opt.resize_or_crop == 'crop':
        transform_list.append(transforms.RandomCrop(opt.fineSize))
    elif opt.resize_or_crop == 'scale_width':
        transform_list.append(transforms.Lambda(
            lambda img: __scale_width(img, opt.fineSize)))
    elif opt.resize_or_crop == 'scale_width_and_crop':
        transform_list.append(transforms.Lambda(
            lambda img: __scale_width(img, opt.loadSize)))
        transform_list.append(transforms.RandomCrop(opt.fineSize))

    if opt.isTrain and not opt.no_flip:
        transform_list.append(transforms.RandomHorizontalFlip())

    transform_list += [transforms.ToTensor(),
                       transforms.Normalize((0.5, 0.5, 0.5),
                                            (0.5, 0.5, 0.5))]
    return transforms.Compose(transform_list)

def __scale_width(img, target_width):
    ow, oh = img.size
    if (ow == target_width):
        return img
    w = target_width
    h = int(target_width * oh / ow)
    return img.resize((w, h), Image.BICUBIC)

def RGB2LAB(I):
    # AB 98.2330538631 -86.1830297444 94.4781222765 -107.857300207
    lab = color.rgb2lab(I)
    l = (lab[:, :, 0] / 100.0) #* 255.0    # L component ranges from 0 to 100
    a = (lab[:, :, 1] + 86.1830297444) / (98.2330538631 + 86.1830297444) #* 255.0         # a component ranges from -127 to 127
    b = (lab[:, :, 2] + 107.857300207) / (94.4781222765 + 107.857300207) #* 255.0         # b component ranges from -127 to 127
    #l = (lab[:, :, 0] / 100.0) * 255.0    # L component ranges from 0 to 100
    #a = (lab[:, :, 1] + 86.1830297444) / (98.2330538631 + 86.1830297444) * 255.0         # a component ranges from -127 to 127
    #b = (lab[:, :, 2] + 107.857300207) / (94.4781222765 + 107.857300207) * 255.0         # b component ranges from -127 to 127
    return numpy.dstack([l, a, b])

def LAB2RGB(I):
    l = I[:, :, 0] / 255.0 * 100.0
    a = I[:, :, 1] / 255.0 * (98.2330538631 + 86.1830297444) - 86.1830297444
    b = I[:, :, 2] / 255.0 * (94.4781222765 + 107.857300207) - 107.857300207

    rgb = color.lab2rgb(numpy.dstack([l, a, b]).astype(numpy.float64))
    return rgb

def RGB2HSV(I):
    hsv = color.rgb2hsv(I)
    h = (hsv[:, :, 0] / 360.0 ) * 255.0
    s = (hsv[:, :, 1] / 100.0 ) * 255.0
    v = (hsv[:, :, 2] / 100.0 ) * 255.0
    return numpy.dstack([h, s, v])

def RGB2HSV_shift_LAB(I,start_time): # shift value0 ~1
    this_time=time.time()
    elapsed_time=this_time-start_time
    shift = ((elapsed_time*1000)%1000)*(1.0)/1000 # above 1
    shift2 = ((elapsed_time*10000)%10000)*(1.2)/10000 # above 1 # Added at 'colorhistogram_noGAN_lr00005_lab_hueshift_histenc_histloss_satadded' 
    if shift2 < 0.3:
        shift2 = 0.3
    # Get Original L in LAB, shift H in HSV

    # Get Original LAB
    lab_original = color.rgb2lab(I)
    l_original = (lab_original[:, :, 0] / 100.0)

    # Shift HSV
    hsv = color.rgb2hsv(I)
    h = ((hsv[:, :, 0] + shift))
    s = (hsv[:, :, 1]) * shift2
    v = (hsv[:, :, 2])
    hsv2 = color.hsv2rgb(numpy.dstack([h, s, v]).astype(numpy.float64))

    # Merge (Original LAB, Shifted HSV)
    lab = color.rgb2lab(hsv2)
    l = l_original
    #l = (lab[:, :, 0] / 100.0)
    a = (lab[:, :, 1] + 86.1830297444) / (98.2330538631 + 86.1830297444) #* 255.0         # a component ranges from -127 to 127
    b = (lab[:, :, 2] + 107.857300207) / (94.4781222765 + 107.857300207) #* 255.0         # b component ranges from -127 to 127

    return numpy.dstack([l, a, b])

def RGB2HSV_shift_LAB2(I,start_time): # shift value0 ~1
    this_time=time.time()
    elapsed_time=this_time-start_time
    shift = ((elapsed_time*10000)%10000)*(1.0)/10000 # above 1
    shift2 = ((elapsed_time*10000)%10000)*(1.2)/10000 # above 1 # Added at 'colorhistogram_noGAN_lr00005_lab_hueshift_histenc_histloss_satadded'
    if shift2 < 0.3:
        shift2 = 0.3

    # Get Original L in LAB, shift H in HSV

    # Get Original LAB
    lab_original = color.rgb2lab(I)
    l_original = (lab_original[:, :, 0] / 100.0)

    # Shift HSV
    hsv = color.rgb2hsv(I)
    h = ((hsv[:, :, 0] + shift))
    s = (hsv[:, :, 1]) * shift2
    v = (hsv[:, :, 2])
    hsv2 = color.hsv2rgb(numpy.dstack([h, s, v]).astype(numpy.float64))

    # Merge (Original LAB, Shifted HSV)
    lab = color.rgb2lab(hsv2)
    l = l_original
    #l = (lab[:, :, 0] / 100.0)
    a = (lab[:, :, 1] + 86.1830297444) / (98.2330538631 + 86.1830297444) #* 255.0         # a component ranges from -127 to 127
    b = (lab[:, :, 2] + 107.857300207) / (94.4781222765 + 107.857300207) #* 255.0         # b component ranges from -127 to 127

    return numpy.dstack([l, a, b])

def RGB2HSV2RGB(I,mode,start_time):

    this_time=time.time()
    elapsed_time=this_time-start_time

    if mode == "original":
        alpha = 1
    elif mode == "random":
        ###### Gray Added
        alpha = ((elapsed_time*1000)%1000)*(1.5)/1000 # above 1

        if alpha > 1.0:
            alpha = 1.0
        if alpha <0.2:
            alpha = 0.2

    elif mode == "gray":
        alpha = 0.1
    elif mode == "decay":
        alpha = 1/(1+exp(0.0003*(elapsed_time-20000)))*0.9 + 0.1
    else:
        alpha = 1

    hsv = color.rgb2hsv(I)
    h = (hsv[:, :, 0] / 360.0 ) * 255.0
    s = (hsv[:, :, 1] / 100.0 ) * 255.0 * alpha
    v = (hsv[:, :, 2] / 100.0 ) * 255.0

    r = (h / 255.0 ) * 360.0
    g = (s / 255.0 ) * 100.0
    b = (v / 255.0 ) * 100.0

    rgb = color.hsv2rgb(numpy.dstack([r, g, b]).astype(numpy.float64))

    return rgb

def RGB2HSV2RGB_Gray(I,mode,start_time):

    hsv = color.rgb2hsv(I)
    h = (hsv[:, :, 0] / 360.0 ) * 255.0
    s = (hsv[:, :, 1] / 100.0 ) * 255.0 * 0.4
    v = (hsv[:, :, 2] / 100.0 ) * 255.0

    r = (h / 255.0 ) * 360.0
    g = (s / 255.0 ) * 100.0
    b = (v / 255.0 ) * 100.0

    rgb = color.hsv2rgb(numpy.dstack([r, g, b]).astype(numpy.float64))

    return rgb

ColorTransferLib/Algorithms/HIS/data/base_data_loader.py

class BaseDataLoader():
    def __init__(self):
        pass

    def initialize(self, opt):
        self.opt = opt
        pass

    def load_data():
        return None

ColorTransferLib/Algorithms/GLO/init.py

from . import GLO

ColorTransferLib/Algorithms/HIS/data/data_loader.py

def CreateDataLoader(opt, src, ref):
    from ColorTransferLib.Algorithms.HIS.data.custom_dataset_data_loader import CustomDatasetDataLoader
    data_loader = CustomDatasetDataLoader()
    data_loader.initialize(opt, src, ref)
    return data_loader

ColorTransferLib/Algorithms/HIS/data/image_folder.py

###############################################################################
# Code from
# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py
# Modified the original code so that it also loads images from the current
# directory as well as the subdirectories
###############################################################################

import torch.utils.data as data

from PIL import Image
import os
import os.path

IMG_EXTENSIONS = [
    '.jpg', '.JPG', '.jpeg', '.JPEG',
    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.t7',
]

def is_image_file(filename):
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)

def make_dataset(dir):
    images = []
    assert os.path.isdir(dir), '%s is not a valid directory' % dir

    for root, _, fnames in sorted(os.walk(dir)):
        for fname in fnames:
            if is_image_file(fname):
                path = os.path.join(root, fname)
                images.append(path)

    return images

def default_loader(path):
    return Image.open(path).convert('RGB')

class ImageFolder(data.Dataset):

    def __init__(self, root, transform=None, return_paths=False,
                 loader=default_loader):
        imgs = make_dataset(root)
        if len(imgs) == 0:
            raise(RuntimeError("Found 0 images in: " + root + "\n"
                               "Supported image extensions are: " +
                               ",".join(IMG_EXTENSIONS)))

        self.root = root
        self.imgs = imgs
        self.transform = transform
        self.return_paths = return_paths
        self.loader = loader

    def __getitem__(self, index):
        path = self.imgs[index]
        img = self.loader(path)
        if self.transform is not None:
            img = self.transform(img)
        if self.return_paths:
            return img, path
        else:
            return img

    def __len__(self):
        return len(self.imgs)

ColorTransferLib/Algorithms/HIS/data/single_dataset.py

import os.path
import torchvision.transforms as transforms
from data.base_dataset import BaseDataset, get_transform
from data.image_folder import make_dataset
from PIL import Image

class SingleDataset(BaseDataset):
    def initialize(self, opt):
        self.opt = opt
        self.root = opt.dataroot
        self.dir_A = os.path.join(opt.dataroot)

        self.A_paths = make_dataset(self.dir_A)

        self.A_paths = sorted(self.A_paths)

        self.transform = get_transform(opt)

    def __getitem__(self, index):
        A_path = self.A_paths[index]
        A_img = Image.open(A_path).convert('RGB')
        A = self.transform(A_img)
        if self.opt.which_direction == 'BtoA':
            input_nc = self.opt.output_nc
        else:
            input_nc = self.opt.input_nc

        if input_nc == 1:  # RGB to gray
            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114
            A = tmp.unsqueeze(0)

        return {'A': A, 'A_paths': A_path}

    def __len__(self):
        return len(self.A_paths)

    def name(self):
        return 'SingleImageDataset'

ColorTransferLib/Algorithms/HIS/init.py

from . import HIS

ColorTransferLib/Algorithms/HIS/data/unaligned_dataset.py

import os.path
import torchvision.transforms as transforms
from data.base_dataset import BaseDataset, get_transform
from data.image_folder import make_dataset
from PIL import Image
import PIL
import random

class UnalignedDataset(BaseDataset):
    def initialize(self, opt):
        self.opt = opt
        self.root = opt.dataroot
        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')
        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')

        self.A_paths = make_dataset(self.dir_A)
        self.B_paths = make_dataset(self.dir_B)

        self.A_paths = sorted(self.A_paths)
        self.B_paths = sorted(self.B_paths)
        self.A_size = len(self.A_paths)
        self.B_size = len(self.B_paths)
        self.transform = get_transform(opt)

    def __getitem__(self, index):
        A_path = self.A_paths[index % self.A_size]
        index_A = index % self.A_size
        if self.opt.serial_batches:
            index_B = index % self.B_size
        else:
            index_B = random.randint(0, self.B_size - 1)
        B_path = self.B_paths[index_B]
        # print('(A, B) = (%d, %d)' % (index_A, index_B))
        A_img = Image.open(A_path).convert('RGB')
        B_img = Image.open(B_path).convert('RGB')

        A = self.transform(A_img)
        B = self.transform(B_img)
        if self.opt.which_direction == 'BtoA':
            input_nc = self.opt.output_nc
            output_nc = self.opt.input_nc
        else:
            input_nc = self.opt.input_nc
            output_nc = self.opt.output_nc

        if input_nc == 1:  # RGB to gray
            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114
            A = tmp.unsqueeze(0)

        if output_nc == 1:  # RGB to gray
            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114
            B = tmp.unsqueeze(0)
        return {'A': A, 'B': B,
                'A_paths': A_path, 'B_paths': B_path}

    def __len__(self):
        return max(self.A_size, self.B_size)

    def name(self):
        return 'UnalignedDataset'

ColorTransferLib/Algorithms/HIS/models/base_model.py

import os
import torch

class BaseModel():
    def name(self):
        return 'BaseModel'

    def initialize(self, opt):
        self.opt = opt
        self.gpu_ids = opt.gpu_ids
        self.isTrain = opt.isTrain
        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor
        self.save_dir = opt.checkpoints_dir

    def set_input(self, input):
        self.input = input

    def forward(self):
        pass

    # used in test time, no backprop
    def test(self):
        pass

    def get_image_paths(self):
        pass

    def optimize_parameters(self):
        pass

    def get_current_visuals(self):
        return self.input

    def get_current_errors(self):
        return {}

    def save(self, label):
        pass

    def load_network(self, network, network_label, epoch_label=None, ckpt_name=None):
        if ckpt_name is None:
            save_filename = '%s_net_%s.pth' % (epoch_label, network_label)
        elif ckpt_name is not None:
            save_filename = '%s_net_%s.pth' % (ckpt_name, network_label)
        save_path = os.path.join(self.save_dir, save_filename)

        network.load_state_dict(torch.load(save_path))

ColorTransferLib/Algorithms/HIS/models/init.py

ColorTransferLib/Algorithms/HIS/models/colorhistogram_model.py

import numpy as np
import torch
import os
from collections import OrderedDict
from torch.autograd import Variable
import itertools
import ColorTransferLib.Algorithms.HIS.util.util as util
from ColorTransferLib.Algorithms.HIS.util.image_pool import ImagePool
from ColorTransferLib.Algorithms.HIS.models.base_model import BaseModel
from ColorTransferLib.Algorithms.HIS.models import networks
import sys
#from torch.utils.serialization import load_lua
import torch.nn as nn
import torchvision
import random
import cv2
import torchfile
import torch.nn.functional as F
import copy
import seaborn as sns
import matplotlib.pyplot as plt

class ColorHistogram_Model(BaseModel):
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def name(self):
        return 'ColorHistogram_Model'
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def initialize(self, opt):
        BaseModel.initialize(self, opt)

        nb = opt.batchSize
        size = opt.fineSize 

        self.hist_l = opt.l_bin
        self.hist_ab = opt.ab_bin
        self.img_type = opt.img_type
        self.pad = 30
        self.reppad = nn.ReplicationPad2d(self.pad)

        self.IRN = networks.IRN(3, 3, opt.ngf, opt.network, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids)

        if not torch.cuda.is_available():
            self.HEN = networks.HEN((self.hist_l+1), 64, opt.ngf, opt.network_H, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids).cpu()
        else:
            self.HEN = networks.HEN((self.hist_l+1), 64, opt.ngf, opt.network_H, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids).cuda()

        if not self.isTrain or opt.continue_train:
            which_epoch = opt.which_epoch
            self.load_network(self.IRN, 'G_A', which_epoch)
            self.load_network(self.HEN, 'C_A', which_epoch)
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def set_input(self, input):
        AtoB = self.opt.which_direction == 'AtoB'

        input_A = input['A']
        input_B = input['B']
        input_A_Map = input['A_map']
        input_B_Map = input['B_map']

        if len(self.gpu_ids) > 0:
            if torch.cuda.is_available():
                input_A = input_A.cuda(self.gpu_ids[0])
                input_B = input_B.cuda(self.gpu_ids[0])

        self.input_A = input_A
        self.input_B = input_B

        self.input_A_Map = input_A_Map
        self.input_B_Map = input_B_Map
        self.input_A_Seg, self.input_B_Seg, self.input_SegNum = self.MakeLabelFromMap(self.input_A_Map,self.input_B_Map)

        self.image_paths = input['A_paths' if AtoB else 'B_paths']
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def forward(self):
        self.inp = self.input_A
        self.tar = self.input_B
        self.A_seg = self.input_A_Seg
        self.B_seg = self.input_B_Seg
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def test(self):
        with torch.no_grad():

            self.inp = Variable(self.input_A)
            self.tar = Variable(self.input_B)

            self.A_seg = Variable(self.input_A_Seg)
            self.B_seg = Variable(self.input_B_Seg)
            self.A_map = Variable(self.input_A_Map)

            # INFO: Removed to ensure a color transfer for images with a size greater than 700x700px
            # # max 700 px with aspect ratio
            # if (self.inp.size(2)>700) or (self.inp.size(3)>700):
            #     aspect_ratio = self.inp.size(2) / self.inp.size(3)
            #     if (self.inp.size(2) > self.inp.size(3)):
            #         self.inp = F.upsample(self.inp , size = ( 700 ,  int(700 / aspect_ratio)), mode = 'bilinear')
            #     else:
            #         self.inp = F.upsample(self.inp , size = ( int(700 * aspect_ratio),  700 ), mode = 'bilinear')
            # # max 700 px with aspect ratio
            # if (self.tar.size(2)>700) or (self.tar.size(3)>700):
            #     aspect_ratio = self.tar.size(2) / self.tar.size(3)
            #     if (self.tar.size(2) > self.tar.size(3)):
            #         self.tar = F.upsample(self.tar , size = ( 700 ,  int(700 / aspect_ratio)), mode = 'bilinear')
            #     else:
            #         self.tar = F.upsample(self.tar , size = ( int(700 * aspect_ratio),  700 ), mode = 'bilinear')

            # In case of mis-alignment
            self.A_seg = F.upsample(self.A_seg.unsqueeze(0).float() , size = (self.inp.size(2),  self.inp.size(3)),  mode = 'bilinear').squeeze(0).long()
            self.B_seg = F.upsample(self.B_seg.unsqueeze(0).float() , size = (self.tar.size(2),  self.tar.size(3)),  mode = 'bilinear').squeeze(0).long()

            self.inp = self.inp.float()
            self.tar = self.tar.float()

            ## HEN ##
            with torch.no_grad():
                hist_inp_ab = self.getHistogram2d_np(self.inp, self.hist_ab)
                hist_inp_l =  self.getHistogram1d_np(self.inp, self.hist_l).repeat(1,1,self.hist_ab,self.hist_ab)
                hist_inp = torch.cat((hist_inp_ab,hist_inp_l),1)

                hist_tar_ab =  self.getHistogram2d_np(self.tar, self.hist_ab)
                hist_tar_l  =  self.getHistogram1d_np(self.tar, self.hist_l).repeat(1,1,self.hist_ab,self.hist_ab)
                hist_tar = torch.cat((hist_tar_ab,hist_tar_l),1) 

                hist_inp_feat  = self.HEN(hist_inp)
                hist_tar_feat  = self.HEN(hist_tar)

                hist_inp_feat_tile =    hist_inp_feat.repeat(1,1,self.inp.size(2),self.inp.size(3))
                hist_tar_feat_tile =    hist_tar_feat.repeat(1,1,self.tar.size(2),self.tar.size(3))
                self.final_result_tar = hist_tar_feat.repeat(1,1,self.inp.size(2),self.inp.size(3))

            if self.opt.is_SR:
                for i in range(0, self.input_SegNum):
                    seg_num_A = torch.sum(torch.sum(self.A_seg == i ,1),1)
                    seg_num_B = torch.sum(torch.sum(self.B_seg == i ,1),1)
                    if (seg_num_A > 0) and (seg_num_B > 0):
                        self.segmentwise_tile( self.tar, self.B_seg, self.A_seg, self.final_result_tar, i)

            # Padding
            self.inp   = self.reppad(self.inp)
            self.inp_H = self.reppad(hist_inp_feat_tile)
            self.tar_H = self.reppad(hist_tar_feat_tile)

            # Network Fake
            if self.opt.is_SR:
                self.tar_H_SR  = self.reppad(self.final_result_tar)
                _, _, _, _, out = self.IRN(self.inp, self.inp_H , self.tar_H_SR)
            else:
                _, _, _, _, out = self.IRN(self.inp, self.inp_H , self.tar_H )

            self.inp = self.inp[:,:,self.pad:(self.inp.size(2)-2*self.pad),self.pad:(self.inp.size(3)-2*self.pad)]
            self.tar = self.tar[:,:,self.pad:(self.tar.size(2)-2*self.pad),self.pad:(self.tar.size(3)-2*self.pad)]
            #self.out = out[:,:,self.pad:(out.size(2)-2*self.pad),self.pad:(out.size(3)-2*self.pad)]
            self.out = out[:,:,self.pad:(out.size(2)-self.pad),self.pad:(out.size(3)-self.pad)]

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def get_current_visuals(self):
        ret_visuals = OrderedDict([('01_input', util.tensor2im(self.inp,self.img_type)),
                                   ('02_target', util.tensor2im(self.tar,self.img_type)),
                                   ('03_output', util.tensor2im(self.out,self.img_type))])

        return ret_visuals

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def getHistogram2d_np(self, img_torch, num_bin):
        arr = img_torch.detach().cpu().numpy()

        # Exclude Zeros and Make value 0 ~ 1
        arr1 = ( arr[0][1].ravel()[np.flatnonzero(arr[0][1])] + 1 ) /2
        arr2 = ( arr[0][2].ravel()[np.flatnonzero(arr[0][2])] + 1 ) /2

        if (arr1.shape[0] != arr2.shape[0]):
            if arr2.shape[0] < arr1.shape[0]:
                arr2 = np.concatenate([arr2, np.array([0])])
            else:
                arr1 = np.concatenate([arr1, np.array([0])])

        # AB space
        arr_new = [arr1, arr2]

        H,edges = np.histogramdd(arr_new, bins = [num_bin, num_bin], range = ((0,1),(0,1)))

        H = np.rot90(H)
        H = np.flip(H,0)

        if not torch.cuda.is_available():
            H_torch = torch.from_numpy(H).float().cpu() #10/224/224
        else:
            H_torch = torch.from_numpy(H).float().cuda() #10/224/224
        H_torch = H_torch.unsqueeze(0).unsqueeze(0)

        # Normalize
        total_num = sum(sum(H_torch.squeeze(0).squeeze(0))) # 256 * 256 => same value as arr[0][0].ravel()[np.flatnonzero(arr[0][0])].shape
        H_torch = H_torch / total_num

        return H_torch

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def getHistogram1d_np(self,img_torch, num_bin): # L space # Idon't know why but they(np, conv) are not exactly same
        # Preprocess
        arr = img_torch.detach().cpu().numpy()
        arr0 = ( arr[0][0].ravel()[np.flatnonzero(arr[0][0])] + 1 ) / 2 
        arr1 = np.zeros(arr0.size)

        arr_new = [arr0, arr1]
        H, edges = np.histogramdd(arr_new, bins = [num_bin, 1], range =((0,1),(-1,2)))

        if not torch.cuda.is_available():
            H_torch = torch.from_numpy(H).float().cpu() #10/224/224
        else:
            H_torch = torch.from_numpy(H).float().cuda() #10/224/224
        H_torch = H_torch.unsqueeze(0).unsqueeze(0).permute(0,2,1,3)

        total_num = sum(sum(H_torch.squeeze(0).squeeze(0))) # 256 * 256 => same value as arr[0][0].ravel()[np.flatnonzero(arr[0][0])].shape
        H_torch = H_torch / total_num

        return H_torch

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def segmentwise_tile(self, img, seg_src, seg_tgt, final_tensor,segment_num):

        # Mask only Specific Segmentation
        if not torch.cuda.is_available():
            mask_seg = torch.mul( img , (seg_src == segment_num).cpu().float() )
        else:
            mask_seg = torch.mul( img , (seg_src == segment_num).cuda().float() )

        #Calc Each Histogram
        with torch.no_grad():
            hist_2d = self.getHistogram2d_np(mask_seg, self.hist_ab)
            hist_1d = self.getHistogram1d_np(mask_seg, self.hist_l).repeat(1,1,self.hist_ab,self.hist_ab)
            hist_cat   = torch.cat((hist_2d,hist_1d),1)

        #Encode Each Histogram Tensor
        hist_feat = self.HEN(hist_cat)

        #Embeded to Final Tensor
        final_tensor[:,:,seg_tgt.squeeze(0)==segment_num] = hist_feat.repeat(1,1, final_tensor[:,:,seg_tgt.squeeze(0)==segment_num].size(2), 1).squeeze(0).permute(2,0,1)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def MakeLabelFromMap(self,input_A_Map, input_B_Map):
        label_A = self.LabelFromMap(input_A_Map)
        label_B = self.LabelFromMap(input_B_Map)

        label_AB2 = np.concatenate((label_A,label_B), axis = 0)
        label_AB2 = np.unique(label_AB2, axis = 0)
        label_AB  = torch.from_numpy(label_AB2)

        A_seg = torch.zeros(1,input_A_Map.size(2),input_A_Map.size(3))
        B_seg = torch.zeros(1,input_B_Map.size(2),input_B_Map.size(3))

        for i in range(0,label_AB.size(0)):            
            A_seg[ (input_A_Map.squeeze(0) == label_AB[i].unsqueeze(0).unsqueeze(0).permute(2,0,1))[0:1,:,:] ] = i
            B_seg[ (input_B_Map.squeeze(0) == label_AB[i].unsqueeze(0).unsqueeze(0).permute(2,0,1))[0:1,:,:] ] = i

        if not torch.cuda.is_available():
            A_seg = A_seg.cpu().float()
            B_seg = B_seg.cpu().float()
        else:
            A_seg = A_seg.cuda().float()
            B_seg = B_seg.cuda().float()

        return A_seg, B_seg, label_AB.size(0)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def LabelFromMap(self, tensor_map):
        np1 = tensor_map.squeeze(0).detach().cpu().numpy()
        np2 = np.transpose(np1, (1,2,0))
        np3 = np.reshape(np2, (np.shape(np2)[0] * np.shape(np2)[1], 3))
        np4 = np.unique(np3, axis= 0)

        return np4    

ColorTransferLib/Algorithms/HIS/models/models.py

def create_model(opt):
    from .colorhistogram_model import ColorHistogram_Model
    model = ColorHistogram_Model()        
    model.initialize(opt)
    return model

ColorTransferLib/Algorithms/HIS/models/modules/architecture.py

import math
import torch
import torch.nn as nn
import torchvision
from torch.autograd import Variable
from . import block as B

####################
# Generator
####################

#class SRResNet(nn.Module):
#    def __init__(self, in_nc, out_nc, nf, nb, upscale=4, norm_type='batch', act_type='relu', \
#            mode='NAC', res_scale=1, upsample_mode='upconv'):
#        super(SRResNet, self).__init__()
#        n_upscale = int(math.log(upscale, 2))
#        if upscale == 3:
#            n_upscale = 1
#
#        fea_conv = B.conv_block(in_nc, nf, kernel_size=3, norm_type=None, act_type=None)
#        resnet_blocks = [B.ResNetBlock(nf, nf, nf, norm_type=norm_type, act_type=act_type,\
#            mode=mode, res_scale=res_scale) for _ in range(nb)]
#        LR_conv = B.conv_block(nf, nf, kernel_size=3, norm_type=norm_type, act_type=None, mode=mode)
#
#        if upsample_mode == 'upconv':
#            upsample_block = B.upconv_blcok
#        elif upsample_mode == 'pixelshuffle':
#            upsample_block = B.pixelshuffle_block
#        else:
#            raise NotImplementedError('upsample mode [%s] is not found' % upsample_mode)
#        if upscale == 3:
#            upsampler = upsample_block(nf, nf, 3, act_type=act_type)
#        else:
#            upsampler = [upsample_block(nf, nf, act_type=act_type) for _ in range(n_upscale)]
#        HR_conv0 = B.conv_block(nf, nf, kernel_size=3, norm_type=None, act_type=act_type)
#        HR_conv1 = B.conv_block(nf, out_nc, kernel_size=3, norm_type=None, act_type=None)
#
#        self.model = B.sequential(fea_conv, B.ShortcutBlock(B.sequential(*resnet_blocks, LR_conv)),\
#            *upsampler, HR_conv0, HR_conv1)
#
#    def forward(self, x):
#        x = self.model(x)
#        return x
#
#
#####################
## Discriminator
#####################
#
#
## VGG style Discriminator with input size 128*128
#class Discriminaotr_VGG_128(nn.Module):
#    def __init__(self, in_nc, base_nf, norm_type='batch', act_type='leakyrelu', mode='CNA'):
#        super(Discriminaotr_VGG_128, self).__init__()
#        # features
#        # hxw, c
#        # 128, 64
#        conv0 = B.conv_block(in_nc, base_nf, kernel_size=3, norm_type=None, act_type=act_type, \
#            mode=mode)
#        conv1 = B.conv_block(base_nf, base_nf, kernel_size=4, stride=2, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        # 64, 64
#        conv2 = B.conv_block(base_nf, base_nf*2, kernel_size=3, stride=1, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        conv3 = B.conv_block(base_nf*2, base_nf*2, kernel_size=4, stride=2, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        # 32, 128
#        conv4 = B.conv_block(base_nf*2, base_nf*4, kernel_size=3, stride=1, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        conv5 = B.conv_block(base_nf*4, base_nf*4, kernel_size=4, stride=2, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        # 16, 256
#        conv6 = B.conv_block(base_nf*4, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        conv7 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        # 8, 512
#        conv8 = B.conv_block(base_nf*8, base_nf*8, kernel_size=3, stride=1, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        conv9 = B.conv_block(base_nf*8, base_nf*8, kernel_size=4, stride=2, norm_type=norm_type, \
#            act_type=act_type, mode=mode)
#        # 4, 512
#        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5, conv6, conv7, conv8,\
#            conv9)
#
#        # classifier
#        self.classifier = nn.Sequential(
#            nn.Linear(512*4*4, 100),
#            nn.LeakyReLU(0.2, True),
#            nn.Linear(100, 1)
#        )
#
#    def forward(self, x):
#        x = self.features(x)
#        x = x.view(x.size(0), -1)
#        x = self.classifier(x)
#        return x

####################
# Perceptual Network
####################

# Assume input range is [0, 1]
class VGGFeatureExtractor(nn.Module):
    def __init__(self,
                 feature_layer=34,
                 use_bn=False,
                 use_input_norm=True,
                 tensor=torch.FloatTensor):
        super(VGGFeatureExtractor, self).__init__()
        if use_bn:
            model = torchvision.models.vgg19_bn(pretrained=True)
        else:
            model = torchvision.models.vgg19(pretrained=True)
        self.use_input_norm = use_input_norm
        if self.use_input_norm:
            mean = Variable(tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1), requires_grad=False)
            # [0.485-1, 0.456-1, 0.406-1] if input in range [-1,1]
            std = Variable(tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1), requires_grad=False)
            # [0.229*2, 0.224*2, 0.225*2] if input in range [-1,1]
            self.register_buffer('mean', mean)
            self.register_buffer('std', std)
        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])
        # No need to BP to variable
        for k, v in self.features.named_parameters():
            v.requires_grad = False

    def forward(self, x):
        if self.use_input_norm:
            x = (x - self.mean) / self.std
        output = self.features(x)
        return output

ColorTransferLib/Algorithms/HIS/models/modules/iccv_model.py

import math
import torch
import torch.nn as nn
import torchvision
from torch.autograd import Variable
import functools
import torch.nn.functional as F

# More Deeper Unet
class StdUnet_woIN(nn.Module): 
    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, padding_type='zero'):
        super(StdUnet_woIN, self).__init__()

        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        enc_nc = 64

        #Init
        self.block_init  = self.build_init_block(input_nc,enc_nc,padding_type, use_bias)
        #self.block_init2 = self.build_init_block(input_nc,enc_nc,padding_type, use_bias)

        #Enc
        self.HistUnetEnc = StdUnetEnc_deep(input_nc, 64, norm_layer)

        #Dec
        self.HistUnetDec1 = StdUnetDec_deep2(512 + 512 + enc_nc + enc_nc,  512, norm_layer) #512 + 512 + 72 + 72
        self.HistUnetDec2 = StdUnetDec_deep2(512 + 512 + enc_nc + enc_nc,  256, norm_layer)
        self.HistUnetDec3 = StdUnetDec_deep2(256 + 256 + enc_nc + enc_nc,  128, norm_layer)
        self.HistUnetDec4 = StdUnetDec_deep2(128 + 128 + enc_nc + enc_nc,   64, norm_layer)
        self.HistUnetDec5 = StdUnetDec_deep1( 64 +  64 + enc_nc + enc_nc,   output_nc, norm_layer)

        #Out
        self.InterOut1 = self.build_inter_out2(512, output_nc, 'zero', use_bias)
        self.InterOut2 = self.build_inter_out2(256, output_nc, 'zero', use_bias)
        self.InterOut3 = self.build_inter_out2(128, output_nc, 'zero', use_bias)
        self.InterOut4 = self.build_inter_out2( 64, output_nc, 'zero', use_bias)

        self.block_last = self.build_last_block(64,output_nc,padding_type,use_bias) 

    def build_init_block(self, input_nc,dim_img, padding_type, use_bias): # 3 -> 64
        block_init =[]

        p = 0
        if padding_type == 'reflect':
            block_init += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_init += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_init += [nn.Conv2d(input_nc,dim_img,kernel_size=3,padding=p,stride=1),
                       nn.InstanceNorm2d(dim_img),
                       nn.ReLU(True),
                       nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,stride=1),
        ]

        return nn.Sequential(*block_init)

    def build_last_block(self,dim_img,output_nc,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,output_nc,kernel_size=3,padding=p,bias=use_bias)
                        ]

        return nn.Sequential(*block_last)

    def build_inter_out2(self,dim_img,dim_out,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [
                        # 1
                        nn.Conv2d(dim_img, dim_img, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(dim_img),
                        nn.ReLU(True),

                        # 2
                        nn.Conv2d(dim_img, dim_img, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(dim_img),
                        nn.ReLU(True),

                        # 3
                        nn.Conv2d(dim_img, dim_out, kernel_size=3,stride=1, padding=1),

                        ]
        return nn.Sequential(*block_last)

    def forward(self, input_img, hist1_enc, hist2_enc):

        # Enc
        mid_img1, mid_img2, mid_img3, mid_img4, mid_img5 = self.HistUnetEnc(input_img) # 1/3/256/256

        # Dec
        out_img2 = self.HistUnetDec2(mid_img5, mid_img4, hist1_enc, hist2_enc, mid_img4.size(2), mid_img4.size(3)) # 
        out_img3 = self.HistUnetDec3(out_img2, mid_img3, hist1_enc, hist2_enc, mid_img3.size(2), mid_img3.size(3)) # 
        out_img4 = self.HistUnetDec4(out_img3, mid_img2, hist1_enc, hist2_enc, mid_img2.size(2), mid_img2.size(3)) # 
        out_img5 = self.HistUnetDec5(out_img4, mid_img1, hist1_enc, hist2_enc, mid_img1.size(2), mid_img1.size(3)) #
        out_img5 = F.upsample(out_img5,size = (input_img.size(2), input_img.size(3)), mode = 'bilinear')

        # Out
        out_img1 = out_img5 
        out_img2 = self.InterOut2(out_img2)
        out_img3 = self.InterOut3(out_img3)
        out_img4 = self.InterOut4(out_img4)

        # Residual
        out_img = out_img5

        return out_img1, out_img2, out_img3, out_img4, out_img

class StdUnetEnc_deep(nn.Module):
    def __init__(self, input_channel, ngf, norm_layer=nn.BatchNorm2d):
        super(StdUnetEnc_deep, self).__init__()

        self.netEnc1 = StdUnetEnc_deep1(input_channel,ngf,norm_layer)
        self.netEnc2 = StdUnetEnc_deep2(ngf * 1, ngf * 2, norm_layer) #64, 128
        self.netEnc3 = StdUnetEnc_deep2(ngf * 2, ngf * 4, norm_layer) #128, 256
        self.netEnc4 = StdUnetEnc_deep2(ngf * 4, ngf * 8, norm_layer) #256, 512
        self.netEnc5 = StdUnetEnc_deep2(ngf * 8, ngf * 8, norm_layer) #512, 512

    def forward(self, input):
        output1 = self.netEnc1.forward(input)  # 256/256/64 -> 128/128/64
        output2 = self.netEnc2.forward(output1) # 128/128/64 -> 64/64/128
        output3 = self.netEnc3.forward(output2) # 64/64/128 -> 32/32/256
        output4 = self.netEnc4.forward(output3) # 32/32/256 -> 16/16/512
        output5 = self.netEnc5.forward(output4) # 16/16/512 -> 8/8/512

        return output1, output2, output3, output4, output5

class StdUnetEnc_deep1(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(StdUnetEnc_deep1, self).__init__()
        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

        #1
        nn.Conv2d(input_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),

        #2
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),

        #2 Down
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),
        ]

        return nn.Sequential(*block_full)

    def forward(self, input_img):
        return self.block(input_img)

class StdUnetEnc_deep2(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(StdUnetEnc_deep2, self).__init__()

        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

        #1
        nn.Conv2d(input_nc, output_nc, kernel_size=4,stride=2, padding=1),
        nn.LeakyReLU(0.2, True),

        #2
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),

        #3
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),
        ]
        return nn.Sequential(*block_full)

    def forward(self, input_img):
        return self.block(input_img)

class StdUnetDec_deep2(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(StdUnetDec_deep2, self).__init__()

        self.up    = self.build_up()
        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_up(self):
        block_full = [  
                        # 0
                        nn.Upsample(scale_factor=2, mode = 'bilinear'),

        ]
        return nn.Sequential(*block_full)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [  

                        # 1
                        nn.Conv2d(input_nc,output_nc,kernel_size=3,stride=1,padding=1, bias=use_bias),
                        nn.ReLU(True),

                        # 2
                        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
                        nn.ReLU(True),

                        # 3
                        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
                        nn.ReLU(True),

                        ]

        return nn.Sequential(*block_full)

    def forward(self, input_prev, input_skip, enc1, enc2, target_size1, target_size2):

        input_prev = self.up(input_prev)

        input_prev= F.upsample(input_prev, size = (target_size1, target_size2), mode = 'bilinear')
        enc1      = F.upsample(enc1,       size = (target_size1, target_size2), mode = 'bilinear')
        enc2      = F.upsample(enc2,       size = (target_size1, target_size2), mode = 'bilinear')
        input2    = F.upsample(input_skip, size = (target_size1, target_size2), mode = 'bilinear')

        # Should be this format
        out = torch.cat([input_prev, input2, enc1, enc2],1)
        out = self.block(out)

        return out

class StdUnetDec_deep1(nn.Module):
    def __init__(self, input_nc, output_nc,use_tanh,norm_layer=nn.BatchNorm2d):
        super(StdUnetDec_deep1, self).__init__()

        self.up    = self.build_up()
        self.block = self.build_block(input_nc,output_nc,use_tanh,norm_layer=norm_layer)

    def build_up(self):
        block_full = [  
                        # 0
                        nn.Upsample(scale_factor=2, mode = 'bilinear'),

        ]
        return nn.Sequential(*block_full)

    def build_block(self,input_nc,output_nc,use_tanh,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

                        # 1
                        nn.Conv2d(input_nc,128,kernel_size=3,stride=1,padding=1, bias=use_bias),
                        nn.ReLU(True),

                        # 2
                        nn.Conv2d(128, 64, kernel_size=3,stride=1, padding=1),
                        nn.ReLU(True),

                        # 3
                        nn.Conv2d(64, output_nc, kernel_size=3,stride=1, padding=1),
                        #nn.InstanceNorm2d(64),
                        #nn.ReLU(True),

                        ]

        #if use_tanh:
        #    block_full += [nn.Tanh()]

        return nn.Sequential(*block_full)

    def forward(self, input_prev, input_skip, enc1, enc2, target_size1, target_size2):

        input_prev = self.up(input_prev)

        input_prev= F.upsample(input_prev,size= (target_size1, target_size2), mode = 'bilinear')
        enc1 =      F.upsample(enc1, size = (target_size1, target_size2), mode = 'bilinear')
        enc2 =      F.upsample(enc2, size = (target_size1, target_size2), mode = 'bilinear')
        input_skip= F.upsample(input_skip,size= (target_size1, target_size2), mode = 'bilinear')

        #out = self.block(torch.cat([input1, input2, enc1, enc2],1))
        out = torch.cat([input_prev, input_skip, enc1, enc2],1)
        out = self.block(out)

        return out

ColorTransferLib/Algorithms/HIS/models/modules/block.py

from collections import OrderedDict

import torch
import torch.nn as nn

####################
# Basic blocks
####################

def act(act_type, inplace=True, neg_slope=0.2, n_prelu=1):
    # helper selecting activation
    # neg_slope: for leakyrelu and init of prelu
    # n_prelu: for p_relu num_parameters
    act_type = act_type.lower()
    if act_type == 'relu':
        layer = nn.ReLU(inplace)
    elif act_type == 'leakyrelu':
        layer = nn.LeakyReLU(neg_slope, inplace)
    elif act_type == 'prelu':
        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)
    else:
        raise NotImplementedError('activation layer [%s] is not found' % act_type)
    return layer

def norm(norm_type, nc):
    # helper selecting normalization layer
    norm_type = norm_type.lower()
    if norm_type == 'batch':
        layer = nn.BatchNorm2d(nc, affine=True)
    elif norm_type == 'instance':
        layer = nn.InstanceNorm2d(nc, affine=False)
    else:
        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)
    return layer

def pad(pad_type, padding):
    # helper selecting padding layer
    # if padding is 'zero', do by conv layers
    pad_type = pad_type.lower()
    if padding == 0:
        return None
    if pad_type == 'reflect':
        layer = nn.ReflectionPad2d(padding)
    elif pad_type == 'replicate':
        layer = nn.ReplicationPad2d(padding)
    else:
        raise NotImplementedError('padding layer [%s] is not implemented' % pad_type)
    return layer

def get_valid_padding(kernel_size, dilation):
    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)
    padding = (kernel_size - 1) // 2
    return padding

class ConcatBlock(nn.Module):
    # Concat the output of a submodule to its input
    def __init__(self, submodule):
        super(ConcatBlock, self).__init__()
        self.sub = submodule

    def forward(self, x):
        output = torch.cat((x, self.sub(x)), dim=1)
        return output

    def __repr__(self):
        tmpstr = 'Identity .. \n|'
        modstr = self.sub.__repr__().replace('\n', '\n|')
        tmpstr = tmpstr + modstr
        return tmpstr

class ShortcutBlock(nn.Module):
    #Elementwise sum the output of a submodule to its input
    def __init__(self, submodule):
        super(ShortcutBlock, self).__init__()
        self.sub = submodule

    def forward(self, x):
        output = x + self.sub(x)
        return output

    def __repr__(self):
        tmpstr = 'Identity + \n|'
        modstr = self.sub.__repr__().replace('\n', '\n|')
        tmpstr = tmpstr + modstr
        return tmpstr

def sequential(*args):
    # Flatten Sequential. It unwraps nn.Sequential.
    if len(args) == 1:
        if isinstance(args[0], OrderedDict):
            raise NotImplementedError('sequential does not support OrderedDict input.')
        return args[0]  # No sequential is needed.
    modules = []
    for module in args:
        if isinstance(module, nn.Sequential):
            for submodule in module.children():
                modules.append(submodule)
        elif isinstance(module, nn.Module):
            modules.append(module)
    return nn.Sequential(*modules)

def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,
               pad_type='zero', norm_type=None, act_type='relu', mode='CNA'):
    """
    Conv layer with padding, normalization, activation
    mode: CNA --> Conv -> Norm -> Act
        NAC --> Norm -> Act --> Conv (Identity Mappings in Deep Residual Networks, ECCV16)
    """
    assert mode in ['CNA', 'NAC', 'CNAC'], 'Wong conv mode [%s]' % mode
    padding = get_valid_padding(kernel_size, dilation)
    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None
    padding = padding if pad_type == 'zero' else 0

    c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding, \
            dilation=dilation, bias=bias, groups=groups)
    a = act(act_type) if act_type else None
    if 'CNA' in mode:
        n = norm(norm_type, out_nc) if norm_type else None
        return sequential(p, c, n, a)
    elif mode == 'NAC':
        if norm_type is None and act_type is not None:
            a = act(act_type, inplace=False)
            # Important!
            # input----ReLU(inplace)----Conv--+----output
            #        |________________________|
            # inplace ReLU will modify the input, therefore wrong output
        n = norm(norm_type, in_nc) if norm_type else None
        return sequential(n, a, p, c)

# TODO: add deconv block

####################
# Useful blocks
####################

class ResNetBlock(nn.Module):
    """
    ResNet Block, 3-3 style
    with extra residual scaling used in EDSR
    (Enhanced Deep Residual Networks for Single Image Super-Resolution, CVPRW 17)
    """

    def __init__(self, in_nc, mid_nc, out_nc, kernel_size=3, stride=1, dilation=1, groups=1, \
            bias=True, pad_type='zero', norm_type=None, act_type='relu', mode='CNA', res_scale=1):
        super(ResNetBlock, self).__init__()
        conv0 = conv_block(in_nc, mid_nc, kernel_size, stride, dilation, groups, bias, pad_type, \
            norm_type, act_type, mode)
        if mode == 'CNA':
            act_type = None
        if mode == 'CNAC':  # Residual path: |-CNAC-|
            act_type = None
            norm_type = None
        conv1 = conv_block(mid_nc, out_nc, kernel_size, stride, dilation, groups, bias, pad_type, \
            norm_type, act_type, mode)
        # if in_nc != out_nc:
        #     self.project = conv_block(in_nc, out_nc, 1, stride, dilation, 1, bias, pad_type, \
        #         None, None)
        #     print('Need a projecter in ResNetBlock.')
        # else:
        #     self.project = lambda x:x
        self.res = sequential(conv0, conv1)
        self.res_scale = res_scale

    def forward(self, x):
        res = self.res(x).mul(self.res_scale)
        return x + res

####################
# Upsampler
####################

def pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True,
                        pad_type='zero', norm_type=None, act_type='relu'):
    """
    Pixel shuffle layer
    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional
    Neural Network, CVPR17)
    """
    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias,
                        pad_type=pad_type, norm_type=None, act_type=None)
    pixel_shuffle = nn.PixelShuffle(upscale_factor)

    n = norm(norm_type, out_nc * (upscale_factor**2)) if norm_type else None
    a = act(act_type) if act_type else None
    return sequential(conv, pixel_shuffle, n, a)

def upconv_blcok(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True,
                pad_type='zero', norm_type=None, act_type='relu', mode='nearest'):
    # Up conv
    # described in https://distill.pub/2016/deconv-checkerboard/
    upsample = nn.Upsample(scale_factor=upscale_factor, mode=mode)
    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias,
                        pad_type=pad_type, norm_type=norm_type, act_type=act_type)
    return sequential(upsample, conv)

ColorTransferLib/Algorithms/HIS/models/modules/sft_arch.py

"""
architecture for sft
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class SFTLayer(nn.Module):
    def __init__(self):
        super(SFTLayer, self).__init__()
        self.SFT_scale_conv0 = nn.Conv2d(32, 32, 1)
        self.SFT_scale_conv1 = nn.Conv2d(32, 64, 1)
        self.SFT_shift_conv0 = nn.Conv2d(32, 32, 1)
        self.SFT_shift_conv1 = nn.Conv2d(32, 64, 1)

    def forward(self, x):
        # x[0]: fea; x[1]: cond
        scale = self.SFT_scale_conv1(F.leaky_relu(self.SFT_scale_conv0(x[1]), 0.1, inplace=True))
        shift = self.SFT_shift_conv1(F.leaky_relu(self.SFT_shift_conv0(x[1]), 0.1, inplace=True))
        return x[0] * (scale + 1) + shift

class ResBlock_SFT(nn.Module):
    def __init__(self):
        super(ResBlock_SFT, self).__init__()
        self.sft0 = SFTLayer()
        self.conv0 = nn.Conv2d(64, 64, 3, 1, 1)
        self.sft1 = SFTLayer()
        self.conv1 = nn.Conv2d(64, 64, 3, 1, 1)

    def forward(self, x):
        # x[0]: fea; x[1]: cond
        fea = self.sft0(x)
        fea = F.relu(self.conv0(fea), inplace=True)
        fea = self.sft1((fea, x[1]))
        fea = self.conv1(fea)
        return (x[0] + fea, x[1])  # return a tuple containing features and conditions

class SFT_Net(nn.Module):
    def __init__(self):
        super(SFT_Net, self).__init__()
        self.conv0 = nn.Conv2d(3, 64, 3, 1, 1)

        sft_branch = []
        for i in range(16):
            sft_branch.append(ResBlock_SFT())
        sft_branch.append(SFTLayer())
        sft_branch.append(nn.Conv2d(64, 64, 3, 1, 1))
        self.sft_branch = nn.Sequential(*sft_branch)

        self.HR_branch = nn.Sequential(
            nn.Conv2d(64, 256, 3, 1, 1),
            nn.PixelShuffle(2),
            nn.ReLU(True),
            nn.Conv2d(64, 256, 3, 1, 1),
            nn.PixelShuffle(2),
            nn.ReLU(True),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.ReLU(True),
            nn.Conv2d(64, 3, 3, 1, 1)
        )

        self.CondNet = nn.Sequential(
            nn.Conv2d(8, 128, 4, 4),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 128, 1),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 128, 1),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 128, 1),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 32, 1)
        )

    def forward(self, x):
        # x[0]: img; x[1]: seg
        cond = self.CondNet(x[1])
        fea = self.conv0(x[0])
        res = self.sft_branch((fea, cond))
        fea = fea + res
        out = self.HR_branch(fea)
        return out

# Auxiliary Classifier Discriminator
class ACD_VGG_BN_96(nn.Module):
    def __init__(self):
        super(ACD_VGG_BN_96, self).__init__()

        self.feature = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(64, 64, 4, 2, 1),
            nn.BatchNorm2d(64, affine=True),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(64, 128, 3, 1, 1),
            nn.BatchNorm2d(128, affine=True),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(128, 128, 4, 2, 1),
            nn.BatchNorm2d(128, affine=True),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(128, 256, 3, 1, 1),
            nn.BatchNorm2d(256, affine=True),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(256, 256, 4, 2, 1),
            nn.BatchNorm2d(256, affine=True),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(256, 512, 3, 1, 1),
            nn.BatchNorm2d(512, affine=True),
            nn.LeakyReLU(0.1, True),

            nn.Conv2d(512, 512, 4, 2, 1),
            nn.BatchNorm2d(512, affine=True),
            nn.LeakyReLU(0.1, True),
        )

        # gan
        self.gan = nn.Sequential(
            nn.Linear(512*6*6, 100),
            nn.LeakyReLU(0.1, True),
            nn.Linear(100, 1)
        )

        self.cls = nn.Sequential(
            nn.Linear(512*6*6, 100),
            nn.LeakyReLU(0.1, True),
            nn.Linear(100, 8)
        )

    def forward(self, x):
        fea = self.feature(x)
        fea = fea.view(fea.size(0), -1)
        gan = self.gan(fea)
        cls = self.cls(fea)
        return [gan, cls]

#############################################
# below is the sft arch for the torch version
#############################################

class SFTLayer_torch(nn.Module):
    def __init__(self):
        super(SFTLayer_torch, self).__init__()
        self.SFT_scale_conv0 = nn.Conv2d(32, 32, 1)
        self.SFT_scale_conv1 = nn.Conv2d(32, 64, 1)
        self.SFT_shift_conv0 = nn.Conv2d(32, 32, 1)
        self.SFT_shift_conv1 = nn.Conv2d(32, 64, 1)

    def forward(self, x):
        # x[0]: fea; x[1]: cond
        scale = self.SFT_scale_conv1(F.leaky_relu(self.SFT_scale_conv0(x[1]), 0.01, inplace=True))
        shift = self.SFT_shift_conv1(F.leaky_relu(self.SFT_shift_conv0(x[1]), 0.01, inplace=True))
        return x[0] * scale + shift

class ResBlock_SFT_torch(nn.Module):
    def __init__(self):
        super(ResBlock_SFT_torch, self).__init__()
        self.sft0 = SFTLayer_torch()
        self.conv0 = nn.Conv2d(64, 64, 3, 1, 1)
        self.sft1 = SFTLayer_torch()
        self.conv1 = nn.Conv2d(64, 64, 3, 1, 1)

    def forward(self, x):
        # x[0]: fea; x[1]: cond
        fea = F.relu(self.sft0(x), inplace=True)
        fea = self.conv0(fea)
        fea = F.relu(self.sft1((fea, x[1])), inplace=True)
        fea = self.conv1(fea)
        return (x[0] + fea, x[1])  # return a tuple containing features and conditions

class SFT_Net_torch(nn.Module):
    def __init__(self):
        super(SFT_Net_torch, self).__init__()
        self.conv0 = nn.Conv2d(3, 64, 3, 1, 1)

        sft_branch = []
        for i in range(16):
            sft_branch.append(ResBlock_SFT_torch())
        sft_branch.append(SFTLayer_torch())
        sft_branch.append(nn.Conv2d(64, 64, 3, 1, 1))
        self.sft_branch = nn.Sequential(*sft_branch)

        self.HR_branch = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='nearest'),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.ReLU(True),
            nn.Upsample(scale_factor=2, mode='nearest'),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.ReLU(True),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.ReLU(True),
            nn.Conv2d(64, 3, 3, 1, 1)
        )

        # Condtion network
        self.CondNet = nn.Sequential(
            nn.Conv2d(8, 128, 4, 4),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 128, 1),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 128, 1),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 128, 1),
            nn.LeakyReLU(0.1, True),
            nn.Conv2d(128, 32, 1)
        )

    def forward(self, x):
        # x[0]: img; x[1]: seg
        cond = self.CondNet(x[1])
        fea = self.conv0(x[0])
        res = self.sft_branch((fea, cond))
        fea = fea + res
        out = self.HR_branch(fea)
        return out

ColorTransferLib/Algorithms/HIS/util/init.py

ColorTransferLib/Algorithms/HIS/models/networks.py

import torch
import torch.nn as nn
from torch.nn import init
import functools
from torch.autograd import Variable
from torch.optim import lr_scheduler
import numpy as np
import torchvision
import torch.nn.functional as F

###############################################################################
# Functions
###############################################################################
def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('Linear') != -1:
        init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm2d') != -1:
        init.normal_(m.weight.data, 1.0, 0.02)
        init.constant(m.bias.data, 0.0)

def weights_init_xavier(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.xavier_normal(m.weight.data, gain=0.02)
    elif classname.find('Linear') != -1:
        init.xavier_normal(m.weight.data, gain=0.02)
    elif classname.find('BatchNorm2d') != -1:
        init.normal(m.weight.data, 1.0, 0.02)
        init.constant(m.bias.data, 0.0)

def weights_init_kaiming(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')
    elif classname.find('Linear') != -1:
        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')
    elif classname.find('BatchNorm2d') != -1:
        init.normal(m.weight.data, 1.0, 0.02)
        init.constant(m.bias.data, 0.0)

def weights_init_orthogonal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.orthogonal(m.weight.data, gain=1)
    elif classname.find('Linear') != -1:
        init.orthogonal(m.weight.data, gain=1)
    elif classname.find('BatchNorm2d') != -1:
        init.normal(m.weight.data, 1.0, 0.02)
        init.constant(m.bias.data, 0.0)

def init_weights(net, init_type='normal'):
    if init_type == 'normal':
        net.apply(weights_init_normal)
    elif init_type == 'xavier':
        net.apply(weights_init_xavier)
    elif init_type == 'kaiming':
        net.apply(weights_init_kaiming)
    elif init_type == 'orthogonal':
        net.apply(weights_init_orthogonal)
    else:
        raise NotImplementedError('initialization method [%s] is not implemented' % init_type)

def get_norm_layer(norm_type='instance'):
    if norm_type == 'batch':
        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)
    elif norm_type == 'instance':
        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)
    elif norm_type == 'none':
        norm_layer = None
    else:
        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)
    return norm_layer

def IRN(input_nc, output_nc, ngf, which_model_netG, norm='none', use_dropout=False, init_type='normal', gpu_ids=[]): # Batch norm -> nonw
    IRN = None

    if not torch.cuda.is_available():
        IRN = HISTUnet3_Res(input_nc, output_nc, ngf, norm_layer=get_norm_layer(norm_type=norm), use_dropout=use_dropout).cpu()
    else:
        IRN = HISTUnet3_Res(input_nc, output_nc, ngf, norm_layer=get_norm_layer(norm_type=norm), use_dropout=use_dropout).cuda()

    init_weights(IRN, init_type=init_type)

    return IRN

def HEN(input_nc, output_nc, ngf, which_model_netC, norm='batch', use_dropout=False, init_type='normal', gpu_ids=[]):
    HEN = None

    if not torch.cuda.is_available():
        HEN = ConditionNetwork2(input_nc, output_nc, ngf, norm_layer=get_norm_layer(norm_type=norm), use_dropout=use_dropout, gpu_ids=gpu_ids).cpu()
    else:
        HEN = ConditionNetwork2(input_nc, output_nc, ngf, norm_layer=get_norm_layer(norm_type=norm), use_dropout=use_dropout, gpu_ids=gpu_ids).cuda()
    init_weights(HEN, init_type=init_type)

    return HEN

##############################################################################
# Classes
##############################################################################

class ConditionNetwork2(nn.Module):
    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[], padding_type='reflect'):
        super(ConditionNetwork2, self).__init__()
        self.input_nc = input_nc # 10
        self.output_nc = output_nc # 32
        self.ngf = ngf
        self.gpu_ids = gpu_ids

        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        dim2 = 128

        model = [
                #### Half Size
                 nn.Conv2d(self.input_nc, dim2, kernel_size=4, padding=1, stride=2, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                 nn.Conv2d(dim2, dim2, kernel_size=4, padding=1, stride=2, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                 nn.Conv2d(dim2, dim2, kernel_size=4, padding=1, stride=2, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                 nn.Conv2d(dim2, dim2, kernel_size=4, padding=1, stride=2, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                #### -1 Size
                 nn.Conv2d(dim2, dim2, kernel_size=4, padding=1, stride=1, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                 nn.Conv2d(dim2, dim2, kernel_size=4, padding=1, stride=1, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                 nn.Conv2d(dim2, dim2, kernel_size=4, padding=1, stride=1, bias=use_bias),
                 nn.LeakyReLU(0.1, True),
                #### Out Change
                 nn.Conv2d(dim2, self.output_nc, kernel_size=1, padding=0,bias=use_bias),
        ]

        self.model = nn.Sequential(*model)

        self.model2 = nn.Sequential(nn.Linear(output_nc,output_nc))

        self.model3 = nn.Sequential(nn.Linear(output_nc,324))

    def forward(self, input):

        # Original
        #a1 = self.model(input) # 1/32/6/6
        #a2 = a1.view(a1.size(0),-1)
        #a3 = self.model3(a2)
        #a3 = a3.unsqueeze(0).unsqueeze(0).permute(0,3,1,2) # 1,32,1,1

        # Ver.2
        a1 = self.model(input)
        a2 = a1.view(a1.size(0),-1)
        a3 = self.model2(a2)
        a3 = a3.unsqueeze(0).unsqueeze(0).permute(2,3,0,1) # 1/64/1/1

        return a3

class HISTUnet3_Res(nn.Module): 
    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, padding_type='zero'):
        super(HISTUnet3_Res, self).__init__()

        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        enc_nc = 64

        #Init
        self.block_init  = self.build_init_block(input_nc,enc_nc,padding_type, use_bias)
        #self.block_init2 = self.build_init_block(input_nc,enc_nc,padding_type, use_bias)

        #Enc
        self.HistUnetEnc = UnetEnc_deep(input_nc, 64, norm_layer)

        #Dec
        self.HistUnetDec1 = UnetDec_deep2(512 + 512 + enc_nc + enc_nc,  512, norm_layer) #512 + 512 + 72 + 72
        self.HistUnetDec2 = UnetDec_deep2(512 + 512 + enc_nc + enc_nc,  256, norm_layer)
        self.HistUnetDec3 = UnetDec_deep2(256 + 256 + enc_nc + enc_nc,  128, norm_layer)
        self.HistUnetDec4 = UnetDec_deep2(128 + 128 + enc_nc + enc_nc,   64, norm_layer)
        self.HistUnetDec5 = UnetDec_deep1( 64 +  64 + enc_nc + enc_nc,   64, norm_layer)

        #Res
        self.ENC_Block1 = ENCResnetBlock(enc_nc, padding_type, norm_layer, use_dropout, use_bias)
        self.ENC_Block2 = ENCResnetBlock(enc_nc, padding_type, norm_layer, use_dropout, use_bias)
        self.ENC_Block3 = ENCResnetBlock(enc_nc, padding_type, norm_layer, use_dropout, use_bias)

        #Out
        self.InterOut1 = self.build_inter_out2(512, output_nc, 'zero', use_bias)
        self.InterOut2 = self.build_inter_out2(256, output_nc, 'zero', use_bias)
        self.InterOut3 = self.build_inter_out2(128, output_nc, 'zero', use_bias)
        self.InterOut4 = self.build_inter_out2( 64, output_nc, 'zero', use_bias)

        self.block_last = self.build_last_block(64,output_nc,padding_type,use_bias) 

    def build_init_block(self, input_nc,dim_img, padding_type, use_bias): # 3 -> 64
        block_init =[]

        p = 0
        if padding_type == 'reflect':
            block_init += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_init += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_init += [nn.Conv2d(input_nc,dim_img,kernel_size=3,padding=p,stride=1),
                       nn.InstanceNorm2d(dim_img),
                       nn.ReLU(True),
                       nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,stride=1),
        ]

        return nn.Sequential(*block_init)

    def build_last_block(self,dim_img,output_nc,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,output_nc,kernel_size=3,padding=p,bias=use_bias)
                        ]

        return nn.Sequential(*block_last)

    def build_inter_out(self,dim_img,dim_out,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [nn.Conv2d(dim_img,dim_out,kernel_size=3,padding=p,bias=use_bias),
                        ]
        return nn.Sequential(*block_last)

    def build_inter_out2(self,dim_img,dim_out,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [
                        # 2
                        nn.ReLU(True),
                        nn.Conv2d(dim_img, dim_img, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(dim_img),

                        # 2
                        nn.ReLU(True),
                        nn.Conv2d(dim_img, dim_out, kernel_size=3,stride=1, padding=1),
                        ]
        return nn.Sequential(*block_last)

    def forward(self, input_img, hist1_enc, hist2_enc):

        # Enc
        mid_img1, mid_img2, mid_img3, mid_img4, mid_img5 = self.HistUnetEnc(input_img) # 1/3/256/256

        # Dec
        out_img1 = self.HistUnetDec1(mid_img5, mid_img5, hist1_enc, hist2_enc, mid_img5.size(2), mid_img5.size(3)) #
        out_img2 = self.HistUnetDec2(out_img1, mid_img4, hist1_enc, hist2_enc, mid_img4.size(2), mid_img4.size(3)) # 
        out_img3 = self.HistUnetDec3(out_img2, mid_img3, hist1_enc, hist2_enc, mid_img3.size(2), mid_img3.size(3)) # 
        out_img4 = self.HistUnetDec4(out_img3, mid_img2, hist1_enc, hist2_enc, mid_img2.size(2), mid_img2.size(3)) # 
        out_img5 = self.HistUnetDec5(out_img4, mid_img1, hist1_enc, hist2_enc, mid_img1.size(2), mid_img1.size(3)) #
        out_img5 = F.upsample(out_img5,size = (input_img.size(2), input_img.size(3)), mode = 'bilinear')

        out_img6 = self.ENC_Block1.forward(out_img5 + self.block_init(input_img), hist1_enc, hist2_enc)
        out_img7 = self.ENC_Block2.forward(out_img6 + self.block_init(input_img), hist1_enc, hist2_enc)
        out_img  = self.ENC_Block3.forward(out_img7 + self.block_init(input_img), hist1_enc, hist2_enc)

        out_img1 = self.InterOut1(out_img1)
        out_img2 = self.InterOut2(out_img2)
        out_img3 = self.InterOut3(out_img3)
        out_img4 = self.InterOut4(out_img4)

        out_img    = self.block_last(out_img + self.block_init(input_img))

        return out_img1, out_img2, out_img3, out_img4, out_img

#################################################################################################################################################################################################################################################################################################

class UnetEnc_deep(nn.Module):
    def __init__(self, input_channel, ngf, norm_layer=nn.BatchNorm2d):
        super(UnetEnc_deep, self).__init__()

        self.netEnc1 = UnetEnc_deep1(input_channel,ngf,norm_layer)
        self.netEnc2 = UnetEnc_deep2(ngf * 1, ngf * 2, norm_layer) #64, 128
        self.netEnc3 = UnetEnc_deep2(ngf * 2, ngf * 4, norm_layer) #128, 256
        self.netEnc4 = UnetEnc_deep2(ngf * 4, ngf * 8, norm_layer) #256, 512
        self.netEnc5 = UnetEnc_deep2(ngf * 8, ngf * 8, norm_layer) #512, 512

    def forward(self, input):
        output1 = self.netEnc1.forward(input)  # 256/256/64 -> 128/128/64
        output2 = self.netEnc2.forward(output1) # 128/128/64 -> 64/64/128
        output3 = self.netEnc3.forward(output2) # 64/64/128 -> 32/32/256
        output4 = self.netEnc4.forward(output3) # 32/32/256 -> 16/16/512
        output5 = self.netEnc5.forward(output4) # 16/16/512 -> 8/8/512

        return output1, output2, output3, output4, output5

class UnetEnc_deep1(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(UnetEnc_deep1, self).__init__()
        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

        #1
        nn.Conv2d(input_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.InstanceNorm2d(output_nc),
        nn.LeakyReLU(0.2, True),

        #2
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.InstanceNorm2d(output_nc),
        nn.LeakyReLU(0.2, True),

        #2 Down
        nn.Conv2d(output_nc, output_nc, kernel_size=4,stride=2, padding=1),
        nn.InstanceNorm2d(output_nc),
        ]

        return nn.Sequential(*block_full)

    def forward(self, input_img):
        return self.block(input_img)

class UnetEnc_deep2(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(UnetEnc_deep2, self).__init__()

        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

        #1
        nn.LeakyReLU(0.2, True),
        nn.Conv2d(input_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.InstanceNorm2d(output_nc),

        #2
        nn.LeakyReLU(0.2, True),
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.InstanceNorm2d(output_nc),

        #3
        nn.LeakyReLU(0.2, True),
        nn.Conv2d(output_nc, output_nc, kernel_size=4,stride=2, padding=1),
        nn.InstanceNorm2d(output_nc),
        ]
        return nn.Sequential(*block_full)

    def forward(self, input_img):
        return self.block(input_img)

class UnetDec_deep1(nn.Module):
    def __init__(self, input_nc, output_nc,use_tanh,norm_layer=nn.BatchNorm2d):
        super(UnetDec_deep1, self).__init__()

        self.block = self.build_block(input_nc,output_nc,use_tanh,norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,use_tanh,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

                        # 1
                        nn.ReLU(True),
                        nn.Upsample(scale_factor=2, mode = 'bilinear'),
                        nn.Conv2d(input_nc,128,kernel_size=3,stride=1,padding=1, bias=use_bias),
                        nn.InstanceNorm2d(128),

                        # 2
                        nn.ReLU(True),
                        nn.Conv2d(128, 64, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(64),

                        # 2
                        nn.ReLU(True),
                        nn.Conv2d(64, output_nc, kernel_size=3,stride=1, padding=1),

                        ]

        return nn.Sequential(*block_full)

    def forward(self, input1, input2, enc1, enc2, target_size1, target_size2):
        input1=F.upsample(input1,size= (target_size1, target_size2), mode = 'bilinear')
        enc1 = F.upsample(enc1, size = (target_size1, target_size2), mode = 'bilinear')
        enc2 = F.upsample(enc2, size = (target_size1, target_size2), mode = 'bilinear')
        input2=F.upsample(input2,size= (target_size1, target_size2), mode = 'bilinear')

        out = self.block(torch.cat([input1, input2, enc1, enc2],1))
        return out

class UnetDec_deep2(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(UnetDec_deep2, self).__init__()

        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [  
                        # 1
                        nn.ReLU(True),
                        nn.Upsample(scale_factor=2, mode = 'bilinear'),
                        nn.Conv2d(input_nc,output_nc,kernel_size=3,stride=1,padding=1, bias=use_bias),
                        nn.InstanceNorm2d(output_nc),

                        # 2
                        nn.ReLU(True),
                        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(output_nc),

                        # 3
                        nn.ReLU(True),
                        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(output_nc),

                        ]

        return nn.Sequential(*block_full)

    def forward(self, input1, input2, enc1, enc2, target_size1, target_size2):

        input1=F.upsample(input1,size = (target_size1, target_size2), mode = 'bilinear')
        enc1 = F.upsample(enc1,  size = (target_size1, target_size2), mode = 'bilinear')
        enc2 = F.upsample(enc2,  size = (target_size1, target_size2), mode = 'bilinear')
        input2=F.upsample(input2,size = (target_size1, target_size2), mode = 'bilinear')

        out = self.block(torch.cat([input1, input2, enc1, enc2],1))

        return out

class ENCResnetBlock(nn.Module):
    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):
        super(ENCResnetBlock, self).__init__()
        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)

    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):
        conv_block = []
        p = 0
        if padding_type == 'reflect':
            conv_block += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            conv_block += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        conv_block += [nn.Conv2d( dim * 3, dim * 3, kernel_size=3, padding=p, bias=use_bias),
                       norm_layer(dim * 3),
                       nn.ReLU(True)]
        if use_dropout:
            conv_block += [nn.Dropout(0.5)]

        p = 0
        if padding_type == 'reflect':
            conv_block += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            conv_block += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)
        conv_block += [nn.Conv2d(dim * 3, dim, kernel_size=3, padding=p, bias=use_bias),
                       norm_layer(dim)]

        return nn.Sequential(*conv_block)

    def forward(self, x, enc1, enc2): #64/64/64
        enc1 = F.upsample(enc1,size = (x.size(2), x.size(3)), mode = 'bilinear')
        enc2 = F.upsample(enc2,size = (x.size(2), x.size(3)), mode = 'bilinear')

        x_cat = torch.cat((x,enc1,enc2),1) # 64/64/64/
        out = x + self.conv_block(x_cat) # 192 -> 64        
        return out

ColorTransferLib/Algorithms/HIS/util/html.py

import dominate
from dominate.tags import *
import os

class HTML:
    def __init__(self, web_dir, title, reflesh=0):
        self.title = title
        self.web_dir = web_dir
        self.img_dir = os.path.join(self.web_dir, 'images')
        if not os.path.exists(self.web_dir):
            os.makedirs(self.web_dir)
        if not os.path.exists(self.img_dir):
            os.makedirs(self.img_dir)
        # print(self.img_dir)

        self.doc = dominate.document(title=title)
        if reflesh > 0:
            with self.doc.head:
                meta(http_equiv="reflesh", content=str(reflesh))

    def get_image_dir(self):
        return self.img_dir

    def add_header(self, str):
        with self.doc:
            h3(str)

    def add_table(self, border=1):
        self.t = table(border=border, style="table-layout: fixed;")
        self.doc.add(self.t)

    def add_images(self, ims, txts, links, width=400):
        self.add_table()
        with self.t:
            with tr():
                for im, txt, link in zip(ims, txts, links):
                    with td(style="word-wrap: break-word;", halign="center", valign="top"):
                        with p():
                            with a(href=os.path.join('images', link)):
                                img(style="width:%dpx" % width, src=os.path.join('images', im))
                            br()
                            p(txt)

    def save(self):
        html_file = '%s/index.html' % self.web_dir
        f = open(html_file, 'wt')
        f.write(self.doc.render())
        f.close()

if __name__ == '__main__':
    html = HTML('web/', 'test_html')
    html.add_header('hello world')

    ims = []
    txts = []
    links = []
    for n in range(4):
        ims.append('image_%d.png' % n)
        txts.append('text_%d' % n)
        links.append('image_%d.png' % n)
    html.add_images(ims, txts, links)
    html.save()

ColorTransferLib/Algorithms/HIS/models/modules/stdunet_woIN.py

import math
import torch
import torch.nn as nn
import torchvision
from torch.autograd import Variable
import functools
import torch.nn.functional as F

# More Deeper Unet
class StdUnet_woIN(nn.Module): 
    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, padding_type='zero'):
        super(StdUnet_woIN, self).__init__()

        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        enc_nc = 64

        #Init
        self.block_init  = self.build_init_block(input_nc,enc_nc,padding_type, use_bias)
        #self.block_init2 = self.build_init_block(input_nc,enc_nc,padding_type, use_bias)

        #Enc
        self.HistUnetEnc = StdUnetEnc_deep(input_nc, 64, norm_layer)

        #Dec
        self.HistUnetDec1 = StdUnetDec_deep2(512 + 512 + enc_nc + enc_nc,  512, norm_layer) #512 + 512 + 72 + 72
        self.HistUnetDec2 = StdUnetDec_deep2(512 + 512 + enc_nc + enc_nc,  256, norm_layer)
        self.HistUnetDec3 = StdUnetDec_deep2(256 + 256 + enc_nc + enc_nc,  128, norm_layer)
        self.HistUnetDec4 = StdUnetDec_deep2(128 + 128 + enc_nc + enc_nc,   64, norm_layer)
        self.HistUnetDec5 = StdUnetDec_deep1( 64 +  64 + enc_nc + enc_nc,   output_nc, norm_layer)

        #Out
        self.InterOut1 = self.build_inter_out2(512, output_nc, 'zero', use_bias)
        self.InterOut2 = self.build_inter_out2(256, output_nc, 'zero', use_bias)
        self.InterOut3 = self.build_inter_out2(128, output_nc, 'zero', use_bias)
        self.InterOut4 = self.build_inter_out2( 64, output_nc, 'zero', use_bias)

        self.block_last = self.build_last_block(64,output_nc,padding_type,use_bias) 

    def build_init_block(self, input_nc,dim_img, padding_type, use_bias): # 3 -> 64
        block_init =[]

        p = 0
        if padding_type == 'reflect':
            block_init += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_init += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_init += [nn.Conv2d(input_nc,dim_img,kernel_size=3,padding=p,stride=1),
                       nn.InstanceNorm2d(dim_img),
                       nn.ReLU(True),
                       nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,stride=1),
        ]

        return nn.Sequential(*block_init)

    def build_last_block(self,dim_img,output_nc,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,dim_img,kernel_size=3,padding=p,bias=use_bias),
                        nn.ReLU(True),
                        nn.Conv2d(dim_img,output_nc,kernel_size=3,padding=p,bias=use_bias)
                        ]

        return nn.Sequential(*block_last)

    def build_inter_out2(self,dim_img,dim_out,padding_type,use_bias):
        block_last = []

        p = 0
        if padding_type == 'reflect':
            block_last += [nn.ReflectionPad2d(1)]
        elif padding_type == 'replicate':
            block_last += [nn.ReplicationPad2d(1)]
        elif padding_type == 'zero':
            p = 1
        else:
            raise NotImplementedError('padding [%s] is not implemented' % padding_type)

        block_last += [
                        # 1
                        nn.Conv2d(dim_img, dim_img, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(dim_img),
                        nn.ReLU(True),

                        # 2
                        nn.Conv2d(dim_img, dim_img, kernel_size=3,stride=1, padding=1),
                        nn.InstanceNorm2d(dim_img),
                        nn.ReLU(True),

                        # 3
                        nn.Conv2d(dim_img, dim_out, kernel_size=3,stride=1, padding=1),

                        ]
        return nn.Sequential(*block_last)

    def forward(self, input_img, hist1_enc, hist2_enc):

        # Enc
        mid_img1, mid_img2, mid_img3, mid_img4, mid_img5 = self.HistUnetEnc(input_img) # 1/3/256/256

        # Dec
        out_img2 = self.HistUnetDec2(mid_img5, mid_img4, hist1_enc, hist2_enc, mid_img4.size(2), mid_img4.size(3)) # 
        out_img3 = self.HistUnetDec3(out_img2, mid_img3, hist1_enc, hist2_enc, mid_img3.size(2), mid_img3.size(3)) # 
        out_img4 = self.HistUnetDec4(out_img3, mid_img2, hist1_enc, hist2_enc, mid_img2.size(2), mid_img2.size(3)) # 
        out_img5 = self.HistUnetDec5(out_img4, mid_img1, hist1_enc, hist2_enc, mid_img1.size(2), mid_img1.size(3)) #
        out_img5 = F.upsample(out_img5,size = (input_img.size(2), input_img.size(3)), mode = 'bilinear')

        # Out
        out_img1 = out_img5 
        out_img2 = self.InterOut2(out_img2)
        out_img3 = self.InterOut3(out_img3)
        out_img4 = self.InterOut4(out_img4)

        # Residual
        out_img = out_img5
        #out_img    = self.block_last(out_img + self.block_init(input_img))

        return out_img1, out_img2, out_img3, out_img4, out_img

class StdUnetEnc_deep(nn.Module):
    def __init__(self, input_channel, ngf, norm_layer=nn.BatchNorm2d):
        super(StdUnetEnc_deep, self).__init__()

        self.netEnc1 = StdUnetEnc_deep1(input_channel,ngf,norm_layer)
        self.netEnc2 = StdUnetEnc_deep2(ngf * 1, ngf * 2, norm_layer) #64, 128
        self.netEnc3 = StdUnetEnc_deep2(ngf * 2, ngf * 4, norm_layer) #128, 256
        self.netEnc4 = StdUnetEnc_deep2(ngf * 4, ngf * 8, norm_layer) #256, 512
        self.netEnc5 = StdUnetEnc_deep2(ngf * 8, ngf * 8, norm_layer) #512, 512

    def forward(self, input):
        output1 = self.netEnc1.forward(input)  # 256/256/64 -> 128/128/64
        output2 = self.netEnc2.forward(output1) # 128/128/64 -> 64/64/128
        output3 = self.netEnc3.forward(output2) # 64/64/128 -> 32/32/256
        output4 = self.netEnc4.forward(output3) # 32/32/256 -> 16/16/512
        output5 = self.netEnc5.forward(output4) # 16/16/512 -> 8/8/512

        return output1, output2, output3, output4, output5

class StdUnetEnc_deep1(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(StdUnetEnc_deep1, self).__init__()
        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

        #1
        nn.Conv2d(input_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),

        #2
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),

        #2 Down
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),
        ]

        return nn.Sequential(*block_full)

    def forward(self, input_img):
        return self.block(input_img)

class StdUnetEnc_deep2(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(StdUnetEnc_deep2, self).__init__()

        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

        #1
        nn.Conv2d(input_nc, output_nc, kernel_size=4,stride=2, padding=1),
        nn.LeakyReLU(0.2, True),

        #2
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),

        #3
        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
        nn.LeakyReLU(0.2, True),
        ]
        return nn.Sequential(*block_full)

    def forward(self, input_img):
        return self.block(input_img)

class StdUnetDec_deep2(nn.Module):
    def __init__(self, input_nc, output_nc,norm_layer=nn.BatchNorm2d):
        super(StdUnetDec_deep2, self).__init__()

        self.up    = self.build_up()
        self.block = self.build_block(input_nc,output_nc, norm_layer=norm_layer)

    def build_up(self):
        block_full = [  
                        # 0
                        nn.Upsample(scale_factor=2, mode = 'bilinear'),

        ]
        return nn.Sequential(*block_full)

    def build_block(self,input_nc,output_nc,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [  

                        # 1
                        nn.Conv2d(input_nc,output_nc,kernel_size=3,stride=1,padding=1, bias=use_bias),
                        nn.ReLU(True),

                        # 2
                        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
                        nn.ReLU(True),

                        # 3
                        nn.Conv2d(output_nc, output_nc, kernel_size=3,stride=1, padding=1),
                        nn.ReLU(True),

                        ]

        return nn.Sequential(*block_full)

    def forward(self, input_prev, input_skip, enc1, enc2, target_size1, target_size2):

        input_prev = self.up(input_prev)

        input_prev= F.upsample(input_prev, size = (target_size1, target_size2), mode = 'bilinear')
        enc1      = F.upsample(enc1,       size = (target_size1, target_size2), mode = 'bilinear')
        enc2      = F.upsample(enc2,       size = (target_size1, target_size2), mode = 'bilinear')
        input2    = F.upsample(input_skip, size = (target_size1, target_size2), mode = 'bilinear')

        # Should be this format
        out = torch.cat([input_prev, input2, enc1, enc2],1)
        out = self.block(out)

        return out

class StdUnetDec_deep1(nn.Module):
    def __init__(self, input_nc, output_nc,use_tanh,norm_layer=nn.BatchNorm2d):
        super(StdUnetDec_deep1, self).__init__()

        self.up    = self.build_up()
        self.block = self.build_block(input_nc,output_nc,use_tanh,norm_layer=norm_layer)

    def build_up(self):
        block_full = [  
                        # 0
                        nn.Upsample(scale_factor=2, mode = 'bilinear'),

        ]
        return nn.Sequential(*block_full)

    def build_block(self,input_nc,output_nc,use_tanh,norm_layer):
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func == nn.InstanceNorm2d
        else:
            use_bias = norm_layer == nn.InstanceNorm2d

        block_full = [

                        # 1
                        nn.Conv2d(input_nc,128,kernel_size=3,stride=1,padding=1, bias=use_bias),
                        nn.ReLU(True),

                        # 2
                        nn.Conv2d(128, 64, kernel_size=3,stride=1, padding=1),
                        nn.ReLU(True),

                        # 3
                        nn.Conv2d(64, output_nc, kernel_size=3,stride=1, padding=1),
                        #nn.InstanceNorm2d(64),
                        #nn.ReLU(True),

                        ]

        #if use_tanh:
        #    block_full += [nn.Tanh()]

        return nn.Sequential(*block_full)

    def forward(self, input_prev, input_skip, enc1, enc2, target_size1, target_size2):

        input_prev = self.up(input_prev)

        input_prev= F.upsample(input_prev,size= (target_size1, target_size2), mode = 'bilinear')
        enc1 =      F.upsample(enc1, size = (target_size1, target_size2), mode = 'bilinear')
        enc2 =      F.upsample(enc2, size = (target_size1, target_size2), mode = 'bilinear')
        input_skip= F.upsample(input_skip,size= (target_size1, target_size2), mode = 'bilinear')

        #out = self.block(torch.cat([input1, input2, enc1, enc2],1))
        out = torch.cat([input_prev, input_skip, enc1, enc2],1)
        out = self.block(out)

        #print(out.size())   

        return out

ColorTransferLib/Algorithms/HIS/util/get_data.py

from __future__ import print_function
import os
import tarfile
import requests
from warnings import warn
from zipfile import ZipFile
from bs4 import BeautifulSoup
from os.path import abspath, isdir, join, basename

class GetData(object):
    """

    Download CycleGAN or Pix2Pix Data.

    Args:
        technique : str
            One of: 'cyclegan' or 'pix2pix'.
        verbose : bool
            If True, print additional information.

    Examples:
        >>> from util.get_data import GetData
        >>> gd = GetData(technique='cyclegan')
        >>> new_data_path = gd.get(save_path='./datasets')  # options will be displayed.

    """

    def __init__(self, technique='cyclegan', verbose=True):
        url_dict = {
            'pix2pix': 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets',
            'cyclegan': 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets'
        }
        self.url = url_dict.get(technique.lower())
        self._verbose = verbose

    def _print(self, text):
        if self._verbose:
            print(text)

    @staticmethod
    def _get_options(r):
        soup = BeautifulSoup(r.text, 'lxml')
        options = [h.text for h in soup.find_all('a', href=True)
                   if h.text.endswith(('.zip', 'tar.gz'))]
        return options

    def _present_options(self):
        r = requests.get(self.url)
        options = self._get_options(r)
        print('Options:\n')
        for i, o in enumerate(options):
            print("{0}: {1}".format(i, o))
        choice = input("\nPlease enter the number of the "
                       "dataset above you wish to download:")
        return options[int(choice)]

    def _download_data(self, dataset_url, save_path):
        if not isdir(save_path):
            os.makedirs(save_path)

        base = basename(dataset_url)
        temp_save_path = join(save_path, base)

        with open(temp_save_path, "wb") as f:
            r = requests.get(dataset_url)
            f.write(r.content)

        if base.endswith('.tar.gz'):
            obj = tarfile.open(temp_save_path)
        elif base.endswith('.zip'):
            obj = ZipFile(temp_save_path, 'r')
        else:
            raise ValueError("Unknown File Type: {0}.".format(base))

        self._print("Unpacking Data...")
        obj.extractall(save_path)
        obj.close()
        os.remove(temp_save_path)

    def get(self, save_path, dataset=None):
        """

        Download a dataset.

        Args:
            save_path : str
                A directory to save the data to.
            dataset : str, optional
                A specific dataset to download.
                Note: this must include the file extension.
                If None, options will be presented for you
                to choose from.

        Returns:
            save_path_full : str
                The absolute path to the downloaded data.

        """
        if dataset is None:
            selected_dataset = self._present_options()
        else:
            selected_dataset = dataset

        save_path_full = join(save_path, selected_dataset.split('.')[0])

        if isdir(save_path_full):
            warn("\n'{0}' already exists. Voiding Download.".format(
                save_path_full))
        else:
            self._print('Downloading Data...')
            url = "{0}/{1}".format(self.url, selected_dataset)
            self._download_data(url, save_path=save_path)

        return abspath(save_path_full)

ColorTransferLib/Algorithms/HIS/util/image_pool.py

import random
import numpy as np
import torch
from torch.autograd import Variable

class ImagePool():
    def __init__(self, pool_size):
        self.pool_size = pool_size
        if self.pool_size > 0:
            self.num_imgs = 0
            self.images = []

    def query(self, images):
        if self.pool_size == 0:
            return Variable(images)
        return_images = []
        for image in images:
            image = torch.unsqueeze(image, 0)
            if self.num_imgs < self.pool_size:
                self.num_imgs = self.num_imgs + 1
                self.images.append(image)
                return_images.append(image)
            else:
                p = random.uniform(0, 1)
                if p > 0.5:
                    random_id = random.randint(0, self.pool_size-1)
                    tmp = self.images[random_id].clone()
                    self.images[random_id] = image
                    return_images.append(tmp)
                else:
                    return_images.append(image)
        return_images = Variable(torch.cat(return_images, 0))
        return return_images

ColorTransferLib/Algorithms/HIS/util/visualizer.py

import numpy as np
import os
import ntpath
import time
from . import util
from . import html
# from scipy.misc import imresize
from skimage.transform import resize

class Visualizer():
    def __init__(self, opt):
        # self.opt = opt
        self.display_id = opt.display_id
        self.use_html = opt.isTrain and not opt.no_html
        self.win_size = opt.display_winsize
        self.name = opt.name
        self.opt = opt
        self.saved = False
        if self.display_id > 0:
            import visdom
            self.vis = visdom.Visdom(port=opt.display_port)

        if self.use_html:
            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web')
            self.img_dir = os.path.join(self.web_dir, 'images')
            print('create web directory %s...' % self.web_dir)
            util.mkdirs([self.web_dir, self.img_dir])
        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.txt')
        with open(self.log_name, "a") as log_file:
            now = time.strftime("%c")
            log_file.write('================ Training Loss (%s) ================\n' % now)

    def reset(self):
        self.saved = False

    # |visuals|: dictionary of images to display or save
    def display_current_results(self, visuals, epoch, save_result):
        if self.display_id > 0:  # show images in the browser
            ncols = self.opt.display_single_pane_ncols
            if ncols > 0:
                h, w = next(iter(visuals.values())).shape[:2]
                table_css = """<style>
                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}
                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}
                        </style>""" % (w, h)
                title = self.name
                label_html = ''
                label_html_row = ''
                nrows = int(np.ceil(len(visuals.items()) / ncols))
                images = []
                idx = 0
                for label, image_numpy in visuals.items():
                    label_html_row += '<td>%s</td>' % label
                    images.append(image_numpy.transpose([2, 0, 1]))
                    idx += 1
                    if idx % ncols == 0:
                        label_html += '<tr>%s</tr>' % label_html_row
                        label_html_row = ''
                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255
                while idx % ncols != 0:
                    images.append(white_image)
                    label_html_row += '<td></td>'
                    idx += 1
                if label_html_row != '':
                    label_html += '<tr>%s</tr>' % label_html_row
                # pane col = image row
                self.vis.images(images, nrow=ncols, win=self.display_id + 1,
                                padding=2, opts=dict(title=title + ' images'), env=self.opt.display_env)
                label_html = '<table>%s</table>' % label_html
                self.vis.text(table_css + label_html, win=self.display_id + 2,
                              opts=dict(title=title + ' labels'), env=self.opt.display_env)
            else:
                idx = 1
                for label, image_numpy in visuals.items():
                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),
                                   win=self.display_id + idx, env=self.opt.display_env)
                    idx += 1

        if self.use_html and (save_result or not self.saved):  # save images to a html file
            self.saved = True
            for label, image_numpy in visuals.items():
                img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))
                util.save_image(image_numpy, img_path)
            # update website
            webpage = html.HTML(self.web_dir, 'Experiment name = %s' % self.name, reflesh=1)
            for n in range(epoch, 0, -1):
                webpage.add_header('epoch [%d]' % n)
                ims = []
                txts = []
                links = []

                for label, image_numpy in visuals.items():
                    img_path = 'epoch%.3d_%s.png' % (n, label)
                    ims.append(img_path)
                    txts.append(label)
                    links.append(img_path)
                webpage.add_images(ims, txts, links, width=self.win_size)
            webpage.save()

    # errors: dictionary of error labels and values
    def plot_current_errors(self, epoch, counter_ratio, opt, errors):
        if not hasattr(self, 'plot_data'):
            self.plot_data = {'X': [], 'Y': [], 'legend': list(errors.keys())}
        self.plot_data['X'].append(epoch + counter_ratio)
        self.plot_data['Y'].append([errors[k] for k in self.plot_data['legend']])
        self.vis.line(
            X=np.stack([np.array(self.plot_data['X'])] * len(self.plot_data['legend']), 1),
            Y=np.array(self.plot_data['Y']),
            opts={
                'title': self.name + ' loss over time',
                'legend': self.plot_data['legend'],
                'xlabel': 'epoch',
                'ylabel': 'loss'},
            win=self.display_id,
            env=self.opt.display_env)

    # errors: same format as |errors| of plotCurrentErrors
    def print_current_errors(self, epoch, i, errors, t):
        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)
        for k, v in errors.items():
            message += '%s: %.3f ' % (k, v)

        print(message)
        with open(self.log_name, "a") as log_file:
            log_file.write('%s\n' % message)

    # save image to the disk
    def save_images(self, webpage, visuals, image_path, aspect_ratio=1.0):
        image_dir = webpage.get_image_dir()
        short_path = ntpath.basename(image_path[0])
        name = os.path.splitext(short_path)[0]

        webpage.add_header(name)
        ims = []
        txts = []
        links = []

        for label, im in visuals.items():
            image_name = '%s_%s.png' % (name, label)
            save_path = os.path.join(image_dir, image_name)
            h, w, _ = im.shape
            if aspect_ratio > 1.0:
                # im = imresize(im, (h, int(w * aspect_ratio)), interp='bicubic')
                im = resize(im, (h, int(w * aspect_ratio)))
            if aspect_ratio < 1.0:
                # im = imresize(im, (int(h / aspect_ratio), w), interp='bicubic')
                im = resize(im, (int(h / aspect_ratio), w))
            util.save_image(im, save_path)

            ims.append(image_name)
            txts.append(label)
            links.append(image_name)
        webpage.add_images(ims, txts, links, width=self.win_size)

ColorTransferLib/Algorithms/MKL/MKL.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import time
from scipy.linalg import fractional_matrix_power
from copy import deepcopy

from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: The Linear Monge-Kantorovitch Linear Colour Mapping forExample-Based Colour Transfer
#   Author: Erik Reinhard, Michael Ashikhmin, Bruce Gooch, Peter Shirley
#   Published in: 4th European Conference on Visual Media Production
#   Year of Publication: 2007
#
# Abstract:
#   A common task in image editing is to change the colours of a picture to match the desired colour grade of another 
#   picture. Finding the correct colour mapping is tricky because it involves numerous interrelated operations, like 
#   balancing the colours, mixing the colour channels or adjusting the contrast. Recently, a number of automated tools 
#   have been proposed to find an adequate one-to-one colour mapping. The focus in this paper is on finding the best 
#   linear colour transformation. Linear transformations have been proposed in the literature but independently. The aim 
#   of this paper is thus to establish a common mathematical background to all these methods. Also, this paper proposes 
#   a novel transformation, which is derived from the Monge-Kantorovitch theory of mass transportation. The proposed 
#   solution is optimal in the sense that it minimises the amount of changes in the picture colours. It favourably 
#   compares theoretically and experimentally with other techniques for various images and under various colour spaces.
#
# Info:
#   Name: MongeKLColorTransfer
#   Identifier: MKL
#   Link: https://doi.org/10.1049/cp:20070055
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class MKL:
    compatibility = {
        "src": ["Image", "Mesh", "PointCloud"],
        "ref": ["Image", "Mesh", "PointCloud"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "MKL",
            "title": "The Linear Monge-Kantorovitch Linear Colour Mapping forExample-Based Colour Transfer",
            "year": 2007,
            "abstract": "A common task in image editing is to change the colours of a picture to match the desired " 
                        "colour grade of another picture. Finding the correct colour mapping is tricky because it "
                        "involves numerous interrelated operations, like balancing the colours, mixing the colour "
                        "channels or adjusting the contrast. Recently, a number of automated tools have been proposed "
                        "to find an adequate one-to-one colour mapping. The focus in this paper is on finding the best "
                        "linear colour transformation. Linear transformations have been proposed in the literature but "
                        "independently. The aim of this paper is thus to establish a common mathematical background to "
                        "all these methods. Also, this paper proposes a novel transformation, which is derived from "
                        "the Monge-Kantorovitch theory of mass transportation. The proposed solution is optimal in the "
                        "sense that it minimises the amount of changes in the picture colours. It favourably compares "
                        "theoretically and experimentally with other techniques for various images and under various "
                        "colour spaces.",
            "types": ["Image", "Mesh", "PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, MKL.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        src_color = src.get_colors()
        ref_color = ref.get_colors()
        out_img = deepcopy(src)

        src_color_rgb = src_color.reshape(src_color.shape[0], 3)
        ref_color_rgb = ref_color.reshape(ref_color.shape[0], 3)

        src_cov = np.cov(src_color_rgb, rowvar=False)
        ref_cov = np.cov(ref_color_rgb, rowvar=False)

        src_covs = fractional_matrix_power(src_cov, 0.5)
        src_covsr = fractional_matrix_power(src_cov, -0.5)

        T = np.dot(src_covsr, np.dot(fractional_matrix_power(np.dot(src_covs, np.dot(ref_cov, src_covs)), 0.5), src_covsr))

        mean_src = np.mean(src_color_rgb, axis=0)
        mean_ref = np.mean(ref_color_rgb, axis=0)
        out = (src_color_rgb - mean_src) @ T + mean_ref
        out = np.reshape(out, src_color.shape)
        out = np.real(out)
        out = np.clip(out, 0, 1)

        out_img.set_colors(out)
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/HIS/util/util.py

from __future__ import print_function
import torch
import numpy as np
from PIL import Image
import inspect
import re
import numpy as np
import os
import collections
from skimage import color
import fnmatch

def LAB2RGB(I):
    l = I[:, :, 0] / 255.0 * 100.0
    a = I[:, :, 1] / 255.0 * (98.2330538631 + 86.1830297444) - 86.1830297444
    b = I[:, :, 2] / 255.0 * (94.4781222765 + 107.857300207) - 107.857300207

    rgb = color.lab2rgb(np.dstack([l, a, b]).astype(np.float64))
    return rgb

def HSV2RGB(I):

    h = I[:, :, 0] / 255.0 * 360.0
    s = I[:, :, 1] / 255.0 * 100.0
    v = I[:, :, 2] / 255.0 * 100.0

    hsv = color.hsv2rgb(np.dstack([h, s, v]).astype(np.float64))*255
    return hsv

# Converts a Tensor into a Numpy array
# |imtype|: the desired type of the converted numpy array

def tensor2im(image_tensor,img_type,imtype=np.uint8):
    image_numpy = image_tensor.detach().cpu().numpy()
    if image_numpy.shape[1] == 1:
        image_numpy = np.tile(image_numpy, (1, 3, 1, 1))
    image_numpy = ((np.transpose(image_numpy, (0, 2, 3, 1)) * 0.5) + 0.5) * 255.0 #-1~1  => 

    # Additional
    image_numpy = image_numpy[0]
    if img_type == 'rgb':
        # image_numpy = np.clip(image_numpy,0,255).astype(np.uint8)
        image_numpy = image_numpy / 255.
        image_numpy = image_numpy * 10.
    elif img_type == 'hsv':
        image_numpy = HSV2RGB(image_numpy) # for Lab to RGB image
        image_numpy = np.clip(image_numpy,0,255)        
    elif img_type == 'lab':
        image_numpy = LAB2RGB(image_numpy) # for Lab to RGB image
        # image_numpy = np.clip(image_numpy,0,255)        
    else:
        print(ERROR)

    # image = torch.from_numpy(image_numpy.transpose(2,0,1)).byte().cuda()
    if not torch.cuda.is_available():
        image = torch.from_numpy(image_numpy.transpose(2,0,1)).cpu()
    else:
        image = torch.from_numpy(image_numpy.transpose(2,0,1)).cuda()

    return image

def diagnose_network(net, name='network'):
    mean = 0.0
    count = 0
    for param in net.parameters():
        if param.grad is not None:
            mean += torch.mean(torch.abs(param.grad.data))
            count += 1
    if count > 0:
        mean = mean / count

def save_image(image_numpy, image_path):
    image_pil = Image.fromarray(image_numpy)
    image_pil.save(image_path)

def print_numpy(x, val=True, shp=False):
    x = x.astype(np.float64)
    if shp:
        print('shape,', x.shape)
    if val:
        x = x.flatten()
        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (
            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))

def mkdirs(paths):
    if isinstance(paths, list) and not isinstance(paths, str):
        for path in paths:
            mkdir(path)
    else:
        mkdir(paths)

def mkdir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def remove_file_end_with(path, regex):
    file_paths = get_file_path(path, [regex])

    for i in np.arange(len(file_paths)):
        os.remove(file_paths[i])

def get_file_path(path, regex):
    file_path = []
    for root, dirnames, filenames in os.walk(path):
        for i in np.arange(len(regex)):
            for filename in fnmatch.filter(filenames, regex[i]):
                file_path.append(os.path.join(root, filename))

    return file_path

ColorTransferLib/Algorithms/MKL/init.py

from . import MKL

ColorTransferLib/Algorithms/HIS/models/modules/init.py

ColorTransferLib/Algorithms/NST/LICENSE.txt

                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    {one line to give the program's name and a brief idea of what it does.}
    Copyright (C) {year}  {name of author}

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    {project}  Copyright (C) {year}  {fullname}
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<http://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<http://www.gnu.org/philosophy/why-not-lgpl.html>.

ColorTransferLib/Algorithms/NST/NST.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.

Adaptation of https://github.com/cysmith/neural-style-tf
"""

import tensorflow as tf
import numpy as np
import torch
import time
import cv2
import os
from copy import deepcopy

from ColorTransferLib.Utils.Helper import check_compatibility
from ColorTransferLib.Algorithms.NST.Model import Model

physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: A Neural Algorithm of Artistic Style
#   Author: Leon A. Gatys, Alexander S. Ecker, Matthias Bethge
#   Published in: arXiv
#   Year of Publication: 2015
#
# Abstract:
#    In fine art, especially painting, humans have mastered the skill to create unique visual experiences through
#    composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this
#    process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of
#    visual perception such as object and face recognition near-human performance was recently demonstrated by a class
#    of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on
#    a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural
#    representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for
#    the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised
#    artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of
#    how humans create and perceive artistic imagery.
#
# Info:
#   Name: NeuralStyleTransfer
#   Identifier: NST
#   Link: https://doi.org/10.48550/arXiv.1508.06576
#   Original Source Code: https://github.com/cysmith/neural-style-tf
#
# Misc:
#   Note: the neural_style.py is adapted to support TensorFLow 2
#
# Implementation Details:
#   optimizer = ADAM, mbecause L-BFGS not working on TF2
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class NST:
    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "NST",
            "title": "A Neural Algorithm of Artistic Style",
            "year": 2015,
            "abstract": "In fine art, especially painting, humans have mastered the skill to create unique visual "
                        "experiences through composing a complex interplay between the content and style of an image. "
                        "Thus far the algorithmic basis of this process is unknown and there exists no artificial "
                        "system with similar capabilities. However, in other key areas ofvisual perception such as "
                        "object and face recognition near-human performance was recently demonstrated by a class "
                        "of biologically inspired vision models called Deep Neural Networks. Here we introduce an "
                        "artificial system based on a Deep Neural Network that creates artistic images of high "
                        "perceptual quality. The system uses neural representations to separate and recombine content "
                        "and style of arbitrary images, providing a neural algorithm for the creation of artistic "
                        "images. Moreover, in light of the striking similarities between performance-optimised "
                        "artificial neural networks and biological vision, our work offers a path forward to an "
                        "algorithmic understanding of how humans create and perceive artistic imagery.",
            "types": ["Image"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, NST.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        if not torch.cuda.is_available():
            opt.device = "/cpu:0"

        # Preprocessing
        src_img = src.get_raw() * 255.0
        ref_img = ref.get_raw() * 255.0
        out_img = deepcopy(src)

        opt.style_layer_weights = NST.normalize(opt.style_layer_weights)
        opt.content_layer_weights = NST.normalize(opt.content_layer_weights)
        opt.style_imgs_weights = NST.normalize(opt.style_imgs_weights)

        h, w, d = src_img.shape
        mx = opt.max_size
        # resize if > max size
        if h > w and h > mx:
            w = (float(mx) / float(h)) * w
            src_img = cv2.resize(src_img, dsize=(int(w), mx), interpolation=cv2.INTER_AREA)
        if w > mx:
            h = (float(mx) / float(w)) * h
            src_img = cv2.resize(src_img, dsize=(mx, int(h)), interpolation=cv2.INTER_AREA)
        src_img = NST.preprocess(src_img)

        _, ch, cw, cd = src_img.shape
        style_imgs = []
        ref_img = cv2.resize(ref_img, dsize=(cw, ch), interpolation=cv2.INTER_AREA)
        ref_img = NST.preprocess(ref_img)
        style_imgs.append(ref_img)

        out = NST.render_single_image(src_img, style_imgs, opt)
        out = NST.postprocess(out)
        tf.compat.v1.reset_default_graph()

        out_img.set_raw(out, normalized=True)
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

    # ------------------------------------------------------------------------------------------------------------------
    # 'a neural algorithm for artistic style' loss functions
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def content_layer_loss(p, x, opt):
        _, h, w, d = p.get_shape()
        M = h * w
        N = d
        if opt.content_loss_function == 1:
            K = 1. / (2. * N ** 0.5 * M ** 0.5)
        elif opt.content_loss_function == 2:
            K = 1. / (N * M)
        elif opt.content_loss_function == 3:
            K = 1. / 2.
        loss = K * tf.reduce_sum(input_tensor=tf.pow((x - p), 2))
        return loss

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def style_layer_loss(a, x):
        _, h, w, d = a.get_shape()
        M = h * w
        N = d
        A = NST.gram_matrix(a, M, N)
        G = NST.gram_matrix(x, M, N)
        loss = (1. / (4 * N ** 2 * M ** 2)) * tf.reduce_sum(input_tensor=tf.pow((G - A), 2))
        return loss

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gram_matrix(x, area, depth):
        F = tf.reshape(x, (area, depth))
        G = tf.matmul(tf.transpose(a=F), F)
        return G

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def mask_style_layer(a, x, mask_img, opt):
        _, h, w, d = a.get_shape()
        mask = NST.get_mask_image(mask_img, w, h, opt)
        mask = tf.convert_to_tensor(value=mask)
        tensors = []
        for _ in range(d):
            tensors.append(mask)
        mask = tf.stack(tensors, axis=2)
        mask = tf.stack(mask, axis=0)
        mask = tf.expand_dims(mask, 0)
        a = tf.multiply(a, mask)
        x = tf.multiply(x, mask)
        return a, x

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def sum_masked_style_losses(sess, net, style_imgs, opt):
        total_style_loss = 0.
        weights = opt.style_imgs_weights
        masks = opt.style_mask_imgs

        for img, img_weight, img_mask in zip(style_imgs, weights, masks):
            sess.run(net['input'].assign(img))
            style_loss = 0.
            for layer, weight in zip(opt.style_layers, opt.style_layer_weights):
                a = sess.run(net[layer])
                x = net[layer]
                a = tf.convert_to_tensor(value=a)
                a, x = NST.mask_style_layer(a, x, img_mask, opt)
                style_loss += NST.style_layer_loss(a, x) * weight
            style_loss /= float(len(opt.style_layers))
            total_style_loss += (style_loss * img_weight)
        total_style_loss /= float(len(style_imgs))
        return total_style_loss

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def sum_style_losses(sess, net, style_imgs, opt):
        total_style_loss = 0.
        weights = opt.style_imgs_weights
        for img, img_weight in zip(style_imgs, weights):
            sess.run(net['input'].assign(img))
            style_loss = 0.
            for layer, weight in zip(opt.style_layers, opt.style_layer_weights):
                a = sess.run(net[layer])
                x = net[layer]
                a = tf.convert_to_tensor(value=a)
                style_loss += NST.style_layer_loss(a, x) * weight
            style_loss /= float(len(opt.style_layers))
            total_style_loss += (style_loss * img_weight)
        total_style_loss /= float(len(style_imgs))
        return total_style_loss

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def sum_content_losses(sess, net, content_img, opt):
        sess.run(net['input'].assign(content_img))
        content_loss = 0.
        for layer, weight in zip(opt.content_layers, opt.content_layer_weights):
            p = sess.run(net[layer])
            x = net[layer]
            p = tf.convert_to_tensor(value=p)
            content_loss += NST.content_layer_loss(p, x, opt) * weight
        content_loss /= float(len(opt.content_layers))
        return content_loss

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def preprocess(img):
        imgpre = np.copy(img)
        # bgr to rgb
        imgpre = imgpre[..., ::-1]
        # shape (h, w, d) to (1, h, w, d)
        imgpre = imgpre[np.newaxis, :, :, :]
        imgpre -= np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))
        return imgpre

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def postprocess(img):
        imgpost = np.copy(img)
        imgpost += np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))
        # shape (1, h, w, d) to (h, w, d)
        imgpost = imgpost[0]
        imgpost = np.clip(imgpost, 0, 255).astype('uint8')
        # rgb to bgr
        imgpost = imgpost[..., ::-1]
        return imgpost

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def normalize(weights):
        denom = sum(weights)
        if denom > 0.:
            return [float(i) / denom for i in weights]
        else:
            return [0.] * len(weights)

    # ------------------------------------------------------------------------------------------------------------------
    # rendering -- where the magic happens
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def stylize(content_img, style_imgs, init_img, opt, frame=None):
        with tf.device(opt.device), tf.compat.v1.Session() as sess:
            # setup network
            net = Model(content_img, opt).net

            # style loss
            if opt.style_mask:
                L_style = NST.sum_masked_style_losses(sess, net, style_imgs, opt)
            else:
                L_style = NST.sum_style_losses(sess, net, style_imgs, opt)

            # content loss
            L_content = NST.sum_content_losses(sess, net, content_img, opt)

            # denoising loss
            L_tv = tf.image.total_variation(net['input'])

            # loss weights
            alpha = opt.content_weight
            beta = opt.style_weight
            theta = opt.tv_weight

            # total loss
            L_total = alpha * L_content
            L_total += beta * L_style
            L_total += theta * L_tv

            # optimization algorithm
            optimizer = NST.get_optimizer(L_total, opt)

            if opt.optimizer == 'adam':
                NST.minimize_with_adam(sess, net, optimizer, init_img, L_total, opt)
            elif opt.optimizer == 'lbfgs':
                NST.minimize_with_lbfgs(sess, net, optimizer, init_img, opt)

            output_img = sess.run(net['input'])

            if opt.original_colors:
                output_img = NST.convert_to_original_colors(np.copy(content_img), output_img, opt)

            sess.close()

            return output_img

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def minimize_with_lbfgs(sess, net, optimizer, init_img, opt):
        if opt.verbose:
            print('\nMINIMIZING LOSS USING: L-BFGS OPTIMIZER')
        init_op = tf.compat.v1.global_variables_initializer()
        sess.run(init_op)
        sess.run(net['input'].assign(init_img))
        optimizer.minimize(sess)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def minimize_with_adam(sess, net, optimizer, init_img, loss, opt):
        if opt.verbose:
            print('\nMINIMIZING LOSS USING: ADAM OPTIMIZER')
        train_op = optimizer.minimize(loss)
        init_op = tf.compat.v1.global_variables_initializer()
        sess.run(init_op)
        sess.run(net['input'].assign(init_img))
        iterations = 0
        while iterations < opt.max_iterations:
            sess.run(train_op)
            if iterations % opt.print_iterations == 0 and opt.verbose:
                curr_loss = loss.eval()
                print("At iterate {}\tf=  {}".format(iterations, curr_loss))
            iterations += 1

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_optimizer(loss, opt):
        print_iterations = opt.print_iterations if opt.verbose else 0
        if opt.optimizer == 'lbfgs':
            optimizer = tf.contrib.opt.ScipyOptimizerInterface(
                loss, method='L-BFGS-B',
                options={'maxiter': opt.max_iterations,
                         'disp': print_iterations})
        elif opt.optimizer == 'adam':
            optimizer = tf.compat.v1.train.AdamOptimizer(opt.learning_rate)
        return optimizer

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_mask_image(mask_img, width, height, opt):
        path = os.path.join(opt.content_img_dir, mask_img)
        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_AREA)
        img = img.astype(np.float32)
        mx = np.amax(img)
        img /= mx
        return img

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def convert_to_original_colors(content_img, stylized_img, opt):
        content_img = NST.postprocess(content_img)
        stylized_img = NST.postprocess(stylized_img)
        if opt.color_convert_type == 'yuv':
            cvt_type = cv2.COLOR_BGR2YUV
            inv_cvt_type = cv2.COLOR_YUV2BGR
        elif opt.color_convert_type == 'ycrcb':
            cvt_type = cv2.COLOR_BGR2YCR_CB
            inv_cvt_type = cv2.COLOR_YCR_CB2BGR
        elif opt.color_convert_type == 'luv':
            cvt_type = cv2.COLOR_BGR2LUV
            inv_cvt_type = cv2.COLOR_LUV2BGR
        elif opt.color_convert_type == 'lab':
            cvt_type = cv2.COLOR_BGR2LAB
            inv_cvt_type = cv2.COLOR_LAB2BGR
        content_cvt = cv2.cvtColor(content_img, cvt_type)
        stylized_cvt = cv2.cvtColor(stylized_img, cvt_type)
        c1, _, _ = cv2.split(stylized_cvt)
        _, c2, c3 = cv2.split(content_cvt)
        merged = cv2.merge((c1, c2, c3))
        dst = cv2.cvtColor(merged, inv_cvt_type).astype(np.float32)
        dst = NST.preprocess(dst)
        return dst

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def render_single_image(content_img, style_img, opt):
        with tf.Graph().as_default():
            print('\n---- RENDERING SINGLE IMAGE ----\n')
            tick = time.time()
            output_img = NST.stylize(content_img, style_img, content_img, opt)
            tock = time.time()
            print('Single image elapsed time: {}'.format(tock - tick))
            return output_img

ColorTransferLib/Algorithms/NST/Model.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.

Adaptation of https://github.com/cysmith/neural-style-tf
"""

import tensorflow as tf
import numpy as np
import scipy.io

"""
Error: CUBLAS_STATUS_NOT_iNITIALIZED 
Source: https://forums.developer.nvidia.com/t/cublas-status-not-initialized/177955
If that solution fixes it, the problem is due to the fact that TF has a greedy allocation method (when you donâ€™t set 
allow_growth). This greedy allocation method uses up nearly all GPU memory. When CUBLAS is asked to initialize (later), 
it requires some GPU memory to initialize. There is not enough memory left for CUBLAS to initialize, so the CUBLAS 
initialization fails.
"""
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Model:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, input_img, opt):
        self.opt = opt
        self.net = self.build_model(input_img, opt)

    # ----------------------------------------------------------------------------------------------------------------------
    # pre-trained vgg19 convolutional neural network
    # remark: layers are manually initialized for clarity.
    # ----------------------------------------------------------------------------------------------------------------------
    def build_model(self, input_img, opt):
        if opt.verbose:
            print('\nBUILDING VGG-19 NETWORK')
        net = {}
        _, h, w, d = input_img.shape

        if opt.verbose:
            print('loading model weights...')
        vgg_rawnet = scipy.io.loadmat(opt.model_weights)
        vgg_layers = vgg_rawnet['layers'][0]
        if opt.verbose:
            print('constructing layers...')
        net['input'] = tf.Variable(np.zeros((1, h, w, d), dtype=np.float32))

        if opt.verbose:
            print('LAYER GROUP 1')
        net['conv1_1'] = self.conv_layer('conv1_1', net['input'], W=self.get_weights(vgg_layers, 0))
        net['relu1_1'] = self.relu_layer('relu1_1', net['conv1_1'], b=self.get_bias(vgg_layers, 0))

        net['conv1_2'] = self.conv_layer('conv1_2', net['relu1_1'], W=self.get_weights(vgg_layers, 2))
        net['relu1_2'] = self.relu_layer('relu1_2', net['conv1_2'], b=self.get_bias(vgg_layers, 2))

        net['pool1'] = self.pool_layer('pool1', net['relu1_2'])

        if opt.verbose:
            print('LAYER GROUP 2')
        net['conv2_1'] = self.conv_layer('conv2_1', net['pool1'], W=self.get_weights(vgg_layers, 5))
        net['relu2_1'] = self.relu_layer('relu2_1', net['conv2_1'], b=self.get_bias(vgg_layers, 5))

        net['conv2_2'] = self.conv_layer('conv2_2', net['relu2_1'], W=self.get_weights(vgg_layers, 7))
        net['relu2_2'] = self.relu_layer('relu2_2', net['conv2_2'], b=self.get_bias(vgg_layers, 7))

        net['pool2'] = self.pool_layer('pool2', net['relu2_2'])

        if opt.verbose:
            print('LAYER GROUP 3')
        net['conv3_1'] = self.conv_layer('conv3_1', net['pool2'], W=self.get_weights(vgg_layers, 10))
        net['relu3_1'] = self.relu_layer('relu3_1', net['conv3_1'], b=self.get_bias(vgg_layers, 10))

        net['conv3_2'] = self.conv_layer('conv3_2', net['relu3_1'], W=self.get_weights(vgg_layers, 12))
        net['relu3_2'] = self.relu_layer('relu3_2', net['conv3_2'], b=self.get_bias(vgg_layers, 12))

        net['conv3_3'] = self.conv_layer('conv3_3', net['relu3_2'], W=self.get_weights(vgg_layers, 14))
        net['relu3_3'] = self.relu_layer('relu3_3', net['conv3_3'], b=self.get_bias(vgg_layers, 14))

        net['conv3_4'] = self.conv_layer('conv3_4', net['relu3_3'], W=self.get_weights(vgg_layers, 16))
        net['relu3_4'] = self.relu_layer('relu3_4', net['conv3_4'], b=self.get_bias(vgg_layers, 16))

        net['pool3'] = self.pool_layer('pool3', net['relu3_4'])

        if opt.verbose:
            print('LAYER GROUP 4')
        net['conv4_1'] = self.conv_layer('conv4_1', net['pool3'], W=self.get_weights(vgg_layers, 19))
        net['relu4_1'] = self.relu_layer('relu4_1', net['conv4_1'], b=self.get_bias(vgg_layers, 19))

        net['conv4_2'] = self.conv_layer('conv4_2', net['relu4_1'], W=self.get_weights(vgg_layers, 21))
        net['relu4_2'] = self.relu_layer('relu4_2', net['conv4_2'], b=self.get_bias(vgg_layers, 21))

        net['conv4_3'] = self.conv_layer('conv4_3', net['relu4_2'], W=self.get_weights(vgg_layers, 23))
        net['relu4_3'] = self.relu_layer('relu4_3', net['conv4_3'], b=self.get_bias(vgg_layers, 23))

        net['conv4_4'] = self.conv_layer('conv4_4', net['relu4_3'], W=self.get_weights(vgg_layers, 25))
        net['relu4_4'] = self.relu_layer('relu4_4', net['conv4_4'], b=self.get_bias(vgg_layers, 25))

        net['pool4'] = self.pool_layer('pool4', net['relu4_4'])

        if opt.verbose:
            print('LAYER GROUP 5')
        net['conv5_1'] = self.conv_layer('conv5_1', net['pool4'], W=self.get_weights(vgg_layers, 28))
        net['relu5_1'] = self.relu_layer('relu5_1', net['conv5_1'], b=self.get_bias(vgg_layers, 28))

        net['conv5_2'] = self.conv_layer('conv5_2', net['relu5_1'], W=self.get_weights(vgg_layers, 30))
        net['relu5_2'] = self.relu_layer('relu5_2', net['conv5_2'], b=self.get_bias(vgg_layers, 30))

        net['conv5_3'] = self.conv_layer('conv5_3', net['relu5_2'], W=self.get_weights(vgg_layers, 32))
        net['relu5_3'] = self.relu_layer('relu5_3', net['conv5_3'], b=self.get_bias(vgg_layers, 32))

        net['conv5_4'] = self.conv_layer('conv5_4', net['relu5_3'], W=self.get_weights(vgg_layers, 34))
        net['relu5_4'] = self.relu_layer('relu5_4', net['conv5_4'], b=self.get_bias(vgg_layers, 34))

        net['pool5'] = self.pool_layer('pool5', net['relu5_4'])

        return net

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def conv_layer(self, layer_name, layer_input, W):
        conv = tf.nn.conv2d(input=layer_input, filters=W, strides=[1, 1, 1, 1], padding='SAME')
        if self.opt.verbose:
            print('--{} | shape={} | weights_shape={}'.format(layer_name, conv.get_shape(), W.get_shape()))
        return conv

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def relu_layer(self, layer_name, layer_input, b):
        relu = tf.nn.relu(layer_input + b)
        if self.opt.verbose:
            print('--{} | shape={} | bias_shape={}'.format(layer_name, relu.get_shape(), b.get_shape()))
        return relu

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def pool_layer(self, layer_name, layer_input):
        if self.opt.pooling_type == 'avg':
            pool = tf.nn.avg_pool2d(input=layer_input,
                                    ksize=[1, 2, 2, 1],
                                    strides=[1, 2, 2, 1],
                                    padding='SAME')
        elif self.opt.pooling_type == 'max':
            pool = tf.nn.max_pool2d(input=layer_input,
                                    ksize=[1, 2, 2, 1],
                                    strides=[1, 2, 2, 1],
                                    padding='SAME')
        if self.opt.verbose:
            print('--{}   | shape={}'.format(layer_name, pool.get_shape()))
        return pool

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def get_weights(self, vgg_layers, i):
        weights = vgg_layers[i][0][0][2][0][0]
        W = tf.constant(weights)
        return W

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def get_bias(self, vgg_layers, i):
        bias = vgg_layers[i][0][0][2][0][1]
        b = tf.constant(np.reshape(bias, (bias.size)))
        return b

ColorTransferLib/Algorithms/NST/init.py

from . import NST

ColorTransferLib/Algorithms/PSN/PSN.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import configparser
from pprint import pprint as ppt
import tensorflow as tf
import os

from ColorTransferLib.Algorithms.PSN.psnet import PSNet
from ColorTransferLib.Algorithms.PSN.utils import *
from ColorTransferLib.Utils.Helper import check_compatibility

opj = os.path.join
ope = os.path.exists
om = os.mkdir

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color
#   Author: Cao, Xu and Wang, Weimin and Nagao, Katashi and Nakamura, Ryosuke
#   Published in: IEEE Computer Graphics and Applications
#   Year of Publication: 2020
#
# Abstract:
#   We propose a neural style transfer method for colored point clouds which allows stylizing the geometry and/or color
#   property of a point cloud from another. The stylization is achieved by manipulating the content representations and
#   Gram-based style representations extracted from a pre-trained PointNet-based classification network for colored
#   point clouds. As Gram-based style representation is invariant to the number or the order of points, the style can
#   also be an image in the case of stylizing the color property of a point cloud by merely treating the image as a set
#   of pixels.Experimental results and analysis demonstrate the capability of the proposed method for stylizing a
#   point cloud either from another point cloud or an image.
#
# Info:
#   Name: PSNetStyleTransfer
#   Identifier: PSN
#   Link: https://doi.org/10.1109/WACV45572.2020.9093513
#   Source: https://github.com/hoshino042/psnet
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class PSN:
    compatibility = {
        "src": ["PointCloud"],
        "ref": ["PointCloud"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "PSN",
            "title": "PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color",
            "year": 2020,
            "abstract": "We propose a neural style transfer method for colored point clouds which allows stylizing the "
                        "geometry and/or color property of a point cloud from another. The stylization is achieved by "
                        "manipulating the content representations and Gram-based style representations extracted from "
                        "a pre-trained PointNet-based classification network for colored point clouds. As Gram-based "
                        "style representation is invariant to the number or the order of points, the style can also be "
                        "an image in the case of stylizing the color property of a point cloud by merely treating the "
                        "image as a set of pixels.Experimental results and analysis demonstrate the capability of the "
                        "proposed method for stylizing a point cloud either from another point cloud or an image.",
            "types": ["PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt): 
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, PSN.compatibility)
        if output["status_code"] != 0:
            output["response"] = "Incompatible type."
            return output

        iteration = opt.iterations  # iteration number for style transfer
        geotransfer = opt.geotransfer

        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'  # Uncomment this line if you are using macOS

        np.random.seed(42)
        trained_model = "Models/PSN/model"

        config = configparser.ConfigParser()
        config.read("Models/PSN/base_config.ini")

        content_layer = list(map(lambda x: int(x), config["style_transfer"]["content_layer"].split(",")))
        style_layer = list(map(lambda x: int(x), config["style_transfer"]["content_layer"].split(",")))
        use_content_color = ["FE_COLOR_FE_{}".format(i) for i in content_layer]
        use_style_color = ["FE_COLOR_FE_{}".format(i) for i in style_layer]

        content_geo = src.get_vertex_positions().reshape(src.get_num_vertices(), 3)
        content_ncolor = src.get_colors().reshape(src.get_num_vertices(), 3)
        content_color = (255 * content_ncolor).astype(np.int16)

        # get content representations
        tf.compat.v1.reset_default_graph()
        sess = tf.compat.v1.Session()
        psnet = PSNet(config=config,
                        sess=sess,
                        train_dir="",
                        mode="test",
                        num_pts=content_ncolor.shape[0])
        psnet.restore_model(trained_model)
        ppt(list(psnet.node.keys()))

        obtained_content_fvs = sess.run(psnet.node, feed_dict={psnet.color: content_ncolor[None, ...],
                                                                psnet.geo: content_geo[None, ...],
                                                                psnet.bn_pl: False,
                                                                psnet.dropout_prob_pl: 1.0})
        sess.close()

        #if not (style_path.endswith("ply") or style_path.endswith("npy")):
        if ref.get_type() == "Image":
            style_ncolor = ref.get_colors().reshape(ref.get_pixelnum(), 3) * 2.0 - 1.0
            from_image = True            
        else:
            from_image = False
            style_geo = ref.get_vertex_positions().reshape(ref.get_num_vertices(), 3)
            style_ncolor = ref.get_colors().reshape(ref.get_num_vertices(), 3)

        # get style representations
        tf.compat.v1.reset_default_graph()
        sess = tf.compat.v1.Session()
        psnet = PSNet(config=config, sess=sess, train_dir="", mode="test", num_pts=style_ncolor.shape[0])
        psnet.restore_model(trained_model)
        if from_image:
            obtained_style_fvs = sess.run({i: psnet.node_color[i] for i in use_style_color},
                                            feed_dict={psnet.color: style_ncolor[None, ...],
                                                        psnet.bn_pl: False,
                                                        psnet.dropout_prob_pl: 1.0})
        else:
            obtained_style_fvs = sess.run(psnet.node,
                                            feed_dict={psnet.color: style_ncolor[None, ...],
                                                        psnet.geo: style_geo[None, ...],
                                                        psnet.bn_pl: False,
                                                        psnet.dropout_prob_pl: 1.0})
        obtained_style_fvs_gram = dict()
        for layer, fvs in obtained_style_fvs.items():
            gram = []
            for row in fvs:
                gram.append(np.matmul(row.T, row) / row.size)
            obtained_style_fvs_gram[layer] = np.array(gram)

        sess.close()

        # code for style transfer
        tf.compat.v1.reset_default_graph()
        with tf.Graph().as_default() as graph:
            sess = tf.compat.v1.Session()
            psnet = PSNet(config=config,
                          sess=sess,
                          train_dir="",
                          mode="styletransfer",
                          num_pts=content_geo.shape[0],  # should be the same as content
                          target_content=obtained_content_fvs,
                          target_style=obtained_style_fvs_gram,
                          geo_init=tf.compat.v1.constant_initializer(value=content_geo),
                          color_init=tf.compat.v1.constant_initializer(value=content_ncolor),
                          from_image=from_image)
            psnet.restore_model(trained_model)
            previous_loss = float("inf")
            for i in range(iteration):
                psnet.style_transfer_one_step()
                if from_image:
                    current_total_loss = sess.run(psnet.total_loss_color, feed_dict={
                        psnet.bn_pl: False,
                        psnet.dropout_prob_pl: 1.0})
                else:
                    current_total_loss = sess.run(psnet.total_loss_color, feed_dict={
                        psnet.bn_pl: False,
                        psnet.dropout_prob_pl: 1.0}) + sess.run(psnet.total_loss_geo, feed_dict={
                        psnet.bn_pl: False,
                        psnet.dropout_prob_pl: 1.0})
                # stop criteria for style transfer, and save results (ply and png)
                if abs(previous_loss - current_total_loss) < 1e-7 or i == iteration - 1:
                    transferred_color = (127.5 * (np.squeeze(np.clip(sess.run(psnet.color), -1, 1)) + 1)).astype(float) / 255.0
                    transferred_color = transferred_color.reshape((transferred_color.shape[0], 1, 3))
                    src.set_colors(transferred_color)

                    if not from_image and geotransfer:
                        transferred_geo = np.squeeze(sess.run(psnet.geo))
                        src.set_vertex_positions(transferred_geo.reshape(src.get_num_vertices(), 3))

                    break
                previous_loss = current_total_loss
            sess.close()

        output = {
            "status_code": 0,
            "response": "",
            "object": src
        }

        return output

ColorTransferLib/Algorithms/PDF/init.py

from . import PDF

ColorTransferLib/Algorithms/PSN/tf_ops.py

import tensorflow as tf
import tf_slim as slim

class BatchNorm(object):
    def __init__(self, epsilon=1e-5, momentum=0.99, name="batch_norm"):
        with tf.compat.v1.variable_scope(name):
            self.epsilon = epsilon
            self.momentum = momentum
            self.name = name

    def __call__(self, x, is_training):
        return slim.layers.batch_norm(x,
                                            decay=self.momentum,
                                            epsilon=self.epsilon,
                                            updates_collections=None,
                                            scale=True,
                                            is_training=is_training,
                                            scope=self.name)

def FE_layer(inputs, cout, aggregate_global=True, bn_is_training=True, scope="FE_layer"):
    """

    :param inputs: a tensor of shape (batch_size, num_pts, cin)
    :param cout: # out channels
    :return:  a tensor of shape (batch_size, num_pts, cout)
    """
    if aggregate_global:
        channel = cout // 2
    else:
        channel = cout
    cin = inputs.get_shape().as_list()[-1]
    with tf.compat.v1.variable_scope(scope) as local_scope:
        num_pts = inputs.get_shape().as_list()[1]

        #point_wise_feature = tf.keras.layers.Dense(units=channel, kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))(inputs)
        point_wise_feature = tf.compat.v1.layers.dense(inputs, channel,
                                                       kernel_initializer=tf.compat.v1.truncated_normal_initializer(
                                                           stddev=0.02))

        batch_norm = BatchNorm()
        point_wise_feature = batch_norm(point_wise_feature, is_training=bn_is_training)
        point_wise_feature = tf.nn.leaky_relu(point_wise_feature)  # (batch_size, num_pts, cout // 2)
        if aggregate_global:
            aggregated_feature = tf.reduce_max(input_tensor=point_wise_feature, axis=1, keepdims=True)  # batch_size, 1, cout//2
            repeated = tf.tile(aggregated_feature, [1, num_pts, 1])  # (batch_size, num_pts, cout // 2)
            point_wise_concatenated_feature = tf.concat(axis=-1, values=[point_wise_feature, repeated])
            return point_wise_feature, point_wise_concatenated_feature
        else:
            return point_wise_feature, point_wise_feature

def dense_norm_nonlinear(inputs, units,
                         norm_type=None,
                         is_training=None,
                         activation_fn=tf.nn.relu,
                         scope="fc"):
    """
    :param inputs: tensor of shape (batch_size, ...,n) from last layer
    :param units: output units
    :param norm_type: a string indicating which type of normalization is used.
                    A string start with "b": use batch norm.
                    A string starting with "l": use layer norm
                    others: do not use normalization
    :param is_training: a boolean placeholder of shape () indicating whether its in training phase or test phase.
    It is only needed when BN is used.
    :param activation_fn:
    :param scope: scope name
    :return: (batch_size, ...,units)
    """
    with tf.compat.v1.variable_scope(scope):
        # out = tf.keras.layers.Dense(units=units,
        #                                          kernel_initializer=tf.compat.v1.truncated_normal_initializer(
        #                                              stddev=0.02))(inputs)
        out = tf.compat.v1.layers.dense(inputs, units,
                              kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))
        #  batch_size, num_point, num_features
        if norm_type is not None:
            if norm_type.lower().startswith("b"):
                batch_norm = BatchNorm()
                if is_training is None:
                    raise ValueError("is_training is not given!")
                out = batch_norm(out, is_training=is_training)
            elif norm_type.lower().startswith("l"):
                out = tf.contrib.layers.layer_norm(out, scope="layer_norm")
            elif norm_type.lower().startswith("i"):
                out = tf.contrib.layers.instance_norm(out, scope="instance_norm")
            else:
                raise ValueError("please give the right norm type beginning with 'b' or 'l'!")
        if activation_fn is not None:
            out = activation_fn(out)
        return out

ColorTransferLib/Algorithms/PSN/LICENSE.txt

MIT License

Copyright (c) 2019 

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

ColorTransferLib/Algorithms/PSN/psnet.py

from ColorTransferLib.Algorithms.PSN.tf_ops import *
import os
import numpy as np
from pprint import pprint as ppt

class PSNet():
    def __init__(self,config, sess, train_dir,
                 mode = "train",
                 num_pts = 4096,
                 late_fusion = True,
                 **st_dict):
        # define model structure
        self.fe_layer_units = list(map(lambda x: int(x), config["modelstructure"]["fe_layer"].split(",")))
        self.fc_layer_units = list(map(lambda x: int(x), config["modelstructure"]["fc_layer"].split(",")))

        self.num_label = config["training"].getint("num_label")
        self.base_lr = config["training"].getfloat("lr")
        self.lr_decay_rate = config["training"].getfloat("lr_decay_rate")
        self.lr_decay_step = config["training"].getfloat("lr_decay_step")
        self.mode = mode
        self.summary_loss = dict()
        self.summary_grad = dict()
        self.summary_hist = dict()
        self.node = dict()
        self.node_color = dict()
        self.node_geo = dict()
        self.kernal = dict()
        # layers used to extract content representation and style representation
        self.content_layer = list(map(lambda x: int(x),config["style_transfer"]["content_layer"].split(",")))
        self.style_layer = list(map(lambda x: int(x),config["style_transfer"]["content_layer"].split(",")))

        self.batch_size = config["training"].getint("batch_size")
        self.num_points = num_pts

        # input placeholders
        if mode == "styletransfer":
            self.from_image = st_dict["from_image"]
            if config["style_transfer"]["color_init"].startswith("r"):
                self.color = tf.compat.v1.get_variable(name="point_cloud_color",shape=[1, self.num_points, 3],
                                            initializer=tf.compat.v1.truncated_normal_initializer(mean=0, stddev=0.5))
            elif config["style_transfer"]["color_init"].startswith("c"):
                self.color = tf.compat.v1.get_variable(name="point_cloud_color", shape=[1, self.num_points, 3],
                                           initializer=st_dict["color_init"])
            self.summary_hist["color"] = tf.compat.v1.summary.histogram(name="color", values=self.color)
            if config["style_transfer"]["geo_init"].startswith("r") or self.from_image:
                self.geo = tf.compat.v1.get_variable(name="point_cloud",shape=[1, self.num_points, 3],
                                      initializer=tf.compat.v1.truncated_normal_initializer(mean=0, stddev=0.5))
            elif config["style_transfer"]["geo_init"].startswith("c"):
                self.geo = tf.compat.v1.get_variable(name="point_cloud", shape=[1, self.num_points, 3],
                                           initializer=st_dict["geo_init"])
            self.summary_hist["pts"] = tf.compat.v1.summary.histogram(name="pts", values=self.geo)
            self.beta_geo = config["style_transfer"].getint("beta_geo")
            self.pointclouds_pl = tf.concat([self.geo, self.color], axis=-1)

            self.target_content_representation = st_dict["target_content"]
            self.target_style_representation = st_dict["target_style"]
            self.st_optimizer_type = config["style_transfer"]["optimizer_type"].split("_")[0]
            self.st_lr = float(config["style_transfer"]["optimizer_type"].split("_")[1])
            self.beta_color = config["style_transfer"].getint("beta_color")

        elif mode == "train" or "test":
            self.geo = tf.compat.v1.placeholder(tf.float32,
                                         shape=[None, num_pts, 3], name="pc_geo")
            self.color = tf.compat.v1.placeholder(tf.float32,
                                           shape=[None, num_pts, 3], name="pc_color")
            #self.pointclouds_pl = tf.placeholder(tf.float32,
             #   shape=[None, self.num_points, 6], name="point_clouds")
            self.labels_pl = tf.compat.v1.placeholder(tf.int32, shape=(None,), name = "label")
        self.bn_pl = tf.compat.v1.placeholder(dtype=tf.bool, shape=(), name="is_bn_training")
        self.dropout_prob_pl = tf.compat.v1.placeholder(dtype=tf.float32, shape=(), name="dropout_prob")
        self.late_fusion = late_fusion

        self.sess = sess
        # construct learning rate strategy
        self.batch_step = tf.Variable(0, name="batch_step")
        self._get_learning_rate()
        # build model

        self._build_model()
        # summary writer and moder save
        if mode == "train":
            self.summary_writer = tf.compat.v1.summary.FileWriter(train_dir, sess.graph) # already saved graph
        elif self.mode.startswith("s"):
            #self.summary_writer = tf.compat.v1.summary.FileWriter(train_dir, sess.graph)
            self.model_saver = tf.compat.v1.train.Saver(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope="cls"))
        if self.mode.startswith("t"):
            self.model_saver = tf.compat.v1.train.Saver(max_to_keep=200)
        # logfile
        #self.log_fout = open(os.path.join(train_dir, 'log.txt'), 'a')
        self.sess.run(tf.compat.v1.global_variables_initializer())

    def _fe_layers(self, input_features, scope):
        with tf.compat.v1.variable_scope(scope):
            net = input_features
            for idx, unit in enumerate(self.fe_layer_units):
                after_relu, net = FE_layer(net, unit,aggregate_global=True, bn_is_training=self.bn_pl,scope="FE_{}".format(idx + 1))
                self.node[scope + "_FE_" + str(idx + 1)] = after_relu
                if "COLOR" in scope:
                   self.node_color[scope + "_FE_" + str(idx + 1)] = after_relu
                if "PTS" in scope:
                    self.node_geo[scope + "_FE_" + str(idx + 1)] = after_relu
                self.kernal[scope + "_FE_" + str(idx + 1)] =  tf.compat.v1.get_default_graph().get_tensor_by_name("cls/" + scope + "/FE_{}/dense/kernel:0".format(idx + 1))
                # self.summary[scope + "_FE_" + str(idx + 1)] = tf.summary.histogram(scope + "_FE_" + str(idx + 1), self.kernal[scope + "_FE_" + str(idx + 1)])
            global_aggregated_feature = tf.reduce_mean(input_tensor=net, axis=1, name="aggregation")  # batch_size, 4096
        return global_aggregated_feature

    def _fc_layers(self, input_vectors, scope):
        with tf.compat.v1.variable_scope(scope):
            net = input_vectors
            for idx, unit in enumerate(self.fc_layer_units):
                net = dense_norm_nonlinear(net, unit, norm_type="bn",is_training=self.bn_pl, scope="FC_{}".format(idx + 1))
                self.node[scope + "_FC_" + str(idx + 1)] = net
                net = tf.nn.dropout(net, rate=1 - (self.dropout_prob_pl), name="dropout_{}".format(idx + 1))
            logits = tf.compat.v1.layers.dense(net, self.num_label)
        return logits

    def _build_model(self):
        # graph
        with tf.compat.v1.variable_scope("cls"):
            if self.late_fusion:
                # if self.mode.startswith("t"):
                #     self.geo = self.pointclouds_pl[...,:3]
                #     self.color = self.pointclouds_pl[...,3:6]
                self.color_aggregated = self._fe_layers(self.color, "FE_COLOR")  # batchsize, num_units
                self.pts_aggregated = self._fe_layers(self.geo, "FE_PTS")  # batchsize, num_units
                self.pts_color_aggregated = tf.concat([self.pts_aggregated, self.color_aggregated], -1)
            else:
                self.pts_color_aggregated = self._fe_layers(self.pointclouds_pl, "FE_PTS_COLOR")
            self.logits = self._fc_layers(self.pts_color_aggregated, scope="FC")

        # loss
        if self.mode.startswith("t"):
            print("Building cross entropy loss...")
            self.loss = tf.reduce_mean(
                input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels_pl), name="cross_entropy")
            loss_sum = tf.compat.v1.summary.scalar(name="training_loss", tensor=self.loss)
            update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
            with tf.control_dependencies(update_ops):
                self.optim_op = tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr) \
                    .minimize(self.loss, global_step=self.batch_step)
        elif self.mode.startswith("s"):
            print("Optimizer: {0} Learning Rate: {1}".format(self.st_optimizer_type, self.st_lr))
            # style transfer loss
            if self.st_optimizer_type == "sgd":
                self.optimizer = tf.compat.v1.train.GradientDescentOptimizer(self.st_lr)
            elif self.st_optimizer_type == "adadelta":
                self.optimizer = tf.compat.v1.train.AdadeltaOptimizer(self.st_lr, rho=0.9)
            elif self.st_optimizer_type == "adagrad":
                self.optimizer = tf.compat.v1.train.AdagradOptimizer(self.st_lr)
            elif self.st_optimizer_type == "adam":
                self.optimizer = tf.compat.v1.train.AdamOptimizer(self.st_lr)
            elif self.st_optimizer_type == "momentum":
                self.optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=self.st_lr, momentum=0.9, use_nesterov= False)
            elif self.st_optimizer_type == "rmsprop":
                self.optimizer = tf.compat.v1.train.RMSPropOptimizer(self.st_lr)
            elif self.st_optimizer_type == "nesterov":
                self.optimizer = tf.compat.v1.train.MomentumOptimizer(learning_rate=self.st_lr, momentum=0.9, use_nesterov= True)
            else:
                raise ValueError("Please choose the correct optimizer for style transfer!")

            print("Building style transfer loss...")
            if self.late_fusion:
                if not self.from_image:
                    use_content_geo = ["FE_PTS_FE_{}".format(i) for i in self.content_layer]
                    use_style_geo = ["FE_PTS_FE_{}".format(i) for i in self.style_layer]
                    # content loss for geo
                    loss_geo_content = []
                    for layer in use_content_geo:
                        loss_geo_content.append(
                            tf.nn.l2_loss(self.target_content_representation[layer] - self.node[layer]) / tf.size(
                                input=self.node[layer], out_type=tf.float32))
                    self.loss_geo_content = tf.add_n(loss_geo_content)

                    # style loss for geo
                    loss_geo_gram = []
                    for layer in use_style_geo:
                        source_gram_pts = tf.matmul(tf.transpose(a=tf.squeeze(self.node[layer])),
                                                    tf.squeeze(self.node[layer])) / tf.size(input=self.node[layer],
                                                                                            out_type=tf.float32)
                        loss_geo_gram.append(tf.nn.l2_loss(self.target_style_representation[layer] - source_gram_pts))
                    self.loss_geo_style = tf.add_n(loss_geo_gram)
                    self.total_loss_geo = self.loss_geo_content + self.beta_geo * self.loss_geo_style
                    self.summary_loss["loss_geo"] = tf.compat.v1.summary.scalar("loss_geo", self.total_loss_geo)
                    self.summary_loss["loss_geo_content"] = tf.compat.v1.summary.scalar("loss_geo_content", self.loss_geo_content)
                    self.summary_loss["loss_geo_style"] = tf.compat.v1.summary.scalar("loss_geo_style", self.loss_geo_style)

                use_content_color = ["FE_COLOR_FE_{}".format(i) for i in self.content_layer]
                use_style_color = ["FE_COLOR_FE_{}".format(i) for i in self.style_layer]
                # content loss for color
                loss_color_content = []
                for layer in use_content_color:
                    loss_color_content.append(
                        tf.nn.l2_loss(self.target_content_representation[layer] - self.node[layer]) / tf.size(input=self.node[layer],out_type=tf.float32))
                self.loss_color_content  = tf.add_n(loss_color_content)

                # style loss for color
                loss_color_gram = []
                for layer in use_style_color:
                    source_gram_color = tf.matmul(tf.transpose(a=tf.squeeze(self.node[layer])),
                                                  tf.squeeze(self.node[layer])) / tf.size(input=self.node[layer],
                                                                                          out_type=tf.float32)
                    loss_color_gram.append(tf.nn.l2_loss(self.target_style_representation[layer] - source_gram_color))
                self.loss_color_style = tf.add_n(loss_color_gram)

                # total loss
                self.total_loss_color = self.loss_color_content + self.beta_color * self.loss_color_style
                self.summary_loss["loss_color"] = tf.compat.v1.summary.scalar("loss_color", self.total_loss_color)
                self.summary_loss["loss_color_content"] = tf.compat.v1.summary.scalar("loss_color_content", self.loss_color_content)
                self.summary_loss["loss_color_style"] = tf.compat.v1.summary.scalar("loss_color_style", self.loss_color_style)

                update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
                with tf.control_dependencies(update_ops):
                    self.optim_op_color = self.optimizer \
                        .minimize(self.total_loss_color, global_step=self.batch_step, var_list=[self.color])
                    if not self.from_image:
                        self.optim_op_pts = self.optimizer \
                            .minimize(self.total_loss_geo, global_step=self.batch_step, var_list=[self.geo])
            else: # early fusion
                use_content = ["FE_PTS_COLOR_FE_{}".format(i) for i in self.content_layer]
                use_style = ["FE_PTS_COLOR_FE_{}".format(i) for i in self.style_layer]
                print("Building style transfer loss...")
                # content loss
                loss_content = []
                for layer in use_content:
                    loss_content.append(
                        tf.nn.l2_loss(self.target_content_representation[layer] - self.node[layer]) / tf.size(
                            input=self.node[layer], out_type=tf.float32))
                self.loss_content = tf.add_n(loss_content)

                # style loss
                loss_style_gram = []
                for layer in use_style:
                    source_gram = tf.matmul(tf.transpose(a=tf.squeeze(self.node[layer])),
                                                  tf.squeeze(self.node[layer])) / tf.size(input=self.node[layer],
                                                                                          out_type=tf.float32)
                    loss_style_gram.append(tf.nn.l2_loss(self.target_style_representation[layer] - source_gram))
                self.loss_style = tf.add_n(loss_style_gram)
                # total loss
                self.total_loss = self.loss_content + self.beta_geo * self.loss_style
                update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
                with tf.control_dependencies(update_ops):
                    self.optim_op_color = self.optimizer \
                        .minimize(self.total_loss, global_step=self.batch_step, var_list=[self.color])
                    self.optim_op_pts = self.optimizer \
                        .minimize(self.total_loss, global_step=self.batch_step, var_list=[self.geo])
                    self.optim_op_total = self.optimizer \
                        .minimize(self.total_loss, global_step=self.batch_step, var_list=[self.geo, self.color])

        else:
            raise ValueError("please choose the right mode!current mode is {}".format(self.mode))

        # summary
        # self.summary["grad_weight_FE1_pts"] = tf.summary.histogram("grad_weight_FE1_pts", tf.get_default_graph().\
        #     get_tensor_by_name("gradients/FE_PTS/FE_1/dense/Tensordot/MatMul_grad/MatMul_1:0"))
        # self.summary["grad_weight_FE1_color"] = tf.summary.histogram("grad_weight_FE1_color", tf.get_default_graph(). \
        #     get_tensor_by_name("gradients/FE_COLOR/FE_1/dense/Tensordot/MatMul_grad/MatMul_1:0"))

        # grad_color = tf.get_default_graph().get_tensor_by_name(
        #     "gradients/cls/FE_COLOR/FE_1/dense/Tensordot/MatMul_grad/MatMul_1:0")
        # self.summary["grad_color"] = tf.summary.histogram("grad_color", grad_color)
        # grad_pts = tf.get_default_graph().get_tensor_by_name(
        #     "gradients/cls/FE_PTS/FE_1/dense/Tensordot/MatMul_grad/MatMul_1:0")
        # self.summary["grad_pts"] = tf.summary.histogram("grad_pts", grad_pts)
        self.all_summary = tf.compat.v1.summary.merge_all()
        return True

    def style_transfer_one_step(self, update_property = "geometry"):
        if self.late_fusion:
            train_dict_color = self.sess.run({"loss_color": self.total_loss_color,
                                              "loss_color_content": self.loss_color_content,
                                              "loss_color_style": self.loss_color_style,
                                              "optim_op": self.optim_op_color,
                                              "batch_step": self.batch_step},
                                             feed_dict={
                                                 self.bn_pl: False,
                                                 self.dropout_prob_pl: 1.0})
            self.log_string("step: {0:5d} st_color_loss: {1:.8f} color_loss_content: {2:.8f} color_loss_style: {3:.8f}".
                            format(train_dict_color["batch_step"], train_dict_color["loss_color"],
                                   train_dict_color["loss_color_content"], train_dict_color["loss_color_style"]))
            if not self.from_image:
                train_dict_pts = self.sess.run({"loss_geo": self.total_loss_geo,
                                                "loss_geo_content":self.loss_geo_content,
                                                "loss_geo_style": self.loss_geo_style,
                                               "optim_op": self.optim_op_pts,
                                               "batch_step": self.batch_step},
                                              feed_dict={
                                                  self.bn_pl: False,
                                                  self.dropout_prob_pl:1.0})
                self.log_string("step: {0:5d} st_pts_loss: {1:.8f} pts_loss_content: {2:.8f} pts_loss_style: {3:.8f}".
                                format(train_dict_pts["batch_step"], train_dict_pts["loss_geo"],
                                       train_dict_pts["loss_geo_content"], train_dict_pts["loss_geo_style"]))
            # geo only
            # self.log_string("step: {0:5d} st_color_loss: {1:.3f} "
            #                 "\n pts_loss_content: {2:.3f} pts_loss_style:{3:.3f}".
            #                 format(train_dict_pts["batch_step"], train_dict_pts["loss_pts"],
            #                        train_dict_pts["loss_pts_content"], train_dict_pts["loss_pts_style"]))
            # # color only
            # self.log_string("step: {0:5d} st_color_loss: {1:.3f} "
            #                 "\n color_loss_content: {2:.3f} color_loss_style:{3:.3f}".
            #                 format(train_dict_color["batch_step"], train_dict_color["loss_color"],
            #                        train_dict_color["loss_color_content"], train_dict_color["loss_color_style"]))
        else: # early fusion
            if update_property.startswith("g"):
                op = self.optim_op_pts
            elif update_property.startswith("c"):
                op = self.optim_op_color
            elif update_property.startswith("t"):
                op = self.optim_op_total
            train_dict_pts = self.sess.run({"loss_total": self.total_loss,
                                            "loss_content": self.loss_content,
                                            "loss_style": self.loss_style,
                                            "optim_op": op,
                                            "batch_step": self.batch_step},
                                           feed_dict={
                                               self.bn_pl: False,
                                               self.dropout_prob_pl: 1.0})
            self.log_string("step: {0:5d} loss_total: {1:.5f} loss_content: {2:.5f} loss_style: {3:.5f}".format(
                                   train_dict_pts["batch_step"],
                                   train_dict_pts["loss_total"],
                                   train_dict_pts["loss_content"],
                                   train_dict_pts["loss_style"]))

    def _get_learning_rate(self):
        learning_rate = tf.compat.v1.train.exponential_decay(
            self.base_lr,  # Base learning rate.
            self.batch_step * self.batch_size,  # Current index into the dataset.
            self.lr_decay_step,  # Decay step.
            self.lr_decay_rate,  # Decay rate.
            staircase=True)
        self.lr = tf.maximum(learning_rate, 0.00001)  # CLIP THE LEARNING RATE!
        self.sum_lr = tf.compat.v1.summary.scalar(name="learning_rate", tensor=self.lr)
        return True

    def save_model(self, path, i):
        """
        save model parameters
        :param path:
        :return:
        """
        self.model_saver.save(self.sess, path, i)
        return True

    def restore_model(self, model_path):
        """
        restore model graph and parameters from path
        :param model_path:
        :return:
        """
        self.model_saver.restore(self.sess, model_path)
        return True

    def train_one_batch(self, batch_data, batch_label):
        all_sum, _, loss, batch_step = self.sess.run([self.all_summary, self.optim_op, self.loss, self.batch_step],
                              feed_dict={
                                  self.pointclouds_pl: batch_data,
                                  self.labels_pl: batch_label,
                                  self.bn_pl: True,
                                  self.dropout_prob_pl:0.7})
        self.summary_writer.add_summary(all_sum, batch_step)
        self.log_string(
            "global_step:{0:5d} train_loss: {1}".format(batch_step, loss))
        return True

    def eval_one_batch(self, batch_data, batch_label):
        eval_dict = self.sess.run({"eval_logtis": self.logits}, feed_dict={
                                  self.pointclouds_pl: batch_data,
                                  self.labels_pl: batch_label,
                                  self.bn_pl: False,
                                  self.dropout_prob_pl:1.0})
        return eval_dict

    # def eval_one_epoch(self,):

    def log_string(self, out_str):
        #self.log_fout.write(out_str + '\n')
        #self.log_fout.flush()
        print(out_str)

ColorTransferLib/Algorithms/PSN/init.py

from . import PSN

ColorTransferLib/Algorithms/PSN/utils.py

import os

import h5py
import numpy as np
import pandas as pd
import tensorflow as tf
#import tensorflow.contrib.slim as slim
import tf_slim as slim
from PIL import Image
from pyntcloud import PyntCloud
from sklearn.metrics import auc, roc_curve

import matplotlib
try:
    matplotlib.use('TkAgg')#uncomment this line if you are using macos
except:
    pass
import matplotlib.pyplot as plt

def show_all_trainable_variables():
    model_vars = tf.compat.v1.trainable_variables()
    slim.model_analyzer.analyze_vars(model_vars, print_info=True)

def display_point(pts, color, color_label=None, title=None, fname=None, axis="on", marker_size=5):
    pts = np.squeeze(pts)
    if isinstance(color, np.ndarray):
        color = np_color_to_hex_str(np.squeeze(color))
    DPI = 300
    PIX_h = 1000
    MARKER_SIZE = marker_size
    if color_label is None:
        PIX_w = PIX_h
    else:
        PIX_w = PIX_h * 2
    X = pts[:, 0]
    Y = pts[:, 2]
    Z = pts[:, 1]
    max_range = np.array([X.max() - X.min(), Y.max() - Y.min(), Z.max() - Z.min()]).max() / 2.0
    mid_x = (X.max() + X.min()) * 0.5
    mid_y = (Y.max() + Y.min()) * 0.5
    mid_z = (Z.max() + Z.min()) * 0.5
    fig = plt.figure()
    fig.set_size_inches(PIX_w / DPI, PIX_h / DPI)
    if axis == "off":
        plt.subplots_adjust(top=1.2, bottom=-0.2, right=1.5, left=-0.5, hspace=0, wspace=-0.7)
    plt.margins(0, 0)
    if color_label is None:
        ax = fig.add_subplot(111, projection='3d')
        ax.scatter(X, - Y, Z, c=color, edgecolors="none", s=MARKER_SIZE, depthshade=True)
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        plt.axis(axis)
        # ax.set_yticklabels([])
        # ax.set_xticklabels([])
        # ax.set_zticklabels([])
        # ax.set_axis_off()
        if title is not None:
            ax.set_title(title, fontdict={'fontsize': 30})

        ax.set_aspect("auto")
        # ax.grid("off")
        if fname:
            plt.savefig(fname, transparent=True, dpi=DPI)
            plt.close(fig)
        else:
            plt.show()
    else:
        ax = fig.add_subplot(121, projection='3d')
        bx = fig.add_subplot(122, projection='3d')
        ax.scatter(X, Y, Z, c=color, edgecolors="none", s=MARKER_SIZE, depthshade=True)
        bx.scatter(X, Y, Z, c=color_label, edgecolors="none", s=MARKER_SIZE, depthshade=True)
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        bx.set_xlim(mid_x - max_range, mid_x + max_range)
        bx.set_ylim(mid_y - max_range, mid_y + max_range)
        bx.set_zlim(mid_z - max_range, mid_z + max_range)
        bx.patch.set_alpha(0)
        ax.set_aspect("equal")
        ax.grid("off")
        bx.set_aspect("equal")
        bx.grid("off")
        ax.axis('off')
        bx.axis("off")
        plt.axis('off')
        if fname:
            plt.savefig(fname, transparent=True, dpi=DPI)
            plt.close(fig)

        else:
            plt.show()

def int16_to_hex_str(color):
    hex_str = ""
    color_map = {i: str(i) for i in range(10)}
    color_map.update({10: "A", 11: "B", 12: "C", 13: "D", 14: "E", 15: "F", 16: "F"})
    # print(color_map)
    hex_str += color_map[color // 16]
    hex_str += color_map[color % 16]
    return hex_str

def horizontal_concatnate_pic(fout, *fnames):
    images = [Image.open(i).convert('RGB') for i in fnames]
    # images = map(Image.open, fnames)
    widths, heights = zip(*(i.size for i in images))

    total_width = sum(widths)
    max_height = max(heights)

    new_im = Image.new('RGB', (total_width, max_height))

    x_offset = 0
    for im in images:
        new_im.paste(im, (x_offset, 0))
        x_offset += im.size[0]

    new_im.save(fout)

def vertical_concatnate_pic(fout, *fnames):
    images = [Image.open(i).convert('RGB') for i in fnames]
    # images = map(Image.open, fnames)
    widths, heights = zip(*(i.size for i in images))
    max_widths = max(widths)
    width_ratio = [max_widths / width for width in widths]
    new_height = [int(width_ratio[idx]) * height for idx, height in enumerate(heights)]

    new_images = [i.resize((max_widths, new_height[idx])) for idx, i in enumerate(images)]
    total_heights = sum(new_height)

    new_im = Image.new('RGB', (max_widths, total_heights))

    x_offset = 0
    for im in new_images:
        new_im.paste(im, (0, x_offset))
        x_offset += im.size[1]

    new_im.save(fout)

def rgb_to_hex_str(*rgb):
    hex_str = "#"
    for item in rgb:
        hex_str += int16_to_hex_str(item)
    return hex_str

def np_color_to_hex_str(color):
    """
    :param color: an numpy array of shape (N, 3)
    :return: a list of hex color strings
    """
    hex_list = []
    for rgb in color:
        hex_list.append(rgb_to_hex_str(rgb[0], rgb[1], rgb[2]))
    return hex_list

def load_h5(path, *kwd):
    f = h5py.File(path)
    list_ = []
    for item in kwd:
        list_.append(f[item][:])
        print("{0} of shape {1} loaded!".format(item, f[item][:].shape))
        if item == "ndata" or item == "data":
            pass  # print(np.mean(f[item][:], axis=1))
        if item == "color":
            print("color is of type {}".format(f[item][:].dtype))
    return list_

def load_single_cat_h5(cat, num_pts, type, *kwd):
    fpath = os.path.join("./data/category_h5py", cat, "PTS_{}".format(num_pts), "ply_data_{}.h5".format(type))
    return load_h5(fpath, *kwd)

def printout(flog, data):  # follow up
    print(data)
    flog.write(data + '\n')

def save_ply(data, color, fname):
    color = color.astype(np.uint8)
    df1 = pd.DataFrame(data, columns=["x", "y", "z"])
    df2 = pd.DataFrame(color, columns=["red", "green", "blue"])
    pc = PyntCloud(pd.concat([df1, df2], axis=1))
    pc.to_file(fname)

def label2onehot(labels, m):
    idx = np.eye(m)
    onehot_labels = np.zeros(shape=(labels.shape[0], m))
    for idx, i in enumerate(labels):
        onehot_labels[idx] = idx[i]
    return onehot_labels

def multiclass_AUC(y_true, y_pred):
    """

    :param y_true: shape (num_instance,)
    :param y_pred: shape (num_instance, num_class)
    :return:
    """
    num_classes = np.unique(y_true).shape[0]
    print(num_classes)
    tpr = dict()
    fpr = dict()
    roc_auc = dict()
    num_instance = dict()
    total_roc_auc = 0
    for i in range(num_classes):
        binary_label = np.where(y_true == i, 1, 0)
        class_score = y_pred[:, i]
        num_instance[i] = np.sum(y_true == i)
        fpr[i], tpr[i], _ = roc_curve(binary_label, class_score)
        roc_auc[i] = auc(fpr[i], tpr[i])
        total_roc_auc += roc_auc[i]
    return total_roc_auc / 16
    # print(roc_auc, num_instance)

def softmax(logits):
    """

    :param logits: of shape (num_instance, num_classes)
    :return: prob of the same shape as that of logits, with each row summed up to 1
    """
    assert logits.shape[-1] == 16
    regulazation = np.max(logits, axis=-1)  # (num_instance,)
    logits -= regulazation[:, np.newaxis]
    prob = np.exp(logits) / np.sum(np.exp(logits), axis=-1)[:, np.newaxis]
    assert prob.shape == logits.shape
    return prob

def construct_label_weight(label, weight):
    """

    :param label: a numpy ndarray of shape (batch_size,) with each entry in [0, num_class)
    :param weight: weight list of length num_class
    :return: a numpy ndarray of the same shape as that of label
    """
    return [weight[i] for i in label]

def jitter_point_cloud(batch_data, sigma=0.01, clip=0.05):
    """ Randomly jitter points. jittering is per point.
        Input:
          BxNx3 array, original batch of point clouds
        Return:
          BxNx3 array, jittered batch of point clouds
    """
    B, N, C = batch_data.shape
    assert (clip > 0)
    jittered_data = np.clip(sigma * np.random.randn(B, N, C), -1 * clip, clip)
    jittered_data += batch_data
    return jittered_data

def generate_sphere(num_pts, radius):
    # http://electron9.phys.utk.edu/vectors/3dcoordinates.htm
    r = np.random.rand(num_pts, 1) * radius
    f = np.random.rand(num_pts, 1) * np.pi * 2
    q = np.random.rand(num_pts, 1) * np.pi
    x = r * np.sin(q) * np.cos(f)
    y = r * np.sin(q) * np.sin(f)
    z = r * np.cos(q)
    return np.concatenate((x, y, z), axis=-1)

def generate_sphere_surface(num_pts, radius):
    r = radius
    f = np.random.rand(num_pts, 1) * np.pi * 2
    q = np.random.rand(num_pts, 1) * np.pi
    x = r * np.sin(q) * np.cos(f)
    y = r * np.sin(q) * np.sin(f)
    z = r * np.cos(q)
    return np.concatenate((x, y, z), axis=-1)

def generate_cube(num_pts, length):
    return (np.random.rand(num_pts, 3) - 0.5) * length

def generate_cube_surface(num_pts, length):
    pass

def generate_ncolor(num_pts):
    return ((np.ones((num_pts, 3)) * 127) - 127.5) / 127.5

def generate_plane(num_pts, length, type="xy"):
    pts = (np.random.rand(num_pts, 2) - 0.5) * length
    if type == "xy":
        pts = np.insert(pts, 2, 0, axis=1)
    elif type == "xz":
        pts = np.insert(pts, 1, 0, axis=1)
    elif type == "yz":
        pts = np.insert(pts, 0, 0, axis=1)
    return pts

def img_to_set(img_path, img_name):
    img = Image.open(os.path.join(img_path, img_name)).convert('RGB')
    w, h = img.size
    # img = img.resize((int(w/10), int(h/10)), Image.LANCZOS)
    # img.save(os.path.join(img_path, img_name.split(".")[0] + "_resized.jpg"), "jpeg")
    print(img.size)
    return np.reshape(np.array(img), [-1, 3])

def prepare_content_or_style(path, downsample_points=None):
    if path.endswith("ply"):
        content = PyntCloud.from_file(path).points.values
        if downsample_points:
            mask = np.random.choice(content.shape[0], downsample_points)
            content = content[mask]
        content_ndata = content[:, :3]
        content_ncolor = (content[:, 3:6] - 127.5) / 127.5
        return content_ndata, content_ncolor
    elif path.endswith("npy"):
        content = np.load(path)
        if downsample_points:
            mask = np.random.choice(content.shape[0], downsample_points)
            content = content[mask]
        content_ndata = content[:, :3]
        content_ncolor = (content[:, 3:6] - 127.5) / 127.5
        return content_ndata, content_ncolor
    else:
        img = Image.open(path).convert("RGB")
        style_color = np.reshape(np.array(img), [-1, 3])
        style_color = (style_color - 127.5) / 127.5
        if downsample_points:
            mask = np.random.choice(style_color.shape[0], downsample_points)
            style_color = style_color[mask]
        return style_color

ColorTransferLib/Algorithms/RHG/ReHistoGAN/init.py

#from ReHistoGAN.rehistoGAN import recoloringTrainer

ColorTransferLib/Algorithms/RHG/LICENSE.md
MIT License

Copyright (c) 2021 Mahmoud Afifi

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
ColorTransferLib/Algorithms/RHG/RHG.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import cv2
import torch
import os
import time
from copy import deepcopy

from .utils.face_preprocessing import face_extraction
from .rehistoGAN import train_from_folder
from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms
#   Author: Mahmoud Afifi, Marcus A. Brubaker, Michael S. Brown
#   Published in: CVPR
#   Year of Publication: 2021
#
# Abstract:
#   In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images' colors. 
#   We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled 
#   from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN 
#   architecture to control the colors of GAN-generated images specified by a target color histogram feature. We then 
#   describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network 
#   along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network 
#   to keep the original image's content while changing the colors based on the given target histogram. We show that 
#   this histogram-based approach offers a better way to control GAN-generated and real images' colors while producing 
#   more compelling results compared to existing alternative strategies.
#
# Info:
#   Name: ReHistoGAN
#   Identifier: RHG
#   Link: https://doi.org/10.1109/CVPR46437.2021.00785
#   Sources: https://github.com/mahmoudnafifi/HistoGAN
#
# Implementation Details:
#   model: Universal rehistoGAN v0, internal down and upsampling
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class RHG:
    identifier = "RHG"
    title = "HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms"
    year = 2021
    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "RHG",
            "title": "HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms",
            "year": 2021,
            "abstract": "In this paper, we present HistoGAN, a color histogram-based method for controlling "
                        "GAN-generated images colors. We focus on color histograms as they provide an intuitive way "
                        "to describe image color while remaining decoupled from domain-specific semantics. "
                        "Specifically, we introduce an effective modification of the recent StyleGAN architecture to "
                        "control the colors of GAN-generated images specified by a target color histogram feature. We "
                        "then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly "
                        "train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an "
                        "unsupervised approach trained to encourage the network to keep the original images content "
                        "while changing the colors based on the given target histogram. We show that this "
                        "histogram-based approach offers a better way to control GAN-generated and real images colors "
                        "while producing more compelling results compared to existing alternative strategies.",
            "types": ["Image"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, RHG.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # START PROCESSING
        img_src = src.get_raw()
        img_ref = ref.get_raw()

        src_orig_wh = img_src.shape[:2]

        out_img = deepcopy(src)

        if torch.cuda.is_available():
            torch.cuda.set_device(opt.gpu)

        if opt.generate and opt.face_extraction:
            if opt.input_image is None:
                raise Exception('No input image is given')
            extension = os.path.splitext(opt.input_image)[1]
            if (extension == str.lower(extension) == '.jpg' or str.lower(extension) == '.png'):
                face_extraction(opt.input_image)
                input_image = f'./temp-faces/{os.path.split(opt.input_image)[-1]}'
            else:
                raise Exception('File extension is not supported!')
        else:
            input_image = opt.input_image

        input_image = img_src
        opt.target_hist = img_ref

        out_temp = train_from_folder(
            results_dir=opt.results_dir,
            models_dir=opt.models_dir,
            name=opt.name,
            new=opt.new,
            load_from=opt.load_from,
            load_histogan_weights=opt.load_histoGAN_weights,
            image_size=opt.image_size,
            network_capacity=opt.network_capacity,
            transparent=opt.transparent,
            batch_size=opt.batch_size,
            gradient_accumulate_every=opt.gradient_accumulate_every,
            num_train_steps=opt.num_train_steps,
            learning_rate=opt.learning_rate,
            num_workers=opt.num_workers,
            save_every=opt.save_every,
            generate=opt.generate,
            trunc_psi=opt.trunc_psi,
            fp16=opt.fp16,
            fq_layers=opt.fq_layers,
            fq_dict_size=opt.fq_dict_size,
            attn_layers=opt.attn_layers,
            hist_method=opt.hist_method,
            hist_resizing=opt.hist_resizing,
            hist_sigma=opt.hist_sigma,
            hist_bin=opt.hist_bin,
            hist_insz=opt.hist_insz,
            target_hist=opt.target_hist,
            alpha=opt.alpha,
            beta=opt.beta,
            gamma=opt.gamma,
            skip_conn_to_GAN=opt.skip_conn_to_GAN,
            fixed_gan_weights=opt.fixed_gan_weights,
            sampling=opt.sampling,
            rec_loss=opt.rec_loss,
            initialize_gan=opt.initialize_gan,
            variance_loss=opt.variance_loss,
            input_image=input_image,
            internal_hist=opt.internal_hist,
            histoGAN_model_name=opt.histoGAN_model_name,
            target_number=opt.target_number,
            change_hyperparameters=opt.change_hyperparameters,
            change_hyperparameters_after=opt.change_hyperparameters_after,
            upsampling_output=opt.upsampling_output,
            upsampling_method=opt.upsampling_method,
            swapping_levels=opt.swapping_levels,
            pyramid_levels=opt.pyramid_levels,
            level_blending=opt.level_blending,
            post_recoloring=opt.post_recoloring
        )

        out_temp = out_temp.squeeze().cpu().detach().numpy().astype("float32")
        out_temp = np.swapaxes(out_temp,0,1)
        out_temp = np.swapaxes(out_temp,1,2)

        # resize output to src size
        out_temp = cv2.resize(out_temp, src_orig_wh, interpolation = cv2.INTER_AREA)

        out_img.set_colors(out_temp)

        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/PDF/PDF.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import math
import time
from copy import deepcopy
import numpy as np

from ColorTransferLib.Utils.Helper import check_compatibility

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: N-dimensional probability density function transfer and its application to color transfer
#   Author: Francois Pitie, Anil C. Kokaram, Rozenn Dahyot
#   Published in: Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1
#   Year of Publication: 2005
#
# Abstract:
#   This article proposes an original method to estimate a continuous transformation that maps one N-dimensional
#   distribution to another. The method is iterative, non-linear, and is shown to converge. Only 1D marginal
#   distribution is used in the estimation process, hence involving low computation costs. As an illustration this
#   mapping is applied to color transfer between two images of different contents. The paper also serves as a central
#   focal point for collecting together the research activity in this area and relating it to the important problem of
#   automated color grading.
#
# Info:
#   Name: PdfColorTransfer
#   Identifier: PDF
#   Link: https://doi.org/10.1109/ICCV.2005.166
#
# Implementation Details:
#   m = 1
#   iterations = 20
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class PDF:
    compatibility = {
        "src": ["Image", "Mesh", "PointCloud"],
        "ref": ["Image", "Mesh", "PointCloud"]
    }
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # HOST METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "PDF",
            "title": "N-dimensional probability density function transfer and its application to color transfer",
            "year": 2005,
            "abstract": "This article proposes an original method to estimate a continuous transformation that maps "
                        "one N-dimensional distribution to another. The method is iterative, non-linear, and is shown "
                        "to converge. Only 1D marginal distribution is used in the estimation process, hence involving "
                        "low computation costs. As an illustration this mapping is applied to color transfer between "
                        "two images of different contents. The paper also serves as a central focal point for "
                        "collecting together the research activity in this area and relating it to the important "
                        "problem of automated color grading.",
            "types": ["Image", "Mesh", "PointCloud"]
        }

        return info

    # ------------------------------------------------------------------------------------------------------------------
    # Generate a random 3x3 rotation matrix
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def random_rotation_matrix():
        random_state = np.random.default_rng()
        H = np.eye(3) + random_state.standard_normal((3, 3))
        Q, R = np.linalg.qr(H)
        return Q

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, PDF.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        output = {
            "status_code": 0,
            "response": "",
            "object": None
        }

        # Preprocessing
        src_color = src.get_colors()
        ref_color = ref.get_colors()
        out_img = deepcopy(src)

        # Change range from [0.0, 1.0] to [0, 255]
        src_color = src_color.squeeze() * 255.0
        ref_color = ref_color.squeeze() * 255.0

        m = 1.0
        max_range = 442
        stretch = round(math.pow(max_range, 1.0 / m))
        c_range = int(stretch * 2 + 1)

        for t in range(opt.iterations):
            mat_rot = PDF.random_rotation_matrix()
            mat_rot_inv = np.linalg.inv(mat_rot)

            src_rotated = np.einsum('ij,kj->ki', mat_rot, src_color)
            ref_rotated = np.einsum('ij,kj->ki', mat_rot, ref_color)

            # Calculate 1D pdf
            src_marginals = [np.histogram(src_rotated[:, i], bins=c_range, range=(-max_range, max_range), density=True)[0] for i in range(3)]
            ref_marginals = [np.histogram(ref_rotated[:, i], bins=c_range, range=(-max_range, max_range), density=True)[0] for i in range(3)]

            # Calculate cumulative 1D pdf
            src_cum_marginals = [np.cumsum(marg) for marg in src_marginals]
            ref_cum_marginals = [np.cumsum(marg) for marg in ref_marginals]

            lut = []
            for src_marg, ref_marg in zip(src_cum_marginals, ref_cum_marginals):
                lut_channel = np.zeros(c_range)
                for i, elem in enumerate(src_marg):
                    lut_channel[i] = np.abs(ref_marg - elem).argmin()
                lut.append(lut_channel)

            src_rotated_marginals = [(src_rotated[:, i].astype("int64") + stretch) for i in range(3)]
            transferred_rotated = np.stack([lut_channel[marginal] for lut_channel, marginal in zip(lut, src_rotated_marginals)], axis=-1)

            src_color = np.einsum('ij,kj->ki', mat_rot_inv, transferred_rotated - stretch)
            src_color = np.clip(src_color, 0, 255)

        out_img.set_colors(src_color[:,np.newaxis,:]/255.0)

        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/Algorithms/RHG/init.py

from . import RHG

ColorTransferLib/Algorithms/RHG/histoGAN/init.py

#from histoGAN.histoGAN import Trainer, HistoGAN, NanException, \
#  Generator, GeneratorBlock, HistVectorizer, RGBBlock, \
#  Conv2DMod, Discriminator, DiscriminatorBlock

ColorTransferLib/Algorithms/RHG/ReHistoGAN/rehistoGAN.py

"""
 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
"""

import json
from math import log2, sqrt, pi
from shutil import rmtree
from functools import partial
import multiprocessing
import numpy as np
import torch
from torch import nn
from torch.utils import data
import torch.nn.functional as F
from torch_optimizer import DiffGrad
from torch.autograd import grad as torch_grad
import torchvision
from torchvision import transforms
from PIL import Image
from pathlib import Path

from ..utils import color_transfer_MKL as ct
from ..utils import pyramid_upsampling as upsampling
from ..histoGAN.histoGAN import GeneratorBlock, HistVectorizer, NanException, Discriminator, Conv2DMod
from ..histogram_classes.RGBuvHistBlock import RGBuvHistBlock

try:
    from apex import amp
    APEX_AVAILABLE = True
except:
    APEX_AVAILABLE = False

# TEMP REMOVED
#assert torch.cuda.is_available(), ('You need to have an Nvidia GPU with CUDA '
#                                   'installed.')

num_cores = multiprocessing.cpu_count()

# constants
EXTS = ['jpg', 'png']
EPS = 1e-8
SCALE = 1 / np.sqrt(2.0)

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# helper classes
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class NanException(Exception):
    pass

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Flatten(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        return x.reshape(x.shape[0], -1)

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Residual(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        return self.fn(x) + x

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Rezero(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, fn):
        super().__init__()
        self.fn = fn
        self.g = nn.Parameter(torch.zeros(1))

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        return self.fn(x) * self.g

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class PermuteToFrom(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        x = x.permute(0, 2, 3, 1)
        out, loss = self.fn(x)
        out = out.permute(0, 3, 1, 2)
        return out, loss

# helpers

def default(value, d):
    return d if value is None else value

def cycle(iterable):
    while True:
        for i in iterable:
            yield i

def cast_list(el):
    return el if isinstance(el, list) else [el]

def is_empty(t):
    if isinstance(t, torch.Tensor):
        return t.nelement() == 0
    return t is None

def raise_if_nan(t):
    if torch.isnan(t):
        raise NanException

def loss_backwards(fp16, loss, optimizer, **kwargs):
    if fp16:
        with amp.scale_loss(loss, optimizer) as scaled_loss:
            scaled_loss.backward(**kwargs)
    else:
        loss.backward(**kwargs)

def gradient_penalty(images, output, weight=10):
    batch_size = images.shape[0]

    if not torch.cuda.is_available():
        ga = torch.ones(output.size()).cpu()
    else:
        ga = torch.ones(output.size()).cuda()

    gradients = torch_grad(outputs=output, inputs=images,
                           grad_outputs=ga,
                           create_graph=True, retain_graph=True,
                           only_inputs=True)[0]

    gradients = gradients.view(batch_size, -1)
    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()

def noise(n, latent_dim):
    if not torch.cuda.is_available():
        return torch.randn(n, latent_dim).cpu()
    else:
        return torch.randn(n, latent_dim).cuda()

def noise_list(n, layers, latent_dim):
    return [(noise(n, latent_dim), layers)]

def mixed_list(n, layers, latent_dim):
    tt = int(torch.rand(()).numpy() * layers)
    return noise_list(n, tt, latent_dim) + noise_list(n, layers - tt,
                                                      latent_dim)

def hist_interpolation(hist1, hist2):
    ratio = torch.rand(1)
    return hist1 * ratio + hist2 * (1 - ratio)

def latent_to_w(style_vectorizer, latent_descr):
    return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]

def image_noise(n, im_size):
    if not torch.cuda.is_available():
        return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0.0, 1.).cpu()
    else:
        return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0.0, 1.).cuda()

def leaky_relu(p=0.2):
    return nn.LeakyReLU(p, inplace=True)

def slerp(val, low, high):
    low_norm = low / torch.norm(low, dim=1, keepdim=True)
    high_norm = high / torch.norm(high, dim=1, keepdim=True)
    omega = torch.acos((low_norm * high_norm).sum(1))
    so = torch.sin(omega)
    res = ((torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (
        torch.sin(val * omega) / so).unsqueeze(1) * high)
    return res

def evaluate_in_chunks(max_batch_size, model, *args):
    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size,
                                                      dim=0), args))))
    chunked_outputs = [model(*i) for i in split_args]
    if len(chunked_outputs) == 1:
        return chunked_outputs[0]
    return torch.cat(chunked_outputs, dim=0)

def styles_def_to_tensor(styles_def):
    return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def],
                     dim=1)

def set_requires_grad(model, bool):
    for p in model.parameters():
        p.requires_grad = bool

def get_gaussian_kernel(kernel_size=15, sigma=3, channels=3):
    x_coord = torch.arange(kernel_size)
    x_grid = x_coord.repeat(kernel_size).view(kernel_size, kernel_size)
    y_grid = x_grid.t()
    xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()
    mean = (kernel_size - 1) / 2.
    variance = sigma ** 2.
    gaussian_kernel = (1. / (2. * pi * variance)) * torch.exp(
        -torch.sum((xy_grid - mean) ** 2., dim=-1) / (2 * variance))
    gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)
    gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)
    gaussian_kernel = gaussian_kernel.repeat(channels, 1, 1, 1)
    gaussian_filter = nn.Conv2d(in_channels=channels, out_channels=channels,
                                kernel_size=kernel_size, groups=channels,
                                bias=False)
    gaussian_filter.weight.data = gaussian_kernel
    gaussian_filter.weight.requires_grad = False

    return gaussian_filter

def gaussian_op(x, kernel=None):
    if not torch.cuda.is_available():
        device = "cpu"
    else:
        device = "cuda"

    if kernel is None:
        kernel = get_gaussian_kernel(kernel_size=15, sigma=15, channels=3).to(
            device=device)
    return kernel(x)

def laplacian_op(x, kernel=None):
    if not torch.cuda.is_available():
        device = "cpu"
    else:
        device = "cuda"

    if kernel is None:
        laplacian = [[0, 1, 0], [1, -4, 1], [0, 1, 0]]
        channels = x.size()[1]
        kernel = torch.tensor(laplacian,
                              dtype=torch.float32).unsqueeze(0).expand(
            1, channels, 3, 3).to(device=device)
    return F.conv2d(x, kernel, stride=1, padding=1)

def sobel_op(x, dir=0, kernel=None):
    if not torch.cuda.is_available():
        device = "cpu"
    else:
        device = "cuda"
    if kernel is None:
        if dir == 0:
            sobel = [[1, 0, -1], [2, 0, -2], [1, 0, -1]]  # x
        elif dir == 1:
            sobel = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]  # y
        channels = x.size()[1]
        kernel = torch.tensor(sobel, dtype=torch.float32).unsqueeze(0).expand(
            1, channels, 3, 3).to(device=device)
    return F.conv2d(x, kernel, stride=1, padding=1)

# dataset

def convert_rgb_to_transparent(image):
    if image.mode == 'RGB':
        return image.convert('RGBA')
    return image

def convert_transparent_to_rgb(image):
    if image.mode == 'RGBA':
        return image.convert('RGB')
    return image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class expand_greyscale(object):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, num_channels):
        self.num_channels = num_channels

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __call__(self, tensor):
        return tensor.expand(self.num_channels, -1, -1)

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class reconstruction_loss(object):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, loss):
        if not torch.cuda.is_available():
            device = "cpu"
        else:
            device = "cuda"

        self.loss = loss
        if self.loss == '1st gradient':
            sobel_x = [[1, 0, -1], [2, 0, -2], [1, 0, -1]]  # x
            sobel_y = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]  # y
            sobel_x = torch.tensor(
                sobel_x, dtype=torch.float32).unsqueeze(0).expand(
                1, 3, 3, 3).to(device=device)
            sobel_y = torch.tensor(
                sobel_y, dtype=torch.float32).unsqueeze(0).expand(
                1, 3, 3, 3).to(device=device)
            self.kernel1 = sobel_x
            self.kernel2 = sobel_y
        elif self.loss == '2nd gradient':
            laplacian = [[0, 1, 0], [1, -4, 1], [0, 1, 0]]
            self.kernel1 = torch.tensor(
                laplacian, dtype=torch.float32).unsqueeze(0).expand(
                1, 3, 3, 3).to(device=device)
            self.kernel2 = None
        else:
            self.kernel1 = None
            self.kernel2 = None

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def compute_loss(self, input, target):
        if self.loss == 'L1':
            reconstruction_loss = torch.mean(torch.abs(input - target))  # L1

        elif self.loss == '1st gradient':
            input_dfdx = sobel_op(input, kernel=self.kernel1)
            input_dfdy = sobel_op(input, kernel=self.kernel2)
            target_dfdx = sobel_op(target, kernel=self.kernel1)
            target_dfdy = sobel_op(target, kernel=self.kernel2)
            input_gradient = torch.sqrt(torch.pow(input_dfdx, 2) +
                                        torch.pow(input_dfdy, 2))
            target_gradient = torch.sqrt(torch.pow(
                target_dfdx, 2) + torch.pow(target_dfdy, 2))
            reconstruction_loss = torch.mean(torch.abs(
                input_gradient - target_gradient))  # L1

        elif self.loss == '2nd gradient':
            input_lap = laplacian_op(input, kernel=self.kernel1)
            target_lap = laplacian_op(target, kernel=self.kernel1)
            reconstruction_loss = torch.mean(
                torch.abs(input_lap - target_lap))  # L1
        else:
            reconstruction_loss = None

        return reconstruction_loss

def resize_to_minimum_size(min_size, image):
    if max(*image.size) < min_size:
        return torchvision.transforms.functional.resize(image, min_size)
    return image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Dataset(data.Dataset):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, folder, image_size=256, transparent=False,
                 hist_insz=150, hist_bin=64, hist_sampling=True,
                 hist_method='inverse-quadratic', hist_resizing='sampling',
                 triple_hist=False,
                 double_hist=False):
        super().__init__()
        self.folder = folder
        self.image_size = image_size
        self.paths = [p for ext in EXTS for p in Path(
            f'{folder}').glob(f'**/*.{ext}')]
        self.histblock = RGBuvHistBlock(
            insz=hist_insz, h=hist_bin, method=hist_method,
            resizing=hist_resizing, device='cpu')
        self.hist_sampling = hist_sampling

        self.triple_hist = triple_hist
        self.double_hist = double_hist
        set_requires_grad(self.histblock, False)
        convert_image_fn = (convert_transparent_to_rgb if not transparent else
                            convert_rgb_to_transparent)
        num_channels = 3 if not transparent else 4

        self.transform = transforms.Compose([
            transforms.Lambda(convert_image_fn),
            transforms.Lambda(partial(resize_to_minimum_size, image_size)),
            transforms.RandomHorizontalFlip(),
            transforms.Resize(image_size),
            transforms.CenterCrop(image_size),
            transforms.ToTensor(),
            transforms.Lambda(expand_greyscale(num_channels))
        ])

        self.transform_hist = transforms.Compose([
            transforms.Lambda(convert_image_fn), transforms.ToTensor(),
            transforms.Lambda(expand_greyscale(num_channels))
        ])

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __len__(self):
        return len(self.paths)

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __getitem__(self, index):

        path = self.paths[index]
        img = Image.open(path)
        if self.hist_sampling is True:
            if self.triple_hist is True:
                inds = np.random.randint(0, high=len(self.paths), size=6)
                img1 = Image.open(self.paths[inds[0]])
                img2 = Image.open(self.paths[inds[1]])
                img3 = Image.open(self.paths[inds[2]])
                img4 = Image.open(self.paths[inds[3]])
                img5 = Image.open(self.paths[inds[4]])
                img6 = Image.open(self.paths[inds[5]])

                hist1 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img1), dim=0))
                hist2 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img2), dim=0))
                hist3 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img3), dim=0))
                hist4 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img4), dim=0))
                hist5 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img5), dim=0))
                hist6 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img6), dim=0))
                return {'images': self.transform(img),
                        'histograms': torch.squeeze(hist_interpolation(hist1, hist2)),
                        'histograms2': torch.squeeze(hist_interpolation(hist3, hist4)),
                        'histograms3': torch.squeeze(hist_interpolation(hist5, hist6))}

            elif self.double_hist is True:
                inds = np.random.randint(0, high=len(self.paths), size=4)
                img1 = Image.open(self.paths[inds[0]])
                img2 = Image.open(self.paths[inds[1]])
                img3 = Image.open(self.paths[inds[2]])
                img4 = Image.open(self.paths[inds[3]])

                hist1 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img1), dim=0))
                hist2 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img2), dim=0))
                hist3 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img3), dim=0))
                hist4 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img4), dim=0))

                return {'images': self.transform(img),
                        'histograms': torch.squeeze(hist_interpolation(
                            hist1, hist2)),
                        'histograms2': torch.squeeze(hist_interpolation(
                            hist3, hist4))}
            else:
                inds = np.random.randint(0, high=len(self.paths), size=2)
                img1 = Image.open(self.paths[inds[0]])
                img2 = Image.open(self.paths[inds[1]])
                hist1 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img1), dim=0))
                hist2 = self.histblock(torch.unsqueeze(
                    self.transform_hist(img2), dim=0))
                return {'images': self.transform(img),
                        'histograms': torch.squeeze(
                    hist_interpolation(hist1, hist2))}

        else:
            hist = self.histblock(torch.unsqueeze(
                self.transform_hist(img), dim=0))
            return {'images': self.transform(img),
                    'histograms': torch.squeeze(hist)}

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class RecoloringGAN(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, image_size, latent_dim, network_capacity=16,
                 transparent=False):
        super().__init__()
        self.image_size = image_size
        self.latent_dim = latent_dim
        self.num_layers = int(log2(image_size) - 1)
        init_channels = 4 * network_capacity
        filters = [init_channels] + [network_capacity * (2 ** (i + 1))
                                     for i in range(self.num_layers)][::-1]
        filters = filters[-3:]
        in_out_pairs = zip(filters[0:-1], filters[1:])
        self.num_layers = 2

        self.blocks = nn.ModuleList([])
        for ind, (in_chan, out_chan) in enumerate(in_out_pairs):
            not_first = True
            not_last = ind != 1

            block = GeneratorBlock(
                latent_dim,
                in_chan,
                out_chan,
                upsample=not_first,
                upsample_rgb=not_last,
                rgba=transparent
            )
            self.blocks.append(block)

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x, rgb, hists, input_noise, latent1=None, latent2=None):
        rgb = None
        x, rgb = self.blocks[0](x, rgb, hists, input_noise, latent=latent1)
        x, rgb = self.blocks[1](x, rgb, hists, input_noise, latent=latent2)
        return rgb

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class EncoderBlock(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, input_channels, filters):
        super().__init__()
        self.conv_res = nn.Conv2d(input_channels, filters, 1)
        self.net = nn.Sequential(
            nn.Conv2d(input_channels, filters, 3, padding=1),
            nn.InstanceNorm2d(filters),
            leaky_relu(),
            nn.Conv2d(filters, filters, 3, padding=1),
            nn.InstanceNorm2d(filters),
            leaky_relu()
        )
        self.downsample = nn.Conv2d(filters, filters, 3, padding=1, stride=2)

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        res = self.conv_res(x)
        x = self.net(x)
        x = x + res
        x_d = self.downsample(x)
        return x_d, x

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class DecoderBlock(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, input_channels, filters, internal_hist=False,
                 latent_dim=None):
        super().__init__()
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear',
                                    align_corners=False)
        self.conv_res = nn.Conv2d(input_channels, filters, 1)
        self.block1 = nn.Sequential(
            nn.Conv2d(input_channels, input_channels, 3, padding=1),
            leaky_relu()
        )
        self.block2 = nn.Sequential(
            nn.Conv2d(input_channels * 2, filters, 3, padding=1),
            leaky_relu()
        )
        self.conv_out_latent = nn.Sequential(
            nn.Conv2d(filters, filters, 3, padding=1),
            leaky_relu())
        self.conv_out_rgb = nn.Conv2d(filters, 3, 1)

        if internal_hist:
            self.to_latent = nn.Linear(latent_dim, input_channels)
            self.conv_latent = Conv2DMod(input_channels, input_channels, 3)
        else:
            self.to_latent = None
            self.conv_latent = None

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x, prev_rgb, prev_latent, h=None):
        curr_latent = self.block1(x)
        if self.to_latent is not None:
            prev_latent = self.conv_latent(prev_latent, self.to_latent(h))
        processed_x = self.block2(torch.cat((curr_latent, prev_latent), dim=1))
        x_res = self.conv_res(x)
        x = self.conv_out_latent(x_res + processed_x)
        rgb = self.conv_out_rgb(x)
        if prev_rgb is not None:
            rgb = rgb + prev_rgb
        rgb = self.upsample(rgb)
        x = self.upsample(x)
        return x, rgb

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class RecoloringEncoderDecoder(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, image_size, network_capacity=16, hist=64,
                 latent_dim=512, style_depth=8,
                 skip_conn_to_GAN=False, internal_hist=False):
        super().__init__()
        self.image_size = image_size
        self.encoder_num_layers = int(log2(image_size) - 2)
        self.decoder_num_layers = int(log2(image_size) - 4)
        self.skip_conn_to_GAN = skip_conn_to_GAN
        self.internal_hist = internal_hist
        encoder_filters = [network_capacity] + \
                          [network_capacity * (2 ** (i + 1))
                           for i in range(self.encoder_num_layers)]

        encoder_in_out_pairs = zip(encoder_filters[0:-1], encoder_filters[1:])

        decoder_filters = encoder_filters
        decoder_filters.reverse()
        decoder_filters = decoder_filters[:-(self.encoder_num_layers -
                                             self.decoder_num_layers)]
        decoder_in_out_pairs = zip(decoder_filters[0:-1], decoder_filters[1:])

        self.encoder_blocks = nn.ModuleList([])
        self.decoder_blocks = nn.ModuleList([])
        self.decoder_mapping = nn.Conv2d(decoder_filters[-1],
                                         8 * network_capacity, 1)
        self.mapping = nn.Conv2d(3, network_capacity, 3, padding=1)
        if self.skip_conn_to_GAN and not self.internal_hist:
            self.hist_projection = HistVectorizer(hist, latent_dim,
                                                  int(style_depth))
            self.to_latent_1 = nn.Linear(latent_dim, encoder_filters[-3])
            self.to_latent_2 = nn.Linear(latent_dim, encoder_filters[-2])
            self.conv_latent_1 = Conv2DMod(encoder_filters[-3],
                                           2 ** 2 * network_capacity, 3)
            self.conv_latent_2 = Conv2DMod(encoder_filters[-2],
                                           2 ** (2 - 1) * network_capacity, 3)
        elif self.skip_conn_to_GAN and self.internal_hist:
            self.to_latent_1 = nn.Linear(latent_dim, encoder_filters[-3])
            self.to_latent_2 = nn.Linear(latent_dim, encoder_filters[-2])
            self.conv_latent_1 = Conv2DMod(encoder_filters[-3],
                                           2 ** 2 * network_capacity, 3)
            self.conv_latent_2 = Conv2DMod(encoder_filters[-2],
                                           2 ** (2 - 1) * network_capacity, 3)

        for ind, (in_chan, out_chan) in enumerate(encoder_in_out_pairs):
            block = EncoderBlock(in_chan, out_chan)
            self.encoder_blocks.append(block)

        for ind, (in_chan, out_chan) in enumerate(decoder_in_out_pairs):
            block = DecoderBlock(
                in_chan, out_chan, internal_hist=self.internal_hist,
                latent_dim=latent_dim)
            self.decoder_blocks.append(block)

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x, hists=None):
        if self.skip_conn_to_GAN and not self.internal_hist:
            h_w_space = self.hist_projection(hists)
            h1 = self.to_latent_1(h_w_space)
            h2 = self.to_latent_2(h_w_space)
        elif self.skip_conn_to_GAN and self.internal_hist:
            h1 = self.to_latent_1(hists)
            h2 = self.to_latent_2(hists)

        x = self.mapping(x)
        x_list = []
        if self.skip_conn_to_GAN:
            x_list_up = []
        for block in self.encoder_blocks:
            x, xup = block(x)
            x_list.append(x)
            if self.skip_conn_to_GAN:
                x_list_up.append(xup)

        x_list.reverse()
        x_list_e = x_list[:-2]
        if self.skip_conn_to_GAN:
            processed_latent_1 = self.conv_latent_1(x_list_up[1], h1)
            processed_latent_2 = self.conv_latent_2(x_list_up[0], h2)
        rgb = None
        for prev_latent, block in zip(x_list_e, self.decoder_blocks):
            x, rgb = block(x, rgb, prev_latent, h=hists)
        x = self.decoder_mapping(x)
        if self.skip_conn_to_GAN:
            return x, rgb, processed_latent_1, processed_latent_2
        else:
            return x, rgb

class recoloringGAN(nn.Module):
    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def __init__(self, image_size, latent_dim=512, style_depth=8,
                 network_capacity=16, transparent=False, fp16=False,
                 steps=1, lr=1e-4, fq_layers=[], fq_dict_size=256,
                 attn_layers=[], hist=64, skip_conn_to_GAN=False,
                 fixed_gan_weights=False, initialize_gan=False,
                 internal_hist=False):
        super().__init__()

        self.lr = lr
        self.steps = steps
        self.fixed_gan_weights = fixed_gan_weights
        self.internal_hist = internal_hist
        self.skip_conn_to_GAN = skip_conn_to_GAN
        self.ED = RecoloringEncoderDecoder(image_size,
                                           network_capacity=network_capacity,
                                           hist=hist,
                                           latent_dim=latent_dim,
                                           style_depth=style_depth,
                                           skip_conn_to_GAN=skip_conn_to_GAN,
                                           internal_hist=self.internal_hist)
        self.H = HistVectorizer(hist, latent_dim, int(style_depth))
        self.G = RecoloringGAN(image_size, latent_dim, network_capacity,
                               transparent=transparent)
        self.D = Discriminator(image_size, network_capacity,
                               fq_layers=fq_layers,
                               fq_dict_size=fq_dict_size,
                               attn_layers=attn_layers, transparent=transparent)

        set_requires_grad(self.ED, True)
        set_requires_grad(self.H, True)
        set_requires_grad(self.G, True)
        set_requires_grad(self.D, True)

        if self.fixed_gan_weights == False:
            learnable_params = list(
                self.ED.parameters()) + list(
                self.G.parameters()) + list(self.H.parameters())
        else:
            learnable_params = self.ED.parameters()
        self.G_opt = DiffGrad(learnable_params, lr=self.lr, betas=(0.5, 0.9))
        self.D_opt = DiffGrad(self.D.parameters(),
                              lr=self.lr, betas=(0.5, 0.9))
        if initialize_gan:
            self._init_weights(initializeGAN=True)
        else:
            self._init_weights(initializeGAN=False)

        self._init_weights(self.ED)
        self._init_weights(self.D)

        if not torch.cuda.is_available():
            self.cpu()
        else:
            self.cuda()

        if fp16:
            (self.ED, self.G, self.H, self.D), (self.G_opt, self.D_opt) = (
                amp.initialize(
                    [self.ED, self.G, self.H, self.D], [self.G_opt, self.D_opt],
                    opt_level='O2'))

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def _init_weights(self, initializeGAN=False):
        if initializeGAN:
            for block in self.G.blocks:
                nn.init.zeros_(block.to_noise1.weight)
                nn.init.zeros_(block.to_noise2.weight)
                nn.init.zeros_(block.to_noise1.bias)
                nn.init.zeros_(block.to_noise2.bias)
            for m in self.H.modules():
                if type(m) in {nn.Conv2d, nn.Linear}:
                    nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in',
                                            nonlinearity='leaky_relu')

        for m in self.ED.modules():
            if type(m) in {nn.Conv2d, nn.Linear}:
                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in',
                                        nonlinearity='leaky_relu')

        for m in self.D.modules():
            if type(m) in {nn.Conv2d, nn.Linear}:
                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in',
                                        nonlinearity='leaky_relu')

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def forward(self, x):
        return x

class recoloringTrainer():
    def __init__(self, name, results_dir, models_dir, image_size,
                 network_capacity, transparent=False, batch_size=4,
                 mixed_prob=0.9, gradient_accumulate_every=1, lr=2e-4,
                 num_workers=None, save_every=1000, trunc_psi=0.6,
                 fp16=False, fq_layers=[], fq_dict_size=256, attn_layers=[],
                 hist_method='inverse-quadratic',
                 hist_resizing='sampling', hist_sigma=0.02, hist_bin=64,
                 hist_insz=150, fixed_gan_weights=False, skip_conn_to_GAN=False,
                 rec_loss='laplacian', initialize_gan=False,
                 variance_loss=True, internal_hist=False,
                 change_hyperparameters=False,
                 change_hyperparameters_after=100000, *args, **kwargs):

        self.GAN_params = [args, kwargs]
        self.GAN = None
        self.hist_method = hist_method
        self.hist_resizing = hist_resizing
        self.hist_sigma = hist_sigma
        self.hist_bin = hist_bin
        self.change_hyperparameters_after = change_hyperparameters_after
        self.hist_insz = hist_insz
        self.rec_loss = rec_loss
        self.internal_hist = internal_hist
        self.change_hyperparameters = change_hyperparameters
        self.variance_loss = variance_loss
        self.fixed_gan_weights = fixed_gan_weights
        self.skip_conn_to_GAN = skip_conn_to_GAN
        self.initialize_gan = initialize_gan
        self.histBlock = RGBuvHistBlock(insz=self.hist_insz, h=self.hist_bin,
                                        method=self.hist_method,
                                        resizing=self.hist_resizing,
                                        sigma=self.hist_sigma)
        set_requires_grad(self.histBlock, True)

        if variance_loss is True:
            if not torch.cuda.is_available():
                device = "cpu"
            else:
                device = "cuda"
            self.histBlock_input = RGBuvHistBlock(insz=self.hist_insz,
                                                  h=self.hist_bin,
                                                  method=self.hist_method,
                                                  resizing=self.hist_resizing,
                                                  sigma=self.hist_sigma)

            self.gaussKernel = get_gaussian_kernel(kernel_size=15,
                                                   sigma=5, channels=3).to(
                                                    device=device)

        if self.rec_loss is None:
            self.rec_loss_func = reconstruction_loss('L1')
        elif self.rec_loss == 'sobel':
            self.rec_loss_func = reconstruction_loss('1st gradient')
        elif self.rec_loss == 'laplacian':
            self.rec_loss_func = reconstruction_loss('2nd gradient')
        else:
            raise Exception('Unknown reconstruction losst!')

        self.name = name
        self.results_dir = Path(results_dir)
        self.models_dir = Path(models_dir)
        self.config_path = self.models_dir / name / '.config.json'

        assert log2(image_size).is_integer(), ('image size must be a power of 2 (64, 128, 256, 512, 1024)')
        self.image_size = image_size
        self.network_capacity = network_capacity
        self.transparent = transparent
        self.fq_layers = cast_list(fq_layers)
        self.fq_dict_size = fq_dict_size

        self.attn_layers = cast_list(attn_layers)

        self.lr = lr
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.mixed_prob = mixed_prob

        self.save_every = save_every
        self.steps = 0

        self.av = None
        self.trunc_psi = trunc_psi

        self.gradient_accumulate_every = gradient_accumulate_every

        assert not fp16 or fp16 and APEX_AVAILABLE, ('Apex is not available for '
                                                     'you to use mixed precision '
                                                     'training')
        self.fp16 = fp16
        self.d_loss = 0
        self.g_loss = 0
        self.last_gp_loss = 0
        self.last_cr_loss = 0
        self.q_loss = 0
        if self.variance_loss is True:
            self.var_loss = 0

        #self.init_folders()

        self.loader = None

        self.loader_evaluate = None

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def init_GAN(self):
        args, kwargs = self.GAN_params
        self.GAN = recoloringGAN(lr=self.lr, image_size=self.image_size,
                                 network_capacity=self.network_capacity,
                                 transparent=self.transparent,
                                 fq_layers=self.fq_layers,
                                 fq_dict_size=self.fq_dict_size,
                                 attn_layers=self.attn_layers,
                                 fp16=self.fp16, hist=self.hist_bin,
                                 fixed_gan_weights=self.fixed_gan_weights,
                                 skip_conn_to_GAN=self.skip_conn_to_GAN,
                                 initialize_gan=self.initialize_gan,
                                 internal_hist=self.internal_hist,
                                 *args, **kwargs)

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def write_config(self):
        self.config_path.write_text(json.dumps(self.config()))

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def load_config(self):
        config = self.config() if not self.config_path.exists() else json.loads(
            self.config_path.read_text())
        self.image_size = config['image_size']
        self.network_capacity = config['network_capacity']
        self.transparent = config['transparent']
        self.fq_layers = config['fq_layers']
        self.fq_dict_size = config['fq_dict_size']
        self.attn_layers = config.pop('attn_layers', [])
        del self.GAN
        self.init_GAN()

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def config(self):
        return {'image_size': self.image_size,
                'network_capacity': self.network_capacity,
                'transparent': self.transparent,
                'fq_layers': self.fq_layers,
                'fq_dict_size': self.fq_dict_size,
                'attn_layers': self.attn_layers}

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def set_data_src(self, folder, sampling):
        self.dataset = Dataset(folder, image_size=self.image_size,
                               transparent=self.transparent,
                               hist_insz=self.hist_insz,
                               hist_bin=self.hist_bin,
                               hist_method=self.hist_method,
                               hist_resizing=self.hist_resizing,
                               hist_sampling=sampling)
        self.loader = cycle(data.DataLoader(
            self.dataset, num_workers=default(self.num_workers, num_cores),
            batch_size=self.batch_size, drop_last=True, shuffle=True,
            pin_memory=True))
        if sampling is True:
            self.dataset_evaluate = Dataset(folder, image_size=self.image_size,
                                            transparent=self.transparent,
                                            hist_insz=self.hist_insz,
                                            hist_bin=self.hist_bin,
                                            hist_method=self.hist_method,
                                            hist_resizing=self.hist_resizing,
                                            hist_sampling=sampling,
                                            triple_hist=True)
        else:
            self.dataset_evaluate = Dataset(folder, image_size=self.image_size,
                                            transparent=self.transparent,
                                            hist_insz=self.hist_insz,
                                            hist_bin=self.hist_bin,
                                            hist_method=self.hist_method,
                                            hist_resizing=self.hist_resizing,
                                            hist_sampling=sampling)
        self.loader_evaluate = cycle(
            data.DataLoader(self.dataset_evaluate, num_workers=default(
                self.num_workers, num_cores), batch_size=4, drop_last=True,
                shuffle=True, pin_memory=True))

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    @torch.no_grad()
    def evaluate(self, num=0, image_batch=None, hist_batch=None,
                 triple_hist=False, double_hist=False, resizing=None,
                 resizing_method=None, swapping_levels=1,
                 pyramid_levels=5, level_blending=False, original_size=None,
                 input_image_name=None, original_image=None,
                 post_recoloring=False, save_input=True):
        self.GAN.eval()

        if hist_batch is None or image_batch is None:
            batch = next(self.loader_evaluate)
            if not torch.cuda.is_available():
                image_batch = batch['images'].cpu()
                hist_batch = batch['histograms'].cpu()
            else:
                image_batch = batch['images'].cuda()
                hist_batch = batch['histograms'].cuda()
            img_bt_sz = image_batch.shape[0]
            if triple_hist is True:
                image_batch = torch.cat((image_batch, image_batch,
                                         image_batch), dim=0)
                if not torch.cuda.is_available():
                    hist_batch_2 = batch['histograms2'].cpu()
                    hist_batch_3 = batch['histograms3'].cpu()
                else:
                    hist_batch_2 = batch['histograms2'].cuda()
                    hist_batch_3 = batch['histograms3'].cuda()
                hist_batch = torch.cat((hist_batch, hist_batch_2,
                                        hist_batch_3), dim=0)
            elif double_hist is True:
                image_batch = torch.cat((image_batch, image_batch), dim=0)
                if not torch.cuda.is_available():
                    hist_batch_2 = batch['histograms2'].cpu()
                else:
                    hist_batch_2 = batch['histograms2'].cuda()
                hist_batch = torch.cat((hist_batch, hist_batch_2), dim=0)
        else:
            img_bt_sz = image_batch.shape[0]

        noise = image_noise(hist_batch.shape[0], image_batch.shape[-1])
        h_w_space = self.GAN.H(hist_batch)

        if self.skip_conn_to_GAN and not self.internal_hist:
            image_latent, rgb, processed_latent_2, processed_latent_1 = (self.GAN.ED(image_batch, hist_batch))
            generated_images = self.GAN.G(image_latent, rgb, h_w_space, noise, processed_latent_2, processed_latent_1)
        elif self.skip_conn_to_GAN and self.internal_hist:
            image_latent, rgb, processed_latent_2, processed_latent_1 = (self.GAN.ED(image_batch, h_w_space))
            generated_images = self.GAN.G(image_latent, rgb, h_w_space, noise, processed_latent_2, processed_latent_1)
        elif self.internal_hist:
            image_latent, rgb = self.GAN.ED(image_batch, h_w_space)
            generated_images = self.GAN.G(image_latent, rgb, h_w_space, noise)
        else:
            image_latent, rgb = self.GAN.ED(image_batch, hist_batch)
            generated_images = self.GAN.G(image_latent, rgb, h_w_space, noise)

        ext = 'jpg' if not self.transparent else 'png'
        if double_hist is True or triple_hist is True:
            num_rows = img_bt_sz
        else:
            num_rows = int(np.ceil(sqrt(hist_batch.shape[0])))
        output_name = str(self.results_dir / self.name / f'{str(num)}-generated.{ext}')
        #torchvision.utils.save_image(generated_images, output_name, nrow=num_rows)

        if resizing is not None:
            if resizing == 'upscaling':
                print('Upsampling')
                if resizing_method == 'pyramid':
                    if not torch.cuda.is_available():
                        device = "cpu"
                    else:
                        device = "cuda"
                    # Image.open(input_image_name)
                    #print(output_name)
                    reference = Image.fromarray((input_image_name*255).astype("uint8"))
                    transform = transforms.Compose([transforms.ToTensor()])
                    reference = torch.unsqueeze(transform(reference), dim=0).to(device=device)
                    #output = upsampling.pyramid_upsampling(generated_images, reference, levels=pyramid_levels,swapping_levels=swapping_levels, blending=level_blending)
                    generated_images = upsampling.pyramid_upsampling(generated_images, reference, levels=pyramid_levels,swapping_levels=swapping_levels, blending=level_blending)

            elif resizing == 'downscaling':
                if original_size is not None:
                    print('Resizing')
                    # img = Image.open(output_name)
                    # img = img.resize((original_size[0], original_size[1]))
                    # img.save(output_name)
                    generated_images = generated_images.resize((original_size[0], original_size[1]))

        if post_recoloring is True:
            target = torch.squeeze(generated_images, dim=0).cpu().detach().numpy()
            target = target.transpose(1, 2, 0)
            print('Post-recoloring')
            result = ct.color_transfer_MKL(original_image, target)
            result = Image.fromarray(np.uint8(result * 255))

        return generated_images

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def print_log(self):
        if hasattr(self, 'var_loss'):
            print(f'\nG: {self.g_loss:.2f} | H: {self.h_loss:.2f} | '
                  f'D: {self.d_loss:.2f} | R: {self.r_loss:.2f} '
                  f'| V: {self.var_loss:.2f} | GP: {self.last_gp_loss:.2f}'
                  f' | CR: {self.last_cr_loss:.2f} | Q: {self.q_loss:.2f}')
        else:
            print(f'\nG: {self.g_loss:.2f} | H: {self.h_loss:.2f} | '
                  f'D: {self.d_loss:.2f} | R: {self.r_loss:.2f} |'
                  f' GP: {self.last_gp_loss:.2f} '
                  f'| CR: {self.last_cr_loss:.2f} | Q: {self.q_loss:.2f}')

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def model_name(self, num):
        return str(self.models_dir / self.name / f'model_0.pt')

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def clear(self):
        rmtree(f'./models/{self.name}', True)
        rmtree(f'./results/{self.name}', True)
        rmtree(str(self.config_path), True)
        self.init_folders()

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def save(self, num):
        torch.save(self.GAN.state_dict(), self.model_name(num))
        self.write_config()

    # ----------------------------------------------------------------------------------------------------------------------
    #
    # ----------------------------------------------------------------------------------------------------------------------
    def load(self, name):
        self.load_config()
        mm = "Models/RHG/" + name + ".pt"
        if not torch.cuda.is_available():
            device = torch.device('cpu')
        else:
            device = torch.device('cuda')
        self.GAN.load_state_dict(torch.load(mm, map_location=device))
        return 0

ColorTransferLib/Algorithms/RHG/histoGAN/histoGAN.py

"""
 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
"""

import json
from math import floor, log2
from random import random
from shutil import rmtree
from functools import partial
import multiprocessing
import numpy as np
import torch
from torch import nn
from torch.utils import data
import torch.nn.functional as F
from torch_optimizer import DiffGrad
from torch.autograd import grad as torch_grad
import torchvision
from torchvision import transforms
from vector_quantize_pytorch import VectorQuantize
from linear_attention_transformer import ImageLinearAttention
from PIL import Image
from pathlib import Path
from ..utils.diff_augment import DiffAugment
from ..histogram_classes.RGBuvHistBlock import RGBuvHistBlock

try:
  from apex import amp

  APEX_AVAILABLE = True
except:
  APEX_AVAILABLE = False

# TEMP REMOVED
#assert torch.cuda.is_available(), ('You need to have an Nvidia GPU with CUDA '
#                                   'installed.')

num_cores = multiprocessing.cpu_count()

# constants
EXTS = ['jpg', 'png']
EPS = 1e-8
SCALE = 1 / np.sqrt(2.0)

# helper classes
class NanException(Exception):
  pass

class EMA():
  def __init__(self, beta):
    super().__init__()
    self.beta = beta

  def update_average(self, old, new):
    if old is None:
      return new
    return old * self.beta + (1 - self.beta) * new

class Flatten(nn.Module):
  def forward(self, x):
    return x.reshape(x.shape[0], -1)

class RandomApply(nn.Module):
  def __init__(self, prob, fn, fn_else=lambda x: x):
    super().__init__()
    self.fn = fn
    self.fn_else = fn_else
    self.prob = prob

  def forward(self, x):
    fn = self.fn if random() < self.prob else self.fn_else
    return fn(x)

class Residual(nn.Module):
  def __init__(self, fn):
    super().__init__()
    self.fn = fn

  def forward(self, x):
    return self.fn(x) + x

class Rezero(nn.Module):
  def __init__(self, fn):
    super().__init__()
    self.fn = fn
    self.g = nn.Parameter(torch.zeros(1))

  def forward(self, x):
    return self.fn(x) * self.g

class PermuteToFrom(nn.Module):
  def __init__(self, fn):
    super().__init__()
    self.fn = fn

  def forward(self, x):
    x = x.permute(0, 2, 3, 1)
    out, loss = self.fn(x)
    out = out.permute(0, 3, 1, 2)
    return out, loss

# helpers

def default(value, d):
  return d if value is None else value

def cycle(iterable):
  while True:
    for i in iterable:
      yield i

def cast_list(el):
  return el if isinstance(el, list) else [el]

def is_empty(t):
  if isinstance(t, torch.Tensor):
    return t.nelement() == 0
  return t is None

def raise_if_nan(t):
  if torch.isnan(t):
    raise NanException

def loss_backwards(fp16, loss, optimizer, **kwargs):
  if fp16:
    with amp.scale_loss(loss, optimizer) as scaled_loss:
      scaled_loss.backward(**kwargs)
  else:
    loss.backward(**kwargs)

def gradient_penalty(images, output, weight=10):
  batch_size = images.shape[0]
  gradients = torch_grad(outputs=output, inputs=images,
                         grad_outputs=torch.ones(output.size()).cuda(),
                         create_graph=True, retain_graph=True,
                         only_inputs=True)[0]
  gradients = gradients.reshape(batch_size, -1)
  return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()

def noise(n, latent_dim):
  return torch.randn(n, latent_dim).cuda()

def noise_list(n, layers, latent_dim):
  return [(noise(n, latent_dim), layers)]

def mixed_list(n, layers, latent_dim):
  tt = int(torch.rand(()).numpy() * layers)
  return noise_list(n, tt, latent_dim) + noise_list(n, layers - tt, latent_dim)

def hist_interpolation(hist1, hist2):
  ratio = torch.rand(1)
  return hist1 * ratio + hist2 * (1 - ratio)

def latent_to_w(style_vectorizer, latent_descr):
  return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]

def image_noise(n, im_size):
  return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0.0, 1.0).cuda()

def leaky_relu(p=0.2):
  return nn.LeakyReLU(p, inplace=True)

def slerp(val, low, high):
  low_norm = low / torch.norm(low, dim=1, keepdim=True)
  high_norm = high / torch.norm(high, dim=1, keepdim=True)
  omega = torch.acos((low_norm * high_norm).sum(1))
  so = torch.sin(omega)
  res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low + (
      torch.sin(val * omega) / so).unsqueeze(1) * high
  return res

def evaluate_in_chunks(max_batch_size, model, *args):
  split_args = list(
    zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))
  chunked_outputs = [model(*i) for i in split_args]
  if len(chunked_outputs) == 1:
    return chunked_outputs[0]
  return torch.cat(chunked_outputs, dim=0)

def styles_def_to_tensor(styles_def):
  return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def],
                   dim=1)

def set_requires_grad(model, bool):
  for p in model.parameters():
    p.requires_grad = bool

# dataset

def convert_rgb_to_transparent(image):
  if image.mode == 'RGB':
    return image.convert('RGBA')
  return image

def convert_transparent_to_rgb(image):
  if image.mode == 'RGBA':
    return image.convert('RGB')
  return image

class expand_greyscale(object):
  def __init__(self, num_channels):
    self.num_channels = num_channels

  def __call__(self, tensor):
    return tensor.expand(self.num_channels, -1, -1)

def resize_to_minimum_size(min_size, image):
  if max(*image.size) < min_size:
    return torchvision.transforms.functional.resize(image, min_size)
  return image

class Dataset(data.Dataset):
  def __init__(self, folder, image_size=256, transparent=False, hist_insz=150,
               hist_bin=64, hist_method='inverse-quadratic',
               hist_resizing='sampling', test=False, aug_prob=0.0):
    super().__init__()
    self.folder = folder
    self.image_size = image_size
    self.test = test
    self.paths = [p for ext in EXTS for p in
                  Path(f'{folder}').glob(f'**/*.{ext}')]
    self.histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                    method=hist_method, resizing=hist_resizing,
                                    device='cpu')
    set_requires_grad(self.histblock, False)
    convert_image_fn = (convert_transparent_to_rgb if not transparent else
                        convert_rgb_to_transparent)
    num_channels = 3 if not transparent else 4

    if self.test is False:
      self.transform = transforms.Compose([
        transforms.Lambda(convert_image_fn),
        transforms.Lambda(partial(resize_to_minimum_size, self.image_size)),
        transforms.Resize(self.image_size),
        RandomApply(aug_prob, transforms.RandomResizedCrop(
          self.image_size, scale=(0.5, 1.0), ratio=(0.98, 1.02)),
                    transforms.CenterCrop(self.image_size)),
        transforms.ToTensor(),
        transforms.Lambda(expand_greyscale(num_channels))
      ])

    self.transform_hist = transforms.Compose([
      transforms.Lambda(convert_image_fn),
      transforms.ToTensor(),
      transforms.Lambda(expand_greyscale(num_channels))
    ])

  def __len__(self):
    return len(self.paths)

  def __getitem__(self, index):
    if self.test is False:
      path = self.paths[index]
      img = Image.open(path)
      inds = np.random.randint(0, high=len(self.paths), size=2)
      img1 = Image.open(self.paths[inds[0]])
      img2 = Image.open(self.paths[inds[1]])
      hist1 = self.histblock(torch.unsqueeze(self.transform_hist(img1), dim=0))
      hist2 = self.histblock(torch.unsqueeze(self.transform_hist(img2), dim=0))
      return {'images': self.transform(img),
              'histograms': torch.squeeze(hist_interpolation(hist1, hist2))}
    else:
      path = self.paths[index]
      img = Image.open(path)
      hist = self.histblock(torch.unsqueeze(self.transform_hist(img), dim=0))
      return {'histograms': torch.squeeze(hist)}

# augmentations

def random_hflip(tensor, prob):
  if prob > random():
    return tensor
  return torch.flip(tensor, dims=(3,))

class AugWrapper(nn.Module):
  def __init__(self, D):
    super().__init__()
    self.D = D

  def forward(self, images, prob=0.0, types=[], detach=False):
    if random() < prob:
      images = random_hflip(images, prob=0.5)
      images = DiffAugment(images, types=types)

    if detach:
      images = images.detach()

    return self.D(images)

class HistVectorizer(nn.Module):
  def __init__(self, insize, emb, depth):
    super().__init__()
    self.flatten = Flatten()
    fc_layers = []
    for i in range(depth):
      if i == 0:
        fc_layers.extend(
          [nn.Linear(insize * insize * 3, emb * 2), leaky_relu()])
      elif i == 1:
        fc_layers.extend([nn.Linear(emb * 2, emb), leaky_relu()])
      else:
        fc_layers.extend([nn.Linear(emb, emb), leaky_relu()])
    self.fcs = nn.Sequential(*fc_layers)

  def forward(self, x):
    return self.fcs(self.flatten(x))

class StyleVectorizer(nn.Module):
  def __init__(self, emb, depth):
    super().__init__()

    layers = []
    for i in range(depth):
      layers.extend([nn.Linear(emb, emb), leaky_relu()])

    self.net = nn.Sequential(*layers)

  def forward(self, x):
    return self.net(x)

class RGBBlock(nn.Module):
  def __init__(self, latent_dim, input_channel, upsample, rgba=False):
    super().__init__()
    self.input_channel = input_channel
    self.to_style = nn.Linear(latent_dim, input_channel)

    out_filters = 3 if not rgba else 4
    self.conv = Conv2DMod(input_channel, out_filters, 1, demod=False)

    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear',
                                align_corners=False) if upsample else None

  def forward(self, x, prev_rgb, istyle):
    style = self.to_style(istyle)
    x = self.conv(x, style)

    if prev_rgb is not None:
      x = x + prev_rgb

    if self.upsample is not None:
      x = self.upsample(x)

    return x

  def forward_(self, x, prev_rgb, style):
    x = self.conv(x, style)

    if prev_rgb is not None:
      x = x + prev_rgb

    if self.upsample is not None:
      x = self.upsample(x)

    return x

class Conv2DMod(nn.Module):
  def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1,
               dilation=1, **kwargs):
    super().__init__()
    self.filters = out_chan
    self.demod = demod
    self.kernel = kernel
    self.stride = stride
    self.dilation = dilation
    self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel)))
    nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in',
                            nonlinearity='leaky_relu')

  def _get_same_padding(self, size, kernel, dilation, stride):
    return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2

  def forward(self, x, y):
    b, c, h, w = x.shape

    w1 = y[:, None, :, None, None]
    w2 = self.weight[None, :, :, :, :]
    weights = w2 * (w1 + 1)

    if self.demod:
      d = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + EPS)
      weights = weights * d

    x = x.reshape(1, -1, h, w)

    _, _, *ws = weights.shape
    weights = weights.reshape(b * self.filters, *ws)

    padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)
    x = F.conv2d(x, weights, padding=padding, groups=b)

    x = x.reshape(-1, self.filters, h, w)
    return x

class GeneratorBlock(nn.Module):
  def __init__(self, latent_dim, input_channels, filters, upsample=True,
               upsample_rgb=True, rgba=False):
    super().__init__()
    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear',
                                align_corners=False) if upsample else None

    self.to_style1 = nn.Linear(latent_dim, input_channels)
    self.to_noise1 = nn.Linear(1, filters)
    self.conv1 = Conv2DMod(input_channels, filters, 3)

    self.to_style2 = nn.Linear(latent_dim, filters)
    self.to_noise2 = nn.Linear(1, filters)
    self.conv2 = Conv2DMod(filters, filters, 3)

    self.activation = leaky_relu()
    self.to_rgb = RGBBlock(latent_dim, filters, upsample_rgb, rgba)

  def forward(self, x, prev_rgb, istyle, inoise, latent=None):
    if self.upsample is not None:
      x = self.upsample(x)

    inoise = inoise[:, :x.shape[2], :x.shape[3], :]
    noise1 = self.to_noise1(inoise).permute((0, 3, 2, 1))
    noise2 = self.to_noise2(inoise).permute((0, 3, 2, 1))

    style1 = self.to_style1(istyle)
    x = self.conv1(x, style1)
    x = self.activation(x + noise1)
    if latent is not None:
      x = x + latent
    style2 = self.to_style2(istyle)
    x = self.conv2(x, style2)
    x = self.activation(x + noise2)

    rgb = self.to_rgb(x, prev_rgb, istyle)
    return x, rgb

  def forward_(self, x, prev_rgb, style1, style2, to_rgb_style,
               inoise=None, noise1=None, noise2=None, latent=None):
    if self.upsample is not None:
      x = self.upsample(x)
    if noise1 is not None and noise2 is not None:
      pass
    elif inoise is None:
      raise Exception('No noise is given')
    else:
      inoise = inoise[:, :x.shape[2], :x.shape[3], :]
      noise1 = self.to_noise1(inoise).permute((0, 3, 2, 1))
      noise2 = self.to_noise2(inoise).permute((0, 3, 2, 1))

    x = self.conv1(x, style1)
    x = self.activation(x + noise1)
    if latent is not None:
      x = x + latent
    x = self.conv2(x, style2)
    x = self.activation(x + noise2)

    rgb = self.to_rgb.forward_(x, prev_rgb, to_rgb_style)
    return x, rgb

class DiscriminatorBlock(nn.Module):
  def __init__(self, input_channels, filters, downsample=True):
    super().__init__()
    self.conv_res = nn.Conv2d(input_channels, filters, 1)

    self.net = nn.Sequential(
      nn.Conv2d(input_channels, filters, 3, padding=1),
      leaky_relu(),
      nn.Conv2d(filters, filters, 3, padding=1),
      leaky_relu()
    )

    self.downsample = nn.Conv2d(filters, filters, 3, padding=1,
                                stride=2) if downsample else None

  def forward(self, x):
    res = self.conv_res(x)
    x = self.net(x)
    x = x + res
    if self.downsample is not None:
      x = self.downsample(x)
    return x

class Generator(nn.Module):
  def __init__(self, image_size, latent_dim, network_capacity=16,
               transparent=False):
    super().__init__()
    self.image_size = image_size
    self.latent_dim = latent_dim
    self.num_layers = int(log2(image_size) - 1)

    init_channels = 4 * network_capacity
    self.initial_block = nn.Parameter(torch.randn((init_channels, 4, 4)))
    filters = [init_channels] + [network_capacity * (2 ** (i + 1)) for i in
                                 range(self.num_layers)][::-1]
    in_out_pairs = zip(filters[0:-1], filters[1:])

    self.blocks = nn.ModuleList([])
    for ind, (in_chan, out_chan) in enumerate(in_out_pairs):
      not_first = ind != 0
      not_last = ind != (self.num_layers - 1)

      block = GeneratorBlock(
        latent_dim,
        in_chan,
        out_chan,
        upsample=not_first,
        upsample_rgb=not_last,
        rgba=transparent
      )
      self.blocks.append(block)

  def forward(self, styles, hists, input_noise):
    batch_size = styles.shape[0]
    x = self.initial_block.expand(batch_size, -1, -1, -1)
    styles = styles.transpose(0, 1)
    hists = hists.transpose(0, 1)
    styles = torch.cat((styles, hists), dim=0)

    rgb = None
    for style, block in zip(styles, self.blocks):
      x, rgb = block(x, rgb, style, input_noise)
    return rgb

class Discriminator(nn.Module):
  def __init__(self, image_size, network_capacity=16, fq_layers=[],
               fq_dict_size=256, attn_layers=[],
               transparent=False):
    super().__init__()
    num_layers = int(log2(image_size) - 1)
    num_init_filters = 3 if not transparent else 4

    filters = [num_init_filters] + [(network_capacity) * (2 ** i) for i in
                                    range(num_layers + 1)]
    chan_in_out = list(zip(filters[0:-1], filters[1:]))

    blocks = []
    quantize_blocks = []
    attn_blocks = []
    for ind, (in_chan, out_chan) in enumerate(chan_in_out):
      num_layer = ind + 1
      is_not_last = ind != (len(chan_in_out) - 1)

      block = DiscriminatorBlock(in_chan, out_chan, downsample=is_not_last)
      blocks.append(block)

      attn_fn = nn.Sequential(*[
        Residual(Rezero(ImageLinearAttention(out_chan))) for _ in range(2)
      ]) if num_layer in attn_layers else None

      attn_blocks.append(attn_fn)

      quantize_fn = PermuteToFrom(VectorQuantize(
        out_chan, fq_dict_size)) if num_layer in fq_layers else None
      quantize_blocks.append(quantize_fn)

    self.blocks = nn.ModuleList(blocks)
    self.attn_blocks = nn.ModuleList(attn_blocks)
    self.quantize_blocks = nn.ModuleList(quantize_blocks)

    latent_dim = 2 * 2 * filters[-1]

    self.flatten = Flatten()
    self.to_logit = nn.Linear(latent_dim, 1)

  def forward(self, x):
    b, *_ = x.shape

    quantize_loss = torch.zeros(1).to(x)

    for (block, attn_block, q_block) in zip(self.blocks, self.attn_blocks,
                                            self.quantize_blocks):
      x = block(x)

      if attn_block is not None:
        x = attn_block(x)

      if q_block is not None:
        x, loss = q_block(x)
        quantize_loss += loss

    x = self.flatten(x)
    x = self.to_logit(x)
    return x.squeeze(), quantize_loss

class HistoGAN(nn.Module):
  def __init__(self, image_size, latent_dim=512, style_depth=8,
               network_capacity=16, transparent=False, fp16=False,
               steps=1, lr=1e-4, fq_layers=[], fq_dict_size=256, attn_layers=[],
               aug=False, hist=64):
    super().__init__()

    self.lr = lr
    self.aug = aug
    self.steps = steps
    self.ema_updater = EMA(0.995)
    self.S = StyleVectorizer(latent_dim, style_depth)
    self.H = HistVectorizer(hist, latent_dim, int(style_depth))
    self.G = Generator(image_size, latent_dim, network_capacity,
                       transparent=transparent)
    self.D = Discriminator(image_size, network_capacity, fq_layers=fq_layers,
                           fq_dict_size=fq_dict_size,
                           attn_layers=attn_layers, transparent=transparent)

    self.SE = StyleVectorizer(latent_dim, style_depth)
    self.HE = HistVectorizer(hist, latent_dim, int(style_depth))
    self.GE = Generator(image_size, latent_dim, network_capacity,
                        transparent=transparent)

    # wrapper for augmenting all images going into the discriminator
    if self.aug:
      self.D_aug = AugWrapper(self.D)
    else:
      self.D_aug = None

    set_requires_grad(self.SE, False)
    set_requires_grad(self.HE, False)
    set_requires_grad(self.GE, False)

    generator_params = list(self.G.parameters()) + list(
      self.S.parameters()) + list(self.H.parameters())
    self.G_opt = DiffGrad(generator_params, lr=self.lr, betas=(0.5, 0.9))
    self.D_opt = DiffGrad(self.D.parameters(), lr=self.lr, betas=(0.5, 0.9))

    self._init_weights()
    self.reset_parameter_averaging()

    self.cuda()

    if fp16:
      (self.S, self.G, self.D, self.H, self.SE, self.HE, self.GE), (
        self.G_opt, self.D_opt) = \
        amp.initialize(
          [self.S, self.G, self.D, self.H, self.SE, self.HE, self.GE],
          [self.G_opt, self.D_opt],
          opt_level='O2')

  def _init_weights(self):
    for m in self.modules():
      if type(m) in {nn.Conv2d, nn.Linear}:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in',
                                nonlinearity='leaky_relu')

    for block in self.G.blocks:
      nn.init.zeros_(block.to_noise1.weight)
      nn.init.zeros_(block.to_noise2.weight)
      nn.init.zeros_(block.to_noise1.bias)
      nn.init.zeros_(block.to_noise2.bias)

  def EMA(self):
    def update_moving_average(ma_model, current_model):
      for current_params, ma_params in zip(current_model.parameters(),
                                           ma_model.parameters()):
        old_weight, up_weight = ma_params.data, current_params.data
        ma_params.data = self.ema_updater.update_average(old_weight, up_weight)

    update_moving_average(self.SE, self.S)
    update_moving_average(self.HE, self.H)
    update_moving_average(self.GE, self.G)

  def reset_parameter_averaging(self):
    self.SE.load_state_dict(self.S.state_dict())
    self.HE.load_state_dict(self.H.state_dict())
    self.GE.load_state_dict(self.G.state_dict())

  def forward(self, x):
    return x

class Trainer():
  def __init__(self, name, results_dir, models_dir, image_size,
               network_capacity, transparent=False, batch_size=4,
               mixed_prob=0.9, gradient_accumulate_every=1, lr=2e-4,
               num_workers=None, save_every=1000, trunc_psi=0.6,
               fp16=False, fq_layers=[], fq_dict_size=256, attn_layers=[],
               hist_method='inverse-quadratic',
               hist_resizing='sampling', hist_sigma=0.02, hist_bin=64,
               hist_insz=150, aug_prob=0.0, dataset_aug_prob=0.0,
               aug_types=None, *args, **kwargs):
    if aug_types is None:
      aug_types = ['translation', 'cutout']
    self.GAN_params = [args, kwargs]
    self.GAN = None
    self.hist_method = hist_method
    self.hist_resizing = hist_resizing
    self.hist_sigma = hist_sigma
    self.hist_bin = hist_bin
    self.hist_insz = hist_insz
    self.histBlock = RGBuvHistBlock(insz=self.hist_insz, h=self.hist_bin,
                                    method=self.hist_method,
                                    resizing=self.hist_resizing,
                                    sigma=self.hist_sigma)
    set_requires_grad(self.histBlock, True)
    self.name = name
    self.results_dir = Path(results_dir)
    self.models_dir = Path(models_dir)
    self.config_path = self.models_dir / name / '.config.json'

    assert log2(
      image_size).is_integer(), ('image size must be a power of 2 (64, 128, '
                                 '256, 512, 1024)')
    self.image_size = image_size
    self.network_capacity = network_capacity
    self.transparent = transparent
    self.fq_layers = cast_list(fq_layers)
    self.fq_dict_size = fq_dict_size

    self.attn_layers = cast_list(attn_layers)

    self.aug_prob = aug_prob
    self.aug_types = aug_types
    self.dataset_aug_prob = dataset_aug_prob

    self.lr = lr
    self.batch_size = batch_size
    self.num_workers = num_workers
    self.mixed_prob = mixed_prob

    self.save_every = save_every
    self.steps = 0

    self.av = None
    self.trunc_psi = trunc_psi

    self.pl_mean = 0

    self.gradient_accumulate_every = gradient_accumulate_every

    assert not fp16 or fp16 and APEX_AVAILABLE, ('Apex is not available for '
                                                 'you to use mixed precision '
                                                 'training')
    self.fp16 = fp16

    self.d_loss = 0
    self.g_loss = 0
    self.last_gp_loss = 0
    self.last_cr_loss = 0
    self.q_loss = 0

    self.pl_length_ma = EMA(0.99)
    self.init_folders()

    self.loader = None

    self.loader_evaluate = None

  def init_GAN(self):
    args, kwargs = self.GAN_params
    self.GAN = HistoGAN(lr=self.lr, image_size=self.image_size,
                        network_capacity=self.network_capacity,
                        transparent=self.transparent,
                        fq_layers=self.fq_layers,
                        fq_dict_size=self.fq_dict_size,
                        attn_layers=self.attn_layers, fp16=self.fp16,
                        hist=self.hist_bin, aug=self.aug_prob > 0,
                        *args, **kwargs)

  def write_config(self):
    self.config_path.write_text(json.dumps(self.config()))

  def load_config(self):
    config = self.config() if not self.config_path.exists() else json.loads(
      self.config_path.read_text())
    self.image_size = config['image_size']
    self.network_capacity = config['network_capacity']
    self.transparent = config['transparent']
    self.fq_layers = config['fq_layers']
    self.fq_dict_size = config['fq_dict_size']
    self.attn_layers = config.pop('attn_layers', [])
    del self.GAN
    self.init_GAN()

  def config(self):
    return {'image_size': self.image_size,
            'network_capacity': self.network_capacity,
            'transparent': self.transparent, 'fq_layers': self.fq_layers,
            'fq_dict_size': self.fq_dict_size, 'attn_layers': self.attn_layers}

  def set_data_src(self, folder):
    self.dataset = Dataset(folder, image_size=self.image_size,
                           transparent=self.transparent,
                           hist_insz=self.hist_insz, hist_bin=self.hist_bin,
                           hist_method=self.hist_method,
                           hist_resizing=self.hist_resizing,
                           aug_prob=self.dataset_aug_prob)
    self.loader = cycle(data.DataLoader(self.dataset,
                                        num_workers=default(self.num_workers,
                                                            num_cores),
                                        batch_size=self.batch_size,
                                        drop_last=True, shuffle=True,
                                        pin_memory=True))
    self.dataset_evaluate = Dataset(folder, image_size=150,
                                    transparent=self.transparent,
                                    hist_insz=self.hist_insz,
                                    hist_bin=self.hist_bin,
                                    hist_method=self.hist_method,
                                    hist_resizing=self.hist_resizing, test=True)
    self.loader_evaluate = cycle(data.DataLoader(self.dataset_evaluate,
                                                 num_workers=default(
                                                   self.num_workers, num_cores),
                                                 batch_size=4,
                                                 drop_last=True, shuffle=True,
                                                 pin_memory=True))

  def train(self, alpha=2):
    assert self.loader is not None, ('You must first initialize the data '
                                     'source with `. set_data_src(<folder of '
                                     'images>)`')

    torch.autograd.set_detect_anomaly(False)

    if self.GAN is None:
      self.init_GAN()
    self.GAN.train()
    total_disc_loss = torch.tensor(0.0).cuda()
    total_gen_loss = torch.tensor(0.0).cuda()
    total_hist_loss = torch.tensor(0.0).cuda()

    batch_size = self.batch_size

    image_size = self.GAN.G.image_size
    latent_dim = self.GAN.G.latent_dim
    num_layers = self.GAN.G.num_layers

    aug_prob = self.aug_prob

    if aug_prob > 0.0:
      Disc = self.GAN.D_aug
      aug_types = self.aug_types
      aug_kwargs = {'prob': aug_prob, 'types': aug_types}
    else:
      Disc = self.GAN.D

    apply_gradient_penalty = self.steps % 4 == 0
    apply_path_penalty = self.steps % 32 == 0

    backwards = partial(loss_backwards, self.fp16)

    # train discriminator
    avg_pl_length = self.pl_mean
    self.GAN.D_opt.zero_grad()

    for i in range(self.gradient_accumulate_every):
      get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list
      style = get_latents_fn(batch_size, num_layers - 2, latent_dim)
      noise = image_noise(batch_size, image_size)
      batch = next(self.loader)
      image_batch = batch['images'].cuda()
      image_batch.requires_grad_()
      hist_batch = batch['histograms'].cuda()
      w_space = latent_to_w(self.GAN.S, style)
      h_w_space = self.GAN.H(hist_batch)
      h_w_space = torch.unsqueeze(h_w_space, dim=1)
      h_w_space = torch.cat((h_w_space, h_w_space), dim=1)
      w_styles = styles_def_to_tensor(w_space)
      generated_images = self.GAN.G(w_styles, h_w_space, noise)
      if aug_prob > 0.0:
        fake_output, fake_q_loss = Disc(generated_images.clone().detach(),
                                        detach=True, **aug_kwargs)
        real_output, real_q_loss = Disc(image_batch, **aug_kwargs)
      else:
        fake_output, fake_q_loss = Disc(generated_images.clone().detach())
        real_output, real_q_loss = Disc(image_batch)

      divergence = (F.relu(1 + real_output) + F.relu(1 - fake_output)).mean()
      disc_loss = divergence
      quantize_loss = (fake_q_loss + real_q_loss).mean()
      self.q_loss = float(quantize_loss.detach().item())
      disc_loss = disc_loss + quantize_loss

      if apply_gradient_penalty:
        gp = gradient_penalty(image_batch, real_output)
        self.last_gp_loss = gp.clone().detach().item()
        disc_loss = disc_loss + gp

      disc_loss = disc_loss / self.gradient_accumulate_every
      disc_loss.register_hook(raise_if_nan)
      backwards(disc_loss, self.GAN.D_opt)

      total_disc_loss += (divergence.detach().item() /
                          self.gradient_accumulate_every)

    self.d_loss = float(total_disc_loss)
    self.GAN.D_opt.step()

    # train generator
    self.GAN.G_opt.zero_grad()
    for i in range(self.gradient_accumulate_every):
      style = get_latents_fn(batch_size, num_layers - 2, latent_dim)
      noise = image_noise(batch_size, image_size)
      batch = next(self.loader)
      hist_batch = batch['histograms'].cuda()
      hist_batch.requires_grad_()

      h_w_space = self.GAN.H(hist_batch)
      h_w_space = torch.unsqueeze(h_w_space, dim=1)
      h_w_space = torch.cat((h_w_space, h_w_space), dim=1)
      w_space = latent_to_w(self.GAN.S, style)
      w_styles = styles_def_to_tensor(w_space)

      generated_images = self.GAN.G(w_styles, h_w_space, noise)
      if aug_prob > 0.0:
        fake_output, _ = Disc(generated_images, **aug_kwargs)
      else:
        fake_output, _ = Disc(generated_images)

      generated_histograms = self.histBlock(F.relu(generated_images))

      histogram_loss = alpha * SCALE * (torch.sqrt(
        torch.sum(
          torch.pow(torch.sqrt(hist_batch) - torch.sqrt(generated_histograms),
                    2)))) / hist_batch.shape[0]

      loss = fake_output.mean()
      gen_loss = loss + histogram_loss

      if apply_path_penalty:
        std = 0.1 / (w_styles.std(dim=0, keepdim=True) + EPS)
        w_styles_2 = w_styles + torch.randn(w_styles.shape).cuda() / (std + EPS)
        pl_images = self.GAN.G(w_styles_2, h_w_space, noise)
        pl_lengths = ((pl_images - generated_images) ** 2).mean(dim=(1, 2, 3))
        avg_pl_length = np.mean(pl_lengths.detach().cpu().numpy())

        if not is_empty(self.pl_mean):
          pl_loss = ((pl_lengths - self.pl_mean) ** 2).mean()
          if not torch.isnan(pl_loss):
            gen_loss = gen_loss + pl_loss

      gen_loss = gen_loss / self.gradient_accumulate_every
      gen_loss.register_hook(raise_if_nan)
      backwards(gen_loss, self.GAN.G_opt)

      total_gen_loss += loss.detach().item() / self.gradient_accumulate_every

      total_hist_loss += (histogram_loss.detach().item() /
                          self.gradient_accumulate_every)

    self.g_loss = float(total_gen_loss)

    self.h_loss = float(total_hist_loss)
    self.GAN.G_opt.step()

    # calculate moving averages
    if apply_path_penalty and not np.isnan(avg_pl_length):
      self.pl_mean = self.pl_length_ma.update_average(self.pl_mean,
                                                      avg_pl_length)

    if self.steps % 10 == 0 and self.steps > 20000:
      self.GAN.EMA()

    if self.steps <= 25000 and self.steps % 1000 == 2:
      self.GAN.reset_parameter_averaging()

    # save from NaN errors
    checkpoint_num = floor(self.steps / self.save_every)

    if any(torch.isnan(l) for l in (total_gen_loss, total_disc_loss)):
      print(
        f'NaN detected for generator or discriminator. Loading from '
        f'checkpoint #{checkpoint_num}')
      self.load(checkpoint_num)
      raise NanException

    # periodically save results
    if self.steps % self.save_every == 0:
      self.save(checkpoint_num)

    if self.steps % 1000 == 0 or (self.steps % 100 == 0 and self.steps < 2500):
      self.evaluate(floor(self.steps / 1000))

    self.steps += 1
    self.av = None

  @torch.no_grad()
  def evaluate(self, num=0, hist_batch=None, num_image_tiles=4,
               latents=None, n=None, save_noise_latent=False,
               load_noise_file=None, load_latent_file=None):
    self.GAN.eval()

    if hist_batch is None:
      batch = next(self.loader_evaluate)
      hist_batch = batch['histograms'].cuda()

    ext = 'jpg' if not self.transparent else 'png'
    num_rows = num_image_tiles

    if latents is None and n is None:
      latent_dim = self.GAN.G.latent_dim
      image_size = self.GAN.G.image_size
      num_layers = self.GAN.G.num_layers

      # latents and noise
      if load_noise_file is not None:
        n = torch.tensor(np.load(load_noise_file)).cuda()
      else:
        n = image_noise(num_rows ** 2, image_size)
      if load_latent_file is not None:
        latents = np.load(load_latent_file)
      else:
        latents = noise_list(num_rows ** 2, num_layers - 2, latent_dim)

    generated_images = self.generate_truncated(
      self.GAN.SE, self.GAN.HE, self.GAN.GE, hist_batch, latents, n,
      trunc_psi=self.trunc_psi)
    if num is not None:
      torchvision.utils.save_image(generated_images,
                                   str(self.results_dir / self.name /
                                       f'{str(num)}-ema.{ext}'),
                                   nrow=num_rows)
    if save_noise_latent:
      np.save(f'temp/{self.name}/{str(num)}-noise.npy', n.clone().cpu().numpy())
      np.save(f'temp/{self.name}/{str(num)}-latents.npy', latents)

    return generated_images

  @torch.no_grad()
  def generate_truncated(self, S, H, G, hist_batch, style, noi, trunc_psi=0.75):
    latent_dim = G.latent_dim

    if self.av is None:
      z = noise(2000, latent_dim)
      samples = evaluate_in_chunks(self.batch_size, S, z).cpu().numpy()
      self.av = np.mean(samples, axis=0)
      self.av = np.expand_dims(self.av, axis=0)

    w_space = []
    for tensor, num_layers in style:
      tmp = S(tensor)
      av_torch = torch.from_numpy(self.av).cuda()
      tmp = trunc_psi * (tmp - av_torch) + av_torch
      w_space.append((tmp, num_layers))

    h_w_space = H(hist_batch)
    h_w_space = torch.unsqueeze(h_w_space, dim=1)
    h_w_space = torch.cat((h_w_space, h_w_space), dim=1)

    for i in range(int(np.log2(np.sqrt(w_space[0][0].shape[0])))):
      h_w_space = torch.cat((h_w_space, h_w_space), dim=0)

    w_styles = styles_def_to_tensor(w_space)
    generated_images = evaluate_in_chunks(self.batch_size, G, w_styles,
                                          h_w_space, noi)
    return generated_images.clamp_(0.0, 1.0)

  def print_log(self):
    if hasattr(self, 'h_loss'):
      print(
        f'\nG: {self.g_loss:.2f} | H: {self.h_loss:.2f} | D: '
        f'{self.d_loss:.2f} | GP: {self.last_gp_loss:.2f}'
        f' | PL: {self.pl_mean:.2f} | CR: {self.last_cr_loss:.2f} | Q: '
        f'{self.q_loss:.2f}')
    else:
      print(
        f'\nG: {self.g_loss:.2f} | D: {self.d_loss:.2f} | GP: '
        f'{self.last_gp_loss:.2f}'
        f' | PL: {self.pl_mean:.2f} | CR: {self.last_cr_loss:.2f} | Q: '
        f'{self.q_loss:.2f}')

  def model_name(self, num):
    return str(self.models_dir / self.name / f'.pt')

  def init_folders(self):
    (self.results_dir / self.name).mkdir(parents=True, exist_ok=True)
    (self.models_dir / self.name).mkdir(parents=True, exist_ok=True)

  def clear(self):
    rmtree(f'./models/{self.name}', True)
    rmtree(f'./results/{self.name}', True)
    rmtree(str(self.config_path), True)
    self.init_folders()

  def save(self, num):
    torch.save(self.GAN.state_dict(), self.model_name(num))
    self.write_config()

  def load(self, num=-1):
    self.load_config()

    name = num
    # if num == -1:

    #   file_paths = [p for p in Path(self.models_dir / self.name).glob('model_*.pt')]
    #   print(self.models_dir)
    #   print(self.name)
    #   exit()
    #   saved_nums = sorted(map(lambda x: int(x.stem.split('_')[1]), file_paths))
    #   if len(saved_nums) == 0:
    #     return
    #   name = saved_nums[-1]
    #   print(f'continuing from previous epoch - {name}')
    #model_path = str(self.models_dir) + "/" + self.name + ".pt"
    model_path = str(self.models_dir) + "/" + self.name + "/model_1.pt"
    print("FUCK")
    self.steps = name * self.save_every
    self.GAN.load_state_dict(torch.load(model_path, map_location=f'cuda:{torch.cuda.current_device()}'))
    #print(torch.load(model_path, map_location=f'cuda:{torch.cuda.current_device()}').keys())
    exit()
    #self.GAN.load_state_dict(torch.load(self.model_name(name), map_location=f'cuda:{torch.cuda.current_device()}'))

ColorTransferLib/Algorithms/RHG/histogram_classes/LabHistBlock.py

"""
##### Copyright 2021 Mahmoud Afifi.

 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
####
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

EPS = 1e-6

class LabHistBlock(nn.Module):
  def __init__(self, h=64, insz=150, resizing='interpolation',
               method='inverse-quadratic', sigma=0.02, intensity_scale=False,
               hist_boundary=None, device='cuda'):
    """ Computes the Lab (CIELAB) histogram feature of a given image.
    Args:
      h: histogram dimension size (scalar). The default value is 64.
      insz: maximum size of the input image; if it is larger than this size, the
        image will be resized (scalar). Default value is 150 (i.e., 150 x 150
        pixels).
      resizing: resizing method if applicable. Options are: 'interpolation' or
        'sampling'. Default is 'interpolation'.
      method: the method used to count the number of pixels for each bin in the
        histogram feature. Options are: 'thresholding', 'RBF' (radial basis
        function), or 'inverse-quadratic'. Default value is 'inverse-quadratic'.
      sigma: if the method value is 'RBF' or 'inverse-quadratic', then this is
        the sigma parameter of the kernel function. The default value is 0.02.
      intensity_scale: boolean variable to use the intensity scale (I_y in
        Equation 2). Default value is False.
      hist_boundary: a list of histogram boundary values. Default is [0, 1].

    Methods:
      forward: accepts input image and returns its histogram feature. Note that
        unless the method is 'thresholding', this is a differentiable function
        and can be easily integrated with the loss function. As mentioned in the
         paper, the 'inverse-quadratic' was found more stable than 'RBF' in our
         training.
    """
    super(LabHistBlock, self).__init__()
    self.h = h
    self.insz = insz
    self.device = device
    self.resizing = resizing
    self.method = method
    self.intensity_scale = intensity_scale
    if hist_boundary is None:
      hist_boundary = [0, 1]
    hist_boundary.sort()
    self.hist_boundary = hist_boundary
    if self.method == 'thresholding':
      self.eps = (abs(hist_boundary[0]) + abs(hist_boundary[1])) / h
    else:
      self.sigma = sigma

  def forward(self, x):
    x = torch.clamp(x, 0, 1)
    if x.shape[2] > self.insz or x.shape[3] > self.insz:
      if self.resizing == 'interpolation':
        x_sampled = F.interpolate(x, size=(self.insz, self.insz),
                                  mode='bilinear', align_corners=False)
      elif self.resizing == 'sampling':
        inds_1 = torch.LongTensor(
          np.linspace(0, x.shape[2], self.h, endpoint=False)).to(
          device=self.device)
        inds_2 = torch.LongTensor(
          np.linspace(0, x.shape[3], self.h, endpoint=False)).to(
          device=self.device)
        x_sampled = x.index_select(2, inds_1)
        x_sampled = x_sampled.index_select(3, inds_2)
      else:
        raise Exception(
          f'Wrong resizing method. It should be: interpolation or sampling. '
          f'But the given value is {self.resizing}.')
    else:
      x_sampled = x

    L = x_sampled.shape[0]  # size of mini-batch
    if x_sampled.shape[1] > 3:
      x_sampled = x_sampled[:, :3, :, :]
    X = torch.unbind(x_sampled, dim=0)
    hists = torch.zeros((x_sampled.shape[0], 1, self.h, self.h)).to(
      device=self.device)
    for l in range(L):
      I = torch.t(torch.reshape(X[l], (3, -1)))
      if self.intensity_scale:
        Il = torch.unsqueeze(I[:, 0], dim=1)
      else:
        Il = 1

      Ia = torch.unsqueeze(I[:, 1], dim=1)
      Ib = torch.unsqueeze(I[:, 2], dim=1)

      diff_a = abs(Ia - torch.unsqueeze(torch.tensor(np.linspace(
        self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
        dim=0).to(self.device))
      diff_b = abs(Ib - torch.unsqueeze(torch.tensor(np.linspace(
        self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
        dim=0).to(self.device))

      if self.method == 'thresholding':
        diff_a = torch.reshape(diff_a, (-1, self.h)) <= self.eps / 2
        diff_b = torch.reshape(diff_b, (-1, self.h)) <= self.eps / 2
      elif self.method == 'RBF':
        diff_a = torch.pow(torch.reshape(diff_a, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_b = torch.pow(torch.reshape(diff_b, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_a = torch.exp(-diff_a)  # Gaussian
        diff_b = torch.exp(-diff_b)
      elif self.method == 'inverse-quadratic':
        diff_a = torch.pow(torch.reshape(diff_a, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_b = torch.pow(torch.reshape(diff_b, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_a = 1 / (1 + diff_a)  # Inverse quadratic
        diff_b = 1 / (1 + diff_b)

      diff_a = diff_a.type(torch.float32)
      diff_b = diff_b.type(torch.float32)
      a = torch.t(Il * diff_a)

      hists[l, 0, :, :] = torch.mm(a, diff_b)

    # normalization
    hists_normalized = hists / (
        ((hists.sum(dim=1)).sum(dim=1)).sum(dim=1).view(-1, 1, 1, 1) + EPS)

    return hists_normalized

ColorTransferLib/Algorithms/RHG/histogram_classes/init.py

from . import RGBuvHistBlock

ColorTransferLib/Algorithms/RHG/histogram_classes/RGBuvHistBlock.py

"""
##### Copyright 2021 Mahmoud Afifi.

 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
####
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

EPS = 1e-6

class RGBuvHistBlock(nn.Module):
  def __init__(self, h=64, insz=150, resizing='interpolation',
               method='inverse-quadratic', sigma=0.02, intensity_scale=True,
               hist_boundary=None, green_only=False, device='cuda'):
    """ Computes the RGB-uv histogram feature of a given image.
    Args:
      h: histogram dimension size (scalar). The default value is 64.
      insz: maximum size of the input image; if it is larger than this size, the
        image will be resized (scalar). Default value is 150 (i.e., 150 x 150
        pixels).
      resizing: resizing method if applicable. Options are: 'interpolation' or
        'sampling'. Default is 'interpolation'.
      method: the method used to count the number of pixels for each bin in the
        histogram feature. Options are: 'thresholding', 'RBF' (radial basis
        function), or 'inverse-quadratic'. Default value is 'inverse-quadratic'.
      sigma: if the method value is 'RBF' or 'inverse-quadratic', then this is
        the sigma parameter of the kernel function. The default value is 0.02.
      intensity_scale: boolean variable to use the intensity scale (I_y in
        Equation 2). Default value is True.
      hist_boundary: a list of histogram boundary values. Default is [-3, 3].
      green_only: boolean variable to use only the log(g/r), log(g/b) channels.
        Default is False.

    Methods:
      forward: accepts input image and returns its histogram feature. Note that
        unless the method is 'thresholding', this is a differentiable function
        and can be easily integrated with the loss function. As mentioned in the
         paper, the 'inverse-quadratic' was found more stable than 'RBF' in our
         training.
    """
    super(RGBuvHistBlock, self).__init__()
    self.h = h
    self.insz = insz
    self.device = device
    self.resizing = resizing
    self.method = method
    self.intensity_scale = intensity_scale
    self.green_only = green_only
    if hist_boundary is None:
      hist_boundary = [-3, 3]
    hist_boundary.sort()
    self.hist_boundary = hist_boundary
    if self.method == 'thresholding':
      self.eps = (abs(hist_boundary[0]) + abs(hist_boundary[1])) / h
    else:
      self.sigma = sigma

  def forward(self, x):
    x = torch.clamp(x, 0, 1)
    if x.shape[2] > self.insz or x.shape[3] > self.insz:
      if self.resizing == 'interpolation':
        x_sampled = F.interpolate(x, size=(self.insz, self.insz),
                                  mode='bilinear', align_corners=False)
      elif self.resizing == 'sampling':
        inds_1 = torch.LongTensor(
          np.linspace(0, x.shape[2], self.h, endpoint=False)).to(
          device=self.device)
        inds_2 = torch.LongTensor(
          np.linspace(0, x.shape[3], self.h, endpoint=False)).to(
          device=self.device)
        x_sampled = x.index_select(2, inds_1)
        x_sampled = x_sampled.index_select(3, inds_2)
      else:
        raise Exception(
          f'Wrong resizing method. It should be: interpolation or sampling. '
          f'But the given value is {self.resizing}.')
    else:
      x_sampled = x

    L = x_sampled.shape[0]  # size of mini-batch
    if x_sampled.shape[1] > 3:
      x_sampled = x_sampled[:, :3, :, :]
    X = torch.unbind(x_sampled, dim=0)
    hists = torch.zeros((x_sampled.shape[0], 1 + int(not self.green_only) * 2,
                         self.h, self.h)).to(device=self.device)
    for l in range(L):
      I = torch.t(torch.reshape(X[l], (3, -1)))
      II = torch.pow(I, 2)
      if self.intensity_scale:
        Iy = torch.unsqueeze(torch.sqrt(II[:, 0] + II[:, 1] + II[:, 2] + EPS),
                             dim=1)
      else:
        Iy = 1
      if not self.green_only:
        Iu0 = torch.unsqueeze(torch.log(I[:, 0] + EPS) - torch.log(I[:, 1] +
                                                                   EPS), dim=1)
        Iv0 = torch.unsqueeze(torch.log(I[:, 0] + EPS) - torch.log(I[:, 2] +
                                                                   EPS), dim=1)
        diff_u0 = abs(
          Iu0 - torch.unsqueeze(torch.tensor(np.linspace(
            self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
            dim=0).to(self.device))
        diff_v0 = abs(
          Iv0 - torch.unsqueeze(torch.tensor(np.linspace(
            self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
            dim=0).to(self.device))
        if self.method == 'thresholding':
          diff_u0 = torch.reshape(diff_u0, (-1, self.h)) <= self.eps / 2
          diff_v0 = torch.reshape(diff_v0, (-1, self.h)) <= self.eps / 2
        elif self.method == 'RBF':
          diff_u0 = torch.pow(torch.reshape(diff_u0, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_v0 = torch.pow(torch.reshape(diff_v0, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_u0 = torch.exp(-diff_u0)  # Radial basis function
          diff_v0 = torch.exp(-diff_v0)
        elif self.method == 'inverse-quadratic':
          diff_u0 = torch.pow(torch.reshape(diff_u0, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_v0 = torch.pow(torch.reshape(diff_v0, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_u0 = 1 / (1 + diff_u0)  # Inverse quadratic
          diff_v0 = 1 / (1 + diff_v0)
        else:
          raise Exception(
            f'Wrong kernel method. It should be either thresholding, RBF,'
            f' inverse-quadratic. But the given value is {self.method}.')
        diff_u0 = diff_u0.type(torch.float32)
        diff_v0 = diff_v0.type(torch.float32)
        a = torch.t(Iy * diff_u0)
        hists[l, 0, :, :] = torch.mm(a, diff_v0)

      Iu1 = torch.unsqueeze(torch.log(I[:, 1] + EPS) - torch.log(I[:, 0] + EPS),
                            dim=1)
      Iv1 = torch.unsqueeze(torch.log(I[:, 1] + EPS) - torch.log(I[:, 2] + EPS),
                            dim=1)
      diff_u1 = abs(
        Iu1 - torch.unsqueeze(torch.tensor(np.linspace(
          self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
          dim=0).to(self.device))
      diff_v1 = abs(
        Iv1 - torch.unsqueeze(torch.tensor(np.linspace(
          self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
          dim=0).to(self.device))

      if self.method == 'thresholding':
        diff_u1 = torch.reshape(diff_u1, (-1, self.h)) <= self.eps / 2
        diff_v1 = torch.reshape(diff_v1, (-1, self.h)) <= self.eps / 2
      elif self.method == 'RBF':
        diff_u1 = torch.pow(torch.reshape(diff_u1, (-1, self.h)),
                            2) / self.sigma ** 2
        diff_v1 = torch.pow(torch.reshape(diff_v1, (-1, self.h)),
                            2) / self.sigma ** 2
        diff_u1 = torch.exp(-diff_u1)  # Gaussian
        diff_v1 = torch.exp(-diff_v1)
      elif self.method == 'inverse-quadratic':
        diff_u1 = torch.pow(torch.reshape(diff_u1, (-1, self.h)),
                            2) / self.sigma ** 2
        diff_v1 = torch.pow(torch.reshape(diff_v1, (-1, self.h)),
                            2) / self.sigma ** 2
        diff_u1 = 1 / (1 + diff_u1)  # Inverse quadratic
        diff_v1 = 1 / (1 + diff_v1)

      diff_u1 = diff_u1.type(torch.float32)
      diff_v1 = diff_v1.type(torch.float32)
      a = torch.t(Iy * diff_u1)
      if not self.green_only:
        hists[l, 1, :, :] = torch.mm(a, diff_v1)
      else:
        hists[l, 0, :, :] = torch.mm(a, diff_v1)

      if not self.green_only:
        Iu2 = torch.unsqueeze(torch.log(I[:, 2] + EPS) - torch.log(I[:, 0] +
                                                                   EPS), dim=1)
        Iv2 = torch.unsqueeze(torch.log(I[:, 2] + EPS) - torch.log(I[:, 1] +
                                                                   EPS), dim=1)
        diff_u2 = abs(
          Iu2 - torch.unsqueeze(torch.tensor(np.linspace(
            self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
            dim=0).to(self.device))
        diff_v2 = abs(
          Iv2 - torch.unsqueeze(torch.tensor(np.linspace(
            self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
            dim=0).to(self.device))
        if self.method == 'thresholding':
          diff_u2 = torch.reshape(diff_u2, (-1, self.h)) <= self.eps / 2
          diff_v2 = torch.reshape(diff_v2, (-1, self.h)) <= self.eps / 2
        elif self.method == 'RBF':
          diff_u2 = torch.pow(torch.reshape(diff_u2, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_v2 = torch.pow(torch.reshape(diff_v2, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_u2 = torch.exp(-diff_u2)  # Gaussian
          diff_v2 = torch.exp(-diff_v2)
        elif self.method == 'inverse-quadratic':
          diff_u2 = torch.pow(torch.reshape(diff_u2, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_v2 = torch.pow(torch.reshape(diff_v2, (-1, self.h)),
                              2) / self.sigma ** 2
          diff_u2 = 1 / (1 + diff_u2)  # Inverse quadratic
          diff_v2 = 1 / (1 + diff_v2)
        diff_u2 = diff_u2.type(torch.float32)
        diff_v2 = diff_v2.type(torch.float32)
        a = torch.t(Iy * diff_u2)
        hists[l, 2, :, :] = torch.mm(a, diff_v2)

    # normalization
    hists_normalized = hists / (
        ((hists.sum(dim=1)).sum(dim=1)).sum(dim=1).view(-1, 1, 1, 1) + EPS)

    return hists_normalized

ColorTransferLib/Algorithms/RHG/histoGAN.py

"""
 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
"""

from tqdm import tqdm
from histoGAN import Trainer, NanException
from histogram_classes.RGBuvHistBlock import RGBuvHistBlock
from datetime import datetime
import torch
import argparse
from retry.api import retry_call
import os
from PIL import Image
from torchvision import transforms
import numpy as np

SCALE = 1 / np.sqrt(2.0)

def train_from_folder(
    data='./dataset/',
    results_dir='./results',
    models_dir='./models',
    name='test',
    new=False,
    load_from=-1,
    image_size=128,
    network_capacity=16,
    transparent=False,
    batch_size=2,
    gradient_accumulate_every=8,
    num_train_steps=150000,
    learning_rate=2e-4,
    num_workers=None,
    save_every=1000,
    generate=False,
    save_noise_latent=False,
    target_noise_file=None,
    target_latent_file=None,
    num_image_tiles=8,
    trunc_psi=0.75,
    fp16=False,
    fq_layers=[],
    fq_dict_size=256,
    attn_layers=[],
    hist_method='inverse-quadratic',
    hist_resizing='sampling',
    hist_sigma=0.02,
    hist_bin=64,
    hist_insz=150,
    alpha=2,
    target_hist=None,
    aug_prob=0.0,
    dataset_aug_prob=0.0,
    aug_types=None):

  model = Trainer(
    name,
    results_dir,
    models_dir,
    batch_size=batch_size,
    gradient_accumulate_every=gradient_accumulate_every,
    image_size=image_size,
    network_capacity=network_capacity,
    transparent=transparent,
    lr=learning_rate,
    num_workers=num_workers,
    save_every=save_every,
    trunc_psi=trunc_psi,
    fp16=fp16,
    fq_layers=fq_layers,
    fq_dict_size=fq_dict_size,
    attn_layers=attn_layers,
    hist_insz=hist_insz,
    hist_bin=hist_bin,
    hist_sigma=hist_sigma,
    hist_resizing=hist_resizing,
    hist_method=hist_method,
    aug_prob=aug_prob,
    dataset_aug_prob=dataset_aug_prob,
    aug_types=aug_types
  )

  if not new:
    model.load(load_from)
  else:
    model.clear()

  if generate:

    now = datetime.now()
    timestamp = now.strftime("%m-%d-%Y_%H-%M-%S")
    if save_noise_latent and not os.path.exists('temp'):
      os.mkdir('./temp')
    if save_noise_latent and not os.path.exists(f'./temp/{name}'):
      os.mkdir(f'./temp/{name}')
    if target_hist is None:
      raise Exception('No target histogram or image is given')
    extension = os.path.splitext(target_hist)[1]
    if extension == '.npy':
      hist = np.load(target_hist)
      h = torch.from_numpy(hist).to(device=torch.cuda.current_device())
      if num_image_tiles > 1:
        num_image_tiles = num_image_tiles - num_image_tiles % 2
        for i in range(int(np.log2(num_image_tiles))):
          h = torch.cat((h, h), dim=0)
      samples_name = ('generated-' +
                      f'{os.path.basename(os.path.splitext(target_hist)[0])}'
                      f'-{timestamp}')
      model.evaluate(samples_name, hist_batch=h,
                     num_image_tiles=num_image_tiles,
                     save_noise_latent=save_noise_latent,
                     load_noise_file=target_noise_file,
                     load_latent_file=target_latent_file)
      print(f'sample images generated at {results_dir}/{name}/{samples_name}')
    elif str.lower(extension) == '.jpg' or str.lower(extension) == '.png':
      histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                 resizing=hist_resizing, method=hist_method,
                                 sigma=hist_sigma,
                                 device=torch.cuda.current_device())
      transform = transforms.Compose([transforms.ToTensor()])
      img = Image.open(target_hist)
      img = torch.unsqueeze(transform(img), dim=0).to(
        device=torch.cuda.current_device())
      h = histblock(img)
      if num_image_tiles > 1:
        num_image_tiles = num_image_tiles - num_image_tiles % 2
        for i in range(int(np.log2(num_image_tiles))):
          h = torch.cat((h, h), dim=0)
      samples_name = ('generated-' +
                      f'{os.path.basename(os.path.splitext(target_hist)[0])}'
                      f'-{timestamp}')
      model.evaluate(samples_name, hist_batch=h,
                     num_image_tiles=num_image_tiles,
                     save_noise_latent=save_noise_latent,
                     load_noise_file=target_noise_file,
                     load_latent_file=target_latent_file)
      print(f'sample images generated at {results_dir}/{name}/{samples_name}')
    elif extension == '':
      files = [os.path.join(target_hist, f) for f in os.listdir(target_hist) if
               os.path.isfile(os.path.join(target_hist, f))]
      histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                 resizing=hist_resizing, method=hist_method,
                                 sigma=hist_sigma,
                                 device=torch.cuda.current_device())
      transform = transforms.Compose([transforms.ToTensor()])
      for f in files:
        extension = os.path.splitext(f)[1]
        if extension == '.npy':
          hist = np.load(f)
          h = torch.from_numpy(hist).to(device=torch.cuda.current_device())
        elif (extension == str.lower(extension) == '.jpg' or str.lower(
            extension) == '.png'):
          img = Image.open(f)
          img = torch.unsqueeze(transform(img), dim=0).to(
            device=torch.cuda.current_device())
          h = histblock(img)
        else:
          print(f'Warning: File extension of {f} is not supported.')
          continue
        if num_image_tiles > 1:
          num_image_tiles = num_image_tiles - num_image_tiles % 2
          for i in range(int(np.log2(num_image_tiles))):
            h = torch.cat((h, h), dim=0)
        samples_name = ('generated-' +
                        f'{os.path.basename(os.path.splitext(f)[0])}'
                        f'-{timestamp}')
        model.evaluate(samples_name, hist_batch=h,
                       num_image_tiles=num_image_tiles,
                       save_noise_latent=save_noise_latent,
                       load_noise_file=target_noise_file,
                       load_latent_file=target_latent_file)
        print(f'sample images generated at {results_dir}/{name}/'
              f'{samples_name}')
    else:
      print('The file extension of target image is not supported.')
      raise NotImplementedError
    return

  print('\nStart training....\n')
  print(f'Alpha = {alpha}')
  model.set_data_src(data)
  for _ in tqdm(range(num_train_steps - model.steps), mininterval=10.,
                desc=f'{name}<{data}>'):
    retry_call(model.train, fargs=[alpha], tries=3, exceptions=NanException)

    if _ % 50 == 0:
      model.print_log()

def get_args():
  parser = argparse.ArgumentParser(description='Train/Test HistoGAN.')
  parser.add_argument('--data', dest='data', default='./dataset/')
  parser.add_argument('--results_dir', dest='results_dir',
                      default='./results_HistoGAN')
  parser.add_argument('--models_dir', dest='models_dir', default='./models')
  parser.add_argument('--target_hist', dest='target_hist', default=None)
  parser.add_argument('--name', dest='name', default='histoGAN_model')
  parser.add_argument('--new', dest='new', default=False)
  parser.add_argument('--load_from', dest='load_from', default=-1)
  parser.add_argument('--image_size', dest='image_size', default=256, type=int)
  parser.add_argument('--network_capacity', dest='network_capacity', default=16,
                      type=int)
  parser.add_argument('--transparent', dest='transparent', default=False)
  parser.add_argument('--batch_size', dest='batch_size', default=2, type=int)
  parser.add_argument('--gradient_accumulate_every',
                      dest='gradient_accumulate_every', default=8, type=int)
  parser.add_argument('--num_train_steps', dest='num_train_steps',
                      default=1500000, type=int)
  parser.add_argument('--learning_rate', dest='learning_rate', default=2e-4,
                      type=float)
  parser.add_argument('--num_workers', dest='num_workers', default=None)
  parser.add_argument('--save_every', dest='save_every', default=5000,
                      type=int)
  parser.add_argument('--generate', dest='generate', default=False)
  parser.add_argument('--save_noise_latent', dest='save_n_l', default=False)
  parser.add_argument('--target_noise_file', dest='target_n', default=None)
  parser.add_argument('--target_latent_file', dest='target_l', default=None)
  parser.add_argument('--num_image_tiles', dest='num_image_tiles',
                      default=16, type=int)
  parser.add_argument('--trunc_psi', dest='trunc_psi', default=0.75,
                      type=float)
  parser.add_argument('--fp 16', dest='fp16', default=False)
  parser.add_argument('--fq_layers', dest='fq_layers', default=[])
  parser.add_argument('--fq_dict_size', dest='fq_dict_size', default=256,
                      type=int)
  parser.add_argument('--attn_layers', dest='attn_layers', default=[])
  parser.add_argument('--gpu', dest='gpu', default=0, type=int)
  parser.add_argument('--hist_bin', dest='hist_bin', default=64, type=int)
  parser.add_argument('--hist_insz', dest='hist_insz', default=150, type=int)
  parser.add_argument('--hist_method', dest='hist_method',
                      default='inverse-quadratic')
  parser.add_argument('--hist_resizing', dest='hist_resizing',
                      default='interpolation')
  parser.add_argument('--hist_sigma', dest='hist_sigma', default=0.02,
                      type=float)
  parser.add_argument('--alpha', dest='alpha', default=2, type=float)
  parser.add_argument('--aug_prob', dest='aug_prob', default=0.0, type=float,
                      help='Probability of discriminator augmentation. It '
                           'applies operations specified in --aug_types.')
  parser.add_argument('--dataset_aug_prob', dest='dataset_aug_prob',
                      default=0.0, type=float,
                      help='Probability of dataset augmentation. It applies '
                           'random cropping')
  parser.add_argument('--aug_types', dest='aug_types',
                      default=['translation', 'cutout'], nargs='+',
                      help='Options include: translation, cutout, and color')

  return parser.parse_args()

if __name__ == "__main__":
  args = get_args()
  torch.cuda.set_device(args.gpu)
  train_from_folder(
    data=args.data,
    results_dir=args.results_dir,
    models_dir=args.models_dir,
    name=args.name,
    new=args.new,
    load_from=args.load_from,
    image_size=args.image_size,
    network_capacity=args.network_capacity,
    transparent=args.transparent,
    batch_size=args.batch_size,
    gradient_accumulate_every=args.gradient_accumulate_every,
    num_train_steps=args.num_train_steps,
    learning_rate=args.learning_rate,
    num_workers=args.num_workers,
    save_every=args.save_every,
    generate=args.generate,
    save_noise_latent=args.save_n_l,
    target_noise_file=args.target_n,
    target_latent_file=args.target_l,
    num_image_tiles=args.num_image_tiles,
    trunc_psi=args.trunc_psi,
    fp16=args.fp16,
    fq_layers=args.fq_layers,
    fq_dict_size=args.fq_dict_size,
    attn_layers=args.attn_layers,
    hist_method=args.hist_method,
    hist_resizing=args.hist_resizing,
    hist_sigma=args.hist_sigma,
    hist_bin=args.hist_bin,
    hist_insz=args.hist_insz,
    target_hist=args.target_hist,
    alpha=args.alpha,
    aug_prob=args.aug_prob,
    dataset_aug_prob=args.dataset_aug_prob,
    aug_types=args.aug_types
  )

ColorTransferLib/Algorithms/RHG/histogram_classes/rgChromaHistBlock.py

"""
##### Copyright 2021 Mahmoud Afifi.

 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
####
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

EPS = 1e-6

class rgChromaHistBlock(nn.Module):
  def __init__(self, h=64, insz=150, resizing='interpolation',
               method='inverse-quadratic', sigma=0.02, intensity_scale=False,
               hist_boundary=None, device='cuda'):
    """ Computes the rg-chroma histogram feature of a given image.
    Args:
      h: histogram dimension size (scalar). The default value is 64.
      insz: maximum size of the input image; if it is larger than this size, the
        image will be resized (scalar). Default value is 150 (i.e., 150 x 150
        pixels).
      resizing: resizing method if applicable. Options are: 'interpolation' or
        'sampling'. Default is 'interpolation'.
      method: the method used to count the number of pixels for each bin in the
        histogram feature. Options are: 'thresholding', 'RBF' (radial basis
        function), or 'inverse-quadratic'. Default value is 'inverse-quadratic'.
      sigma: if the method value is 'RBF' or 'inverse-quadratic', then this is
        the sigma parameter of the kernel function. The default value is 0.02.
      intensity_scale: boolean variable to use the intensity scale (I_y in
        Equation 2). Default value is False.
      hist_boundary: a list of histogram boundary values. Default is [0, 1].

    Methods:
      forward: accepts input image and returns its histogram feature. Note that
        unless the method is 'thresholding', this is a differentiable function
        and can be easily integrated with the loss function. As mentioned in the
         paper, the 'inverse-quadratic' was found more stable than 'RBF' in our
         training.
    """
    super(rgChromaHistBlock, self).__init__()
    self.h = h
    self.insz = insz
    self.device = device
    self.resizing = resizing
    self.method = method
    self.intensity_scale = intensity_scale
    if hist_boundary is None:
      hist_boundary = [0, 1]
    hist_boundary.sort()
    self.hist_boundary = hist_boundary
    if self.method == 'thresholding':
      self.eps = (abs(hist_boundary[0]) + abs(hist_boundary[1])) / h
    else:
      self.sigma = sigma

  def forward(self, x):
    x = torch.clamp(x, 0, 1)
    if x.shape[2] > self.insz or x.shape[3] > self.insz:
      if self.resizing == 'interpolation':
        x_sampled = F.interpolate(x, size=(self.insz, self.insz),
                                  mode='bilinear', align_corners=False)
      elif self.resizing == 'sampling':
        inds_1 = torch.LongTensor(
          np.linspace(0, x.shape[2], self.h, endpoint=False)).to(
          device=self.device)
        inds_2 = torch.LongTensor(
          np.linspace(0, x.shape[3], self.h, endpoint=False)).to(
          device=self.device)
        x_sampled = x.index_select(2, inds_1)
        x_sampled = x_sampled.index_select(3, inds_2)
      else:
        raise Exception(
          f'Wrong resizing method. It should be: interpolation or sampling. '
          f'But the given value is {self.resizing}.')
    else:
      x_sampled = x

    L = x_sampled.shape[0]  # size of mini-batch
    if x_sampled.shape[1] > 3:
      x_sampled = x_sampled[:, :3, :, :]
    X = torch.unbind(x_sampled, dim=0)
    hists = torch.zeros((x_sampled.shape[0], 1, self.h, self.h)).to(
      device=self.device)
    for l in range(L):
      I = torch.t(torch.reshape(X[l], (3, -1)))
      II = torch.pow(I, 2)
      if self.intensity_scale:
        Iy = torch.unsqueeze(torch.sqrt(II[:, 0] + II[:, 1] + II[:, 2] + EPS),
                             dim=1)
      else:
        Iy = 1

      Ir = torch.unsqueeze(I[:, 0] / (torch.sum(I, dim=-1) + EPS), dim=1)
      Ig = torch.unsqueeze(I[:, 1] / (torch.sum(I, dim=-1) + EPS), dim=1)

      diff_r = abs(Ir - torch.unsqueeze(torch.tensor(np.linspace(
        self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
        dim=0).to(self.device))
      diff_g = abs(Ig - torch.unsqueeze(torch.tensor(np.linspace(
        self.hist_boundary[0], self.hist_boundary[1], num=self.h)),
        dim=0).to(self.device))

      if self.method == 'thresholding':
        diff_r = torch.reshape(diff_r, (-1, self.h)) <= self.eps / 2
        diff_g = torch.reshape(diff_g, (-1, self.h)) <= self.eps / 2
      elif self.method == 'RBF':
        diff_r = torch.pow(torch.reshape(diff_r, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_g = torch.pow(torch.reshape(diff_g, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_r = torch.exp(-diff_r)  # Gaussian
        diff_g = torch.exp(-diff_g)
      elif self.method == 'inverse-quadratic':
        diff_r = torch.pow(torch.reshape(diff_r, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_g = torch.pow(torch.reshape(diff_g, (-1, self.h)),
                           2) / self.sigma ** 2
        diff_r = 1 / (1 + diff_r)  # Inverse quadratic
        diff_g = 1 / (1 + diff_g)

      diff_r = diff_r.type(torch.float32)
      diff_g = diff_g.type(torch.float32)
      a = torch.t(Iy * diff_r)

      hists[l, 0, :, :] = torch.mm(a, diff_g)

    # normalization
    hists_normalized = hists / (
        ((hists.sum(dim=1)).sum(dim=1)).sum(dim=1).view(-1, 1, 1, 1) + EPS)

    return hists_normalized

ColorTransferLib/Algorithms/RHG/projection_gaussian.py

"""
 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
"""

from histoGAN import Trainer
import torch
import argparse
import torchvision
import os
from datetime import datetime
from PIL import Image
from torchvision import transforms
from torch import optim
from utils.vggloss import VGGPerceptualLoss
import pickle
import numpy as np
import utils.pyramid_upsampling as upsampling
from utils import color_transfer_MKL as ct
from histogram_classes.RGBuvHistBlock import RGBuvHistBlock

mse = torch.nn.MSELoss()

# helpers
def noise(n, latent_dim):
  return torch.randn(n, latent_dim).cuda()

def noise_list(n, layers, latent_dim):
  return [(noise(n, latent_dim), layers)]

def latent_to_w(style_vectorizer, latent_descr):
  return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]

def image_noise(n, im_size):
  return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0., 1.).cuda()

def set_requires_grad(model, bool):
  for p in model.parameters():
    p.requires_grad = bool

def styles_def_to_tensor(styles_def):
  return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def],
                   dim=1)

def noise(n, latent_dim):
  return torch.randn(n, latent_dim).cuda()

def process_image(model, histogram, styles, noise1_list=None,
                  noise2_list=None, in_noise=None):

  rgb = None
  x = model.GAN.GE.initial_block.expand(1, -1, -1, -1)
  if noise1_list is not None and noise2_list is not None:
    for i, (n1, n2, block) in enumerate(zip(noise1_list, noise2_list,
                                            model.GAN.GE.blocks)):
      if i < (len(model.GAN.GE.blocks) - 2):
        style = styles[:, i, :]
        style = model.GAN.SE(style)
        s1 = block.to_style1(style)
        s2 = block.to_style2(style)
        rgb_s = block.to_rgb.to_style(style)
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, noise1=n1, noise2=n2)
      else:
        s1 = block.to_style1(histogram)
        s2 = block.to_style2(histogram)
        rgb_s = block.to_rgb.to_style(histogram)
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, noise1=n1, noise2=n2)
  else:
    for i, block in enumerate(model.GAN.GE.blocks):
      if i < (len(model.GAN.GE.blocks) - 2):
        style = styles[:, i, :]
        style = model.GAN.SE(style)
        s1 = block.to_style1(style)
        s2 = block.to_style2(style)
        rgb_s = block.to_rgb.to_style(style)
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, inoise=in_noise)
      else:
        s1 = block.to_style1(histogram)
        s2 = block.to_style2(histogram)
        rgb_s = block.to_rgb.to_style(histogram)
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, inoise=in_noise)

  return rgb

def recolor_image(model, model_name, target_hist_name, input_image_name,
                  target_hist, latent_noise, optimize_noise, add_noise=False,
                  random_styles=[], results_dir='results_projection_gaussian',
                  post_recoloring=False, upsampling_output=False,
                  upsampling_method='pyramid', swapping_levels=1,
                  pyramid_levels=5, level_blending=False):
  now = datetime.now()
  timestamp = now.strftime("%m-%d-%Y_%H-%M-%S")
  postfix = round(np.random.rand() * 1000)
  filename = os.path.basename(os.path.splitext(input_image_name)[0])

  with open(
      f'{results_dir}/{model_name}/{filename}/{filename}_final.pickle',
      'rb') as handle:
    data = pickle.load(handle)
    styles = data['styles']
    if random_styles:
      assert max(random_styles) <= (model.GAN.GE.num_layers - 2)
      random_styles = list(set(random_styles))
      latent_dim = model.GAN.G.latent_dim
      style = noise_list(1, len(random_styles), latent_dim)
      new_styles = torch.Tensor.repeat(style[0][0], (1, style[0][1], 1)).to(
        device=styles.get_device())
      for j, i in enumerate(random_styles):
        styles[:, i - 1, :] = new_styles[:, j, :]

    if optimize_noise:
      if latent_noise:
        noise1_list = data['noise1_list']
        noise2_list = data['noise2_list']
        in_noise = None
      else:
        noise1_list = None
        noise2_list = None
        in_noise = data['in_noise']
        if add_noise:
          image_size = model.GAN.G.image_size
          shift = image_noise(1, image_size).to(device=in_noise.get_device())
          in_noise = (in_noise + shift) / 2
    else:
      noise1_list = None
      noise2_list = None
      image_size = model.GAN.G.image_size
      in_noise = image_noise(1, image_size)

  h_w_space = model.GAN.HE(target_hist)

  rgb = process_image(model, h_w_space, styles, noise1_list=noise1_list,
                      noise2_list=noise2_list, in_noise=in_noise)

  samples_name = ('generated-' + filename +
                  f'{os.path.basename(os.path.splitext(target_hist_name)[0])}'
                  f'-{timestamp}-{postfix}')

  out_name = f'{results_dir}/{model_name}/{filename}/{samples_name}.jpg'
  torchvision.utils.save_image(rgb, out_name, nrow=1)

  if post_recoloring:
    target = torch.squeeze(rgb, dim=0).cpu().detach().numpy()
    target = target.transpose(1, 2, 0)
    print('Post-recoloring')
    source = np.array(Image.open(input_image_name)) / 255
    result = ct.color_transfer_MKL(source, target)
    result = Image.fromarray(np.uint8(result * 255))
    result.save(out_name)

  if upsampling_output:
    print('Upsampling ...')
    if upsampling_method == 'BGU':
      os.system('BGU.exe '
                f'"{input_image_name}" '
                f'"{out_name}" "{out_name}"')
    elif upsampling_method == 'pyramid':
      reference = Image.open(input_image_name)
      transform = transforms.Compose([transforms.ToTensor()])
      reference = torch.unsqueeze(transform(reference), dim=0).to(
        device=torch.cuda.current_device())
      rgb = upsampling.pyramid_upsampling(rgb, reference,
                                          swapping_levels=swapping_levels,
                                          levels=pyramid_levels,
                                          blending=level_blending)
      torchvision.utils.save_image(rgb, out_name, nrow=1)
    else:
      raise Exception('Unknown upsampling method')

  print(f'sample images generated at {out_name}')

def project_to_latent(
    results_dir='./results_projection_gaussian',
    models_dir='./models',
    name='test',
    load_from=-1,
    image_size=128,
    target_hist=None,
    latent_noise=False,
    optimize_noise=True,
    pixel_loss_weight=1.0,
    vgg_loss_weight=0.005,
    noise_reg_weight=0.0,
    style_reg_weight=0.0,
    add_noise=False,
    random_styles=[],
    generate=False,
    post_recoloring=False,
    upsampling_output=False,
    upsampling_method='pyramid',
    swapping_levels=1,
    pyramid_levels=5,
    level_blending=False,
    network_capacity=16,
    transparent=False,
    num_train_steps=10000,
    learning_rate=2e-4,
    pixel_loss='L1',
    save_every=500,
    trunc_psi=0.75,
    fp16=False,
    fq_layers=[],
    fq_dict_size=256,
    attn_layers=[],
    hist_method='inverse-quadratic',
    hist_resizing='sampling',
    hist_sigma=0.02,
    hist_bin=64,
    hist_insz=150,
    input_image=None,
    aug_prob=0.0):
  model = Trainer(
    name,
    results_dir,
    models_dir,
    image_size=image_size,
    network_capacity=network_capacity,
    transparent=transparent,
    lr=learning_rate,
    save_every=save_every,
    trunc_psi=trunc_psi,
    fp16=fp16,
    fq_layers=fq_layers,
    fq_dict_size=fq_dict_size,
    attn_layers=attn_layers,
    hist_insz=hist_insz,
    hist_bin=hist_bin,
    hist_sigma=hist_sigma,
    hist_resizing=hist_resizing,
    hist_method=hist_method,
    aug_prob=aug_prob,
  )

  model.load(load_from)

  model.GAN.eval()

  histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                             resizing=hist_resizing, method=hist_method,
                             sigma=hist_sigma,
                             device=torch.cuda.current_device())
  transform = transforms.Compose([transforms.ToTensor()])

  if generate:

    if target_hist is None:
      raise Exception('No target histogram is given')

    extension = os.path.splitext(target_hist)[1]
    if extension == '.npy':
      hist = np.load(target_hist)
      h = torch.from_numpy(hist).to(device=torch.cuda.current_device())

      recolor_image(model=model, model_name=name, target_hist_name=target_hist,
                    input_image_name=input_image,
                    results_dir=results_dir,
                    target_hist=h, latent_noise=latent_noise,
                    add_noise=add_noise,
                    random_styles=random_styles,
                    optimize_noise=optimize_noise,
                    post_recoloring=post_recoloring,
                    upsampling_output=upsampling_output,
                    upsampling_method=upsampling_method,
                    swapping_levels=swapping_levels,
                    pyramid_levels=pyramid_levels,
                    level_blending=level_blending)

    elif str.lower(extension) == '.jpg' or str.lower(extension) == '.png':
      histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                 resizing=hist_resizing, method=hist_method,
                                 sigma=hist_sigma,
                                 device=torch.cuda.current_device())
      transform = transforms.Compose([transforms.ToTensor()])
      img = Image.open(target_hist)
      img = torch.unsqueeze(transform(img), dim=0).to(
        device=torch.cuda.current_device())
      h = histblock(img)
      recolor_image(model=model, model_name=name, target_hist_name=target_hist,
                    input_image_name=input_image,
                    results_dir=results_dir,
                    target_hist=h, latent_noise=latent_noise,
                    add_noise=add_noise,
                    random_styles=random_styles,
                    optimize_noise=optimize_noise,
                    post_recoloring=post_recoloring,
                    upsampling_output=upsampling_output,
                    upsampling_method=upsampling_method,
                    swapping_levels=swapping_levels,
                    pyramid_levels=pyramid_levels,
                    level_blending=level_blending)

    elif extension == '':
      files = [os.path.join(target_hist, f) for f in os.listdir(target_hist) if
               os.path.isfile(os.path.join(target_hist, f))]
      histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                 resizing=hist_resizing, method=hist_method,
                                 sigma=hist_sigma,
                                 device=torch.cuda.current_device())
      transform = transforms.Compose([transforms.ToTensor()])
      for f in files:
        extension = os.path.splitext(f)[1]
        if extension == '.npy':
          hist = np.load(f)
          h = torch.from_numpy(hist).to(device=torch.cuda.current_device())
          recolor_image(model=model, model_name=name,
                        target_hist_name=target_hist,
                        results_dir=results_dir,
                        input_image_name=input_image,
                        optimize_noise=optimize_noise, add_noise=add_noise,
                        random_styles=random_styles,
                        target_hist=h, latent_noise=latent_noise,
                        post_recoloring=post_recoloring,
                        upsampling_output=upsampling_output,
                        upsampling_method=upsampling_method,
                        swapping_levels=swapping_levels,
                        pyramid_levels=pyramid_levels,
                        level_blending=level_blending)

        elif (extension == str.lower(extension) == '.jpg' or str.lower(
            extension) == '.png'):
          img = Image.open(f)
          img = torch.unsqueeze(transform(img), dim=0).to(
            device=torch.cuda.current_device())
          h = histblock(img)
          recolor_image(model=model, model_name=name,
                        target_hist_name=target_hist,
                        results_dir=results_dir,
                        input_image_name=input_image,
                        optimize_noise=optimize_noise, add_noise=add_noise,
                        random_styles=random_styles,
                        target_hist=h, latent_noise=latent_noise,
                        post_recoloring=post_recoloring,
                        upsampling_output=upsampling_output,
                        upsampling_method=upsampling_method,
                        swapping_levels=swapping_levels,
                        pyramid_levels=pyramid_levels,
                        level_blending=level_blending)

        else:
          print(f'Warning: File extension of {f} is not supported.')
          continue

    else:
      print('The file extension of target image is not supported.')
      raise NotImplementedError

    return

  ## optimization part

  if vgg_loss_weight > 0:
    vgg_loss = VGGPerceptualLoss(device=torch.cuda.current_device())

  set_requires_grad(model.GAN.SE, False)
  set_requires_grad(model.GAN.HE, False)
  set_requires_grad(model.GAN.GE, False)

  extension = os.path.splitext(input_image)[1]

  if not (str.lower(extension) == '.jpg' or str.lower(extension) == '.png'):
    raise Exception('No target histogram or image is given')

  filename = os.path.basename(os.path.splitext(input_image)[0])

  if not os.path.exists(results_dir):
    os.mkdir(results_dir)
  if not os.path.exists(f'{results_dir}/{name}/{filename}'):
    os.mkdir(f'{results_dir}/{name}/{filename}')

  latent_dim = model.GAN.G.latent_dim
  image_size = model.GAN.G.image_size
  num_layers = model.GAN.G.num_layers

  input_image = Image.open(input_image)
  input_image = input_image.resize((image_size, image_size))
  input_image = torch.unsqueeze(transform(input_image), dim=0).to(
    device=torch.cuda.current_device())
  input_image.requires_grad = False
  in_h = torch.unsqueeze(histblock(input_image), dim=0)
  in_h.requires_grad = False

  get_latents_fn = noise_list
  style = get_latents_fn(1, num_layers - 2, latent_dim)
  styles = torch.Tensor.repeat(style[0][0],
                               (1, style[0][1], 1)).requires_grad_()
  in_noise = image_noise(1, image_size)
  histogram = model.GAN.HE(in_h)
  x = model.GAN.GE.initial_block.expand(1, -1, -1, -1)

  rgb = None
  if optimize_noise and latent_noise:
    noise1_list = []
    noise2_list = []
  for i, block in enumerate(model.GAN.GE.blocks):
    noise = in_noise
    if block.upsample is not None:
      noise = noise[:, :x.shape[2] * 2, :x.shape[3] * 2, :]
    else:
      noise = noise[:, :x.shape[2], :x.shape[3], :]
    if optimize_noise and latent_noise:
      noise1 = block.to_noise1(noise).permute((0, 3, 2, 1))
      noise2 = block.to_noise2(noise).permute((0, 3, 2, 1))
      noise1_list.append(noise1.detach().requires_grad_())
      noise2_list.append(noise2.detach().requires_grad_())

    if i < (len(model.GAN.GE.blocks) - 2):
      style = styles[:, i, :]
      style = model.GAN.SE(style)
      style1 = block.to_style1(style)
      style2 = block.to_style2(style)
      torgb_style = block.to_rgb.to_style(style)
    else:
      style1 = block.to_style1(histogram)
      style2 = block.to_style2(histogram)
      torgb_style = block.to_rgb.to_style(histogram)
    if latent_noise:
      x, rgb = block.forward_(x, rgb, style1, style2, torgb_style,
                              noise1=noise1,
                              noise2=noise2)
    else:
      x, rgb = block.forward_(x, rgb, style1, style2, torgb_style, inoise=noise)

  torchvision.utils.save_image(
    rgb, f'{results_dir}/{name}/{filename}/{filename}_start.jpg', nrow=1)

  if optimize_noise:
    if latent_noise:
      optimizer = optim.Adam([styles] + noise1_list + noise2_list,
                             lr=learning_rate)
    else:
      in_noise = in_noise.detach().requires_grad_()
      optimizer = optim.Adam([styles] + [in_noise], lr=learning_rate)
  else:
    optimizer = optim.Adam([styles], lr=learning_rate)

  if optimize_noise and latent_noise:
    in_noise = None
  else:
    noise1_list = noise2_list = None

  for training_step in range(num_train_steps):

    rgb = process_image(model, histogram, styles, noise1_list=noise1_list,
                        noise2_list=noise2_list, in_noise=in_noise)

    if pixel_loss == 'L1':
      rec_loss = pixel_loss_weight * torch.mean(torch.abs(input_image - rgb))
    elif pixel_loss == 'L2':
      rec_loss = pixel_loss_weight * mse(input_image, rgb)
    if vgg_loss_weight:
      latent_loss = vgg_loss_weight * vgg_loss(input_image, rgb)
      loss = rec_loss + latent_loss
    else:
      loss = rec_loss

    if optimize_noise:
      if latent_noise:
        for i, (noise1, noise2) in enumerate(zip(noise1_list, noise2_list)):
          if i == 0:
            noise_loss = noise_reg_weight * (noise1.mean() ** 2 +
                                             noise2.mean() ** 2)
          else:
            noise_loss += noise_reg_weight * (noise1.mean() ** 2 +
                                              noise2.mean() ** 2)
        noise_loss = noise_loss / (len(noise1_list))
      else:
        noise_loss = noise_reg_weight * in_noise.mean() ** 2
      loss = loss + noise_loss

    else:
      noise_loss = torch.tensor(0)

    style_loss = style_reg_weight * styles.mean() ** 2 / styles.shape[1]

    loss = loss + style_loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if vgg_loss_weight:
      print(f'Optimization step {training_step + 1}, '
            f'rec. loss = {rec_loss.item()}, vgg loss = {latent_loss.item()}, '
            f'rec. noise reg loss = {noise_loss.item()}, style reg loss ='
            f' {style_loss.item()}')

    else:
      print(f'Optimization step {training_step + 1}, rec. loss = '
            f'{rec_loss.item()}, rec. noise reg loss = {noise_loss.item()}, '
            f'style reg loss = {style_loss.item()}')

    if (training_step + 1) % save_every == 0:

      if latent_noise:
        projected_image = process_image(model, histogram, styles,
                                        noise1_list=noise1_list,
                                        noise2_list=noise2_list)
      else:
        projected_image = process_image(model, histogram, styles,
                                        in_noise=in_noise)

      torchvision.utils.save_image(
        projected_image, f'{results_dir}/{name}/{filename}/{filename}_'
                         f'{training_step + 1}.jpg', nrow=1)

      if latent_noise:
        data = {'styles': styles, 'noise1_list':
          noise1_list, 'noise2_list': noise2_list}
      else:
        data = {'styles': styles, 'in_noise': in_noise}

      with open(f'{results_dir}/{name}/{filename}/{filename}_'
                f'{training_step + 1}.pickle', 'wb') as handle:
        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

  if latent_noise:
    data = {'styles': styles, 'noise1_list':
      noise1_list, 'noise2_list': noise2_list}
  else:
    data = {'styles': styles, 'in_noise': in_noise}

  with open(f'{results_dir}/{name}/{filename}/{filename}_final.pickle',
            'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

  print('End of optimization.')

  with open(f'{results_dir}/{name}/{filename}/{filename}_final.pickle',
            'rb') as handle:
    data = pickle.load(handle)
    styles = data['styles']
    if optimize_noise and latent_noise:
      noise1_list = data['noise1_list']
      noise2_list = data['noise2_list']

      projected_image = process_image(model, histogram, styles,
                                      noise1_list=noise1_list,
                                      noise2_list=noise2_list)
    else:
      in_noise = data['in_noise']
      projected_image = process_image(model, histogram, styles,
                                      in_noise=in_noise)

  torchvision.utils.save_image(
    projected_image, f'{results_dir}/{name}/{filename}/'
                     f'{filename}_final.jpg', nrow=1)

def get_args():
  parser = argparse.ArgumentParser(description='Project into HistoGAN latent.')
  parser.add_argument('--results_dir', dest='results_dir',
                      default='./results_projection_gaussian')
  parser.add_argument('--models_dir', dest='models_dir', default='./models')
  parser.add_argument('--input_image', dest='input_image', default=None)
  parser.add_argument('--target_hist', dest='target_hist', default=None)
  parser.add_argument('--generate', dest='generate', default=False, type=bool)
  parser.add_argument('--name', dest='name', default='histoGAN_model')
  parser.add_argument('--image_size', dest='image_size', default=256, type=int)
  parser.add_argument('--network_capacity', dest='network_capacity', default=16,
                      type=int)
  parser.add_argument('--load_from', dest='load_from', default=-1)
  parser.add_argument('--transparent', dest='transparent', default=False)
  parser.add_argument('--num_train_steps', dest='num_train_steps',
                      default=1000, type=int)
  parser.add_argument('--learning_rate', dest='learning_rate', default=1e-1,
                      type=float)
  parser.add_argument('--latent_noise', dest='latent_noise', default=False,
                      type=bool)
  parser.add_argument('--optimize_noise', dest='optimize_noise', default=False,
                      type=bool, help='Use it for noise optimization. In '
                                      'testing, use it to load saved noise '
                                      'even when training without noise '
                                      'optimization.')
  parser.add_argument('--add_noise', dest='add_noise', default=False)
  parser.add_argument('--random_styles', dest='random_styles', nargs='+',
                      default=[], type=int)
  parser.add_argument('--save_every', dest='save_every', default=2, type=int)
  parser.add_argument('--fp 16', dest='fp16', default=False)
  parser.add_argument('--fq_layers', dest='fq_layers', default=[])
  parser.add_argument('--fq_dict_size', dest='fq_dict_size', default=256,
                      type=int)
  parser.add_argument('--pixel_loss', dest='pixel_loss', default='L1',
                      help='L1 or L2')
  parser.add_argument('--vgg_loss_weight', dest='vgg_loss_weight',
                      default=0.001, type=float)
  parser.add_argument('--pixel_loss_weight', dest='pixel_loss_weight',
                      default=1.0, type=float)
  parser.add_argument('--noise_reg_weight', dest='noise_reg_weight',
                      default=0.0, type=float)
  parser.add_argument('--style_reg_weight', dest='style_reg_weight',
                      default=1e1, type=float)
  parser.add_argument('--trunc_psi', dest='trunc_psi', default=0.75,
                      type=float)
  parser.add_argument('--attn_layers', dest='attn_layers', default=[])
  parser.add_argument('--gpu', dest='gpu', default=0, type=int)
  parser.add_argument('--hist_bin', dest='hist_bin', default=64, type=int)
  parser.add_argument('--hist_insz', dest='hist_insz', default=150, type=int)
  parser.add_argument('--hist_method', dest='hist_method',
                      default='inverse-quadratic')
  parser.add_argument('--hist_resizing', dest='hist_resizing',
                      default='interpolation')
  parser.add_argument('--hist_sigma', dest='hist_sigma', default=0.02,
                      type=float)
  parser.add_argument('--upsampling_output', dest='upsampling_output',
                      default=False, type=bool,
                      help='TESTING PHASE: Applies a guided upsampling '
                           'post-processing step.')
  parser.add_argument('--upsampling_method', dest='upsampling_method',
                      default='pyramid', type=str,
                      help='TESTING PHASE: BGU or pyramid.')
  parser.add_argument('--pyramid_levels', dest='pyramid_levels',
                      default=6, type=int,
                      help='TESTING PHASE: when --upsampling_output True and '
                           '--upsampling_method is pyramid, this controls the '
                           'number of levels in the Laplacian pymraid.')
  parser.add_argument('--swapping_levels', dest='swapping_levels',
                      default=1, type=int,
                      help='TESTING PHASE: when --upsampling_output True and '
                           '--upsampling_method is pyramid, this controls the '
                           'number of levels to swap.')
  parser.add_argument('--level_blending', dest='level_blending',
                      default=False, type=bool,
                      help='TESTING PHASE: when --upsampling_output True and '
                           '--upsampling_method is pyramid, this allows to '
                           'blend between pyramid levels.')
  parser.add_argument('--post_recoloring',
                      dest='post_recoloring',
                      default=False, type=bool,
                      help='TESTING PHASE: Applies post-recoloring to '
                           'reduce artifacts. It is recommended if initial '
                           'results have some color bleeding/artifacts.')
  parser.add_argument('--aug_prob', dest='aug_prob', default=0.0, type=float,
                      help='There is no augmentation here, but if the trained '
                           'model originally was trained with aug_prob > 0.0, '
                           'this should be the same here.')

  return parser.parse_args()

if __name__ == "__main__":
  args = get_args()
  torch.cuda.set_device(args.gpu)

  assert args.pixel_loss == 'L1' or args.pixel_loss == 'L2', (
    'pixel loss should be either L1 or L2')
  assert args.vgg_loss_weight >= 0, 'vgg loss weight should be >= 0'

  project_to_latent(
    results_dir=args.results_dir,
    models_dir=args.models_dir,
    name=args.name,
    latent_noise=args.latent_noise,
    optimize_noise=args.optimize_noise,
    pixel_loss_weight=args.pixel_loss_weight,
    noise_reg_weight=args.noise_reg_weight,
    style_reg_weight=args.style_reg_weight,
    add_noise=args.add_noise,
    random_styles=args.random_styles,
    load_from=args.load_from,
    image_size=args.image_size,
    target_hist=args.target_hist,
    generate=args.generate,
    network_capacity=args.network_capacity,
    transparent=args.transparent,
    num_train_steps=args.num_train_steps,
    learning_rate=args.learning_rate,
    save_every=args.save_every,
    fp16=args.fp16,
    post_recoloring=args.post_recoloring,
    upsampling_output=args.upsampling_output,
    upsampling_method=args.upsampling_method,
    swapping_levels=args.swapping_levels,
    pyramid_levels=args.pyramid_levels,
    level_blending=args.level_blending,
    pixel_loss=args.pixel_loss,
    vgg_loss_weight=args.vgg_loss_weight,
    trunc_psi=args.trunc_psi,
    fq_layers=args.fq_layers,
    fq_dict_size=args.fq_dict_size,
    attn_layers=args.attn_layers,
    hist_method=args.hist_method,
    hist_resizing=args.hist_resizing,
    hist_sigma=args.hist_sigma,
    hist_bin=args.hist_bin,
    hist_insz=args.hist_insz,
    input_image=args.input_image,
    aug_prob=args.aug_prob
  )

ColorTransferLib/Algorithms/RHG/upsampling/bguSlice.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Slice (apply) bilateral guided upsampling.
%
% Inputs:
%
% gamma: the affine bilateral grid.
%   size: [height width depth num_output_channels num_input_channels]
% input_image is a double tensor: the (full-resolution) input image
% edge_image is a double matrix: the (full-resolution) edge image
%
function output = bguSlice(gamma, input_image, edge_image)

% Find downsampling coordinates, without rounding.
input_height = size(input_image, 1);
input_width = size(input_image, 2);
grid_height = size(gamma, 1);
grid_width = size(gamma, 2);
grid_depth = size(gamma, 3);
affine_output_size = size(gamma, 4);
affine_input_size = size(gamma, 5);

% meshgrid inputs and outputs are x, then y, with x right, y down.
[ x, y ] = meshgrid(0:(input_width - 1), 0:(input_height - 1));

% Downsample x and y to grid space (leaving them as floats).
bg_coord_x = ((x + 0.5) * (grid_width - 1) / input_width);
bg_coord_y = ((y + 0.5) * (grid_height - 1) / input_height);
bg_coord_z = edge_image * (grid_depth - 1);

% Add 1 to all coordinates for MATLAB.
bg_coord_xx = bg_coord_x + 1;
bg_coord_yy = bg_coord_y + 1;
bg_coord_zz = bg_coord_z + 1;

% interp3 takes xx, yy, zz.
affine_model = {affine_output_size, affine_input_size};
for j = 1:affine_input_size
    for i = 1:affine_output_size        
            affine_model{i,j} = interp3(gamma(:,:,:,i,j), ...
                bg_coord_xx, bg_coord_yy, bg_coord_zz);
    end
end

% Iterate over each row.
% TODO: optimize this.
for i = 1:affine_output_size
    affine_model2{i,1} = cat(4, affine_model{i,:});
end
affine_model3 = cat(3, affine_model2{:,1});

input1 = cat(3, input_image, ones(size(input_image,1), size(input_image,2)));

output = 0;
for i = 1:affine_input_size
    output = output + bsxfun(@times, affine_model3(:,:,:,i), input1(:,:,i));
end

ColorTransferLib/Algorithms/RHG/projection_to_latent.py

"""
 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
"""

from histoGAN import Trainer
import torch
import argparse
import torchvision
import os
from datetime import datetime
from PIL import Image
from torchvision import transforms
from torch import optim
from utils.vggloss import VGGPerceptualLoss
import pickle
import numpy as np
import utils.pyramid_upsampling as upsampling
from utils import color_transfer_MKL as ct
from histogram_classes.RGBuvHistBlock import RGBuvHistBlock
mse = torch.nn.MSELoss()

# helpers
def noise(n, latent_dim):
  return torch.randn(n, latent_dim).cuda()

def noise_list(n, layers, latent_dim):
  return [(noise(n, latent_dim), layers)]

def latent_to_w(style_vectorizer, latent_descr):
  return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]

def image_noise(n, im_size):
  return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0., 1.).cuda()

def set_requires_grad(model, bool):
  for p in model.parameters():
    p.requires_grad = bool

def styles_def_to_tensor(styles_def):
  return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def],
                   dim=1)

def noise(n, latent_dim):
  return torch.randn(n, latent_dim).cuda()

def process_image(model, histogram, style1_list, style2_list, torgb_style_list,
                  noise1_list=None, noise2_list=None, in_noise=None):
  rgb = None
  x = model.GAN.GE.initial_block.expand(1, -1, -1, -1)
  if noise1_list is not None and noise2_list is not None:
    for i, (s1, s2, rgb_s, n1, n2, block) in enumerate(zip(
        style1_list, style2_list, torgb_style_list,
        noise1_list, noise2_list, model.GAN.GE.blocks)):
      if i < (len(model.GAN.GE.blocks) - 2):
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, noise1=n1, noise2=n2)
      else:
        s1 = block.to_style1(histogram)
        s2 = block.to_style2(histogram)
        rgb_s = block.to_rgb.to_style(histogram)
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, noise1=n1, noise2=n2)
  else:
    for i, (s1, s2, rgb_s, block) in enumerate(zip(
        style1_list, style2_list, torgb_style_list, model.GAN.GE.blocks)):
      if i < (len(model.GAN.GE.blocks) - 2):
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, inoise=in_noise)
      else:
        s1 = block.to_style1(histogram)
        s2 = block.to_style2(histogram)
        rgb_s = block.to_rgb.to_style(histogram)
        x, rgb = block.forward_(x, rgb, s1, s2, rgb_s, inoise=in_noise)

  return rgb

def recolor_image(model, model_name, target_hist_name, input_image_name,
                  target_hist, latent_noise, optimize_noise, add_noise=False,
                  random_styles=[], results_dir='results_projection_to_latent',
                  post_recoloring=False, upsampling_output=False,
                  upsampling_method='pyramid', swapping_levels=1,
                  pyramid_levels=5, level_blending=False):

  if random_styles:
    assert max(random_styles) <= (model.GAN.GE.num_layers - 2)
    random_styles = list(set(random_styles))
    new_style1_list = []
    new_style2_list = []
    new_torgb_style_list = []
    get_latents_fn = noise_list
    latent_dim = model.GAN.G.latent_dim
    style = get_latents_fn(1, len(random_styles), latent_dim)
    w_space = latent_to_w(model.GAN.SE, style)
    w_styles = styles_def_to_tensor(w_space)

    styles = w_styles.transpose(0, 1)

    for j, i in enumerate(random_styles):
      style = styles[j, :, :]
      style1 = model.GAN.GE.blocks[i-1].to_style1(style)
      style2 = model.GAN.GE.blocks[i-1].to_style2(style)
      torgb_style = model.GAN.GE.blocks[i-1].to_rgb.to_style(style)
      new_style1_list.append(style1.detach().requires_grad_())
      new_style2_list.append(style2.detach().requires_grad_())
      new_torgb_style_list.append(torgb_style.detach().requires_grad_())

  now = datetime.now()
  timestamp = now.strftime("%m-%d-%Y_%H-%M-%S")
  postfix = round(np.random.rand() * 1000)
  filename = os.path.basename(os.path.splitext(input_image_name)[0])

  with open(
      f'{results_dir}/{model_name}/{filename}/{filename}_final.pickle',
      'rb') as handle:
    data = pickle.load(handle)
    style1_list = data['style1_list']
    style2_list = data['style2_list']
    torgb_style_list = data['torgb_style_list']
    if random_styles:
      for j, i in enumerate(random_styles):
        style1_list[i - 1] = new_style1_list[j]
        style2_list[i - 1] = new_style2_list[j]
        torgb_style_list[i - 1] = new_torgb_style_list[j]

    if optimize_noise:
      if latent_noise:
        noise1_list = data['noise1_list']
        noise2_list = data['noise2_list']
        in_noise = None
      else:
        noise1_list = None
        noise2_list = None
        in_noise = data['in_noise']
        if add_noise:
          image_size = model.GAN.G.image_size
          shift = image_noise(1, image_size).to(device=in_noise.get_device())
          in_noise = (in_noise + shift) / 2
    else:
      noise1_list = None
      noise2_list = None
      image_size = model.GAN.G.image_size
      in_noise = image_noise(1, image_size)

  h_w_space = model.GAN.HE(target_hist)

  rgb = process_image(model, h_w_space, style1_list, style2_list,
                      torgb_style_list, noise1_list=noise1_list,
                      noise2_list=noise2_list, in_noise=in_noise)

  samples_name = ('generated-' + filename +
                  f'{os.path.basename(os.path.splitext(target_hist_name)[0])}'
                  f'-{timestamp}-{postfix}')

  out_name = f'{results_dir}/{model_name}/{filename}/{samples_name}.jpg'
  torchvision.utils.save_image(rgb, out_name, nrow=1)

  if post_recoloring:
    target = torch.squeeze(rgb, dim=0).cpu().detach().numpy()
    target = target.transpose(1, 2, 0)
    print('Post-recoloring')
    source = np.array(Image.open(input_image_name)) / 255
    result = ct.color_transfer_MKL(source, target)
    result = Image.fromarray(np.uint8(result * 255))
    result.save(out_name)

  if upsampling_output:
    print('Upsampling ...')
    if upsampling_method == 'BGU':
      os.system('BGU.exe '
                f'"{input_image_name}" '
                f'"{out_name}" "{out_name}"')
    elif upsampling_method == 'pyramid':
      reference = Image.open(input_image_name)
      transform = transforms.Compose([transforms.ToTensor()])
      reference = torch.unsqueeze(transform(reference), dim=0).to(
        device=torch.cuda.current_device())
      rgb = upsampling.pyramid_upsampling(rgb, reference,
                                          swapping_levels=swapping_levels,
                                          levels=pyramid_levels,
                                          blending=level_blending)
      torchvision.utils.save_image(rgb, out_name, nrow=1)
    else:
      raise Exception('Unknown upsampling method')

  print(f'sample images generated at {out_name}')

def project_to_latent(
    results_dir='./results_projection_to_latent',
    models_dir='./models',
    name='test',
    load_from=-1,
    image_size=128,
    target_hist=None,
    latent_noise=False,
    optimize_noise=True,
    add_noise=False,
    random_styles=[],
    generate=False,
    post_recoloring=False,
    upsampling_output=False,
    upsampling_method='pyramid',
    swapping_levels=1,
    pyramid_levels=5,
    level_blending=False,
    network_capacity=16,
    transparent=False,
    num_train_steps=10000,
    learning_rate=2e-4,
    pixel_loss='L1',
    vgg_loss_weight=0.005,
    pixel_loss_weight=1.0,
    style_reg_weight=0.0,
    noise_reg_weight=0.0,
    save_every=500,
    trunc_psi=0.75,
    fp16=False,
    fq_layers=[],
    fq_dict_size=256,
    attn_layers=[],
    hist_method='inverse-quadratic',
    hist_resizing='sampling',
    hist_sigma=0.02,
    hist_bin=64,
    hist_insz=150,
    input_image=None,
    aug_prob=0.0):
  model = Trainer(
    name,
    results_dir,
    models_dir,
    image_size=image_size,
    network_capacity=network_capacity,
    transparent=transparent,
    lr=learning_rate,
    save_every=save_every,
    trunc_psi=trunc_psi,
    fp16=fp16,
    fq_layers=fq_layers,
    fq_dict_size=fq_dict_size,
    attn_layers=attn_layers,
    hist_insz=hist_insz,
    hist_bin=hist_bin,
    hist_sigma=hist_sigma,
    hist_resizing=hist_resizing,
    hist_method=hist_method,
    aug_prob=aug_prob,
  )

  model.load(load_from)

  model.GAN.eval()

  histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                             resizing=hist_resizing, method=hist_method,
                             sigma=hist_sigma,
                             device=torch.cuda.current_device())
  transform = transforms.Compose([transforms.ToTensor()])

  if generate:

    if target_hist is None:
      raise Exception('No target histogram is given')

    extension = os.path.splitext(target_hist)[1]
    if extension == '.npy':
      hist = np.load(target_hist)
      h = torch.from_numpy(hist).to(device=torch.cuda.current_device())

      recolor_image(model=model, model_name=name, target_hist_name=target_hist,
                    input_image_name=input_image,
                    results_dir=results_dir,
                    target_hist=h, latent_noise=latent_noise,
                    optimize_noise=optimize_noise,
                    add_noise=add_noise, random_styles=random_styles,
                    post_recoloring=post_recoloring,
                    upsampling_output=upsampling_output,
                    upsampling_method=upsampling_method,
                    swapping_levels=swapping_levels,
                    pyramid_levels=pyramid_levels,
                    level_blending=level_blending)

    elif str.lower(extension) == '.jpg' or str.lower(extension) == '.png':
      histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                 resizing=hist_resizing, method=hist_method,
                                 sigma=hist_sigma,
                                 device=torch.cuda.current_device())
      transform = transforms.Compose([transforms.ToTensor()])
      img = Image.open(target_hist)
      img = torch.unsqueeze(transform(img), dim=0).to(
        device=torch.cuda.current_device())
      h = histblock(img)
      recolor_image(model=model, model_name=name, target_hist_name=target_hist,
                    input_image_name=input_image,
                    results_dir=results_dir,
                    target_hist=h, latent_noise=latent_noise,
                    optimize_noise=optimize_noise,
                    add_noise=add_noise, random_styles=random_styles,
                    post_recoloring=post_recoloring,
                    upsampling_output=upsampling_output,
                    upsampling_method=upsampling_method,
                    swapping_levels=swapping_levels,
                    pyramid_levels=pyramid_levels,
                    level_blending=level_blending)

    elif extension == '':
      files = [os.path.join(target_hist, f) for f in os.listdir(target_hist) if
               os.path.isfile(os.path.join(target_hist, f))]
      histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin,
                                 resizing=hist_resizing, method=hist_method,
                                 sigma=hist_sigma,
                                 device=torch.cuda.current_device())
      transform = transforms.Compose([transforms.ToTensor()])
      for f in files:
        extension = os.path.splitext(f)[1]
        if extension == '.npy':
          hist = np.load(f)
          h = torch.from_numpy(hist).to(device=torch.cuda.current_device())
          recolor_image(model=model, model_name=name,
                        target_hist_name=target_hist,
                        results_dir=results_dir,
                        input_image_name=input_image,
                        optimize_noise=optimize_noise,
                        add_noise=add_noise, random_styles=random_styles,
                        target_hist=h, latent_noise=latent_noise,
                        post_recoloring=post_recoloring,
                        upsampling_output=upsampling_output,
                        upsampling_method=upsampling_method,
                        swapping_levels=swapping_levels,
                        pyramid_levels=pyramid_levels,
                        level_blending=level_blending)

        elif (extension == str.lower(extension) == '.jpg' or str.lower(
            extension) == '.png'):
          img = Image.open(f)
          img = torch.unsqueeze(transform(img), dim=0).to(
            device=torch.cuda.current_device())
          h = histblock(img)
          recolor_image(model=model, model_name=name,
                        target_hist_name=target_hist,
                        results_dir=results_dir,
                        input_image_name=input_image,
                        optimize_noise=optimize_noise,
                        add_noise=add_noise, random_styles=random_styles,
                        target_hist=h, latent_noise=latent_noise,
                        post_recoloring=post_recoloring,
                        upsampling_output=upsampling_output,
                        upsampling_method=upsampling_method,
                        swapping_levels=swapping_levels,
                        pyramid_levels=pyramid_levels,
                        level_blending=level_blending)

        else:
          print(f'Warning: File extension of {f} is not supported.')
          continue

    else:
      print('The file extension of target image is not supported.')
      raise NotImplementedError

    return

  ## optimization part

  if vgg_loss_weight > 0:
    vgg_loss = VGGPerceptualLoss(device=torch.cuda.current_device())

  set_requires_grad(model.GAN.SE, False)
  set_requires_grad(model.GAN.HE, False)
  set_requires_grad(model.GAN.GE, False)

  extension = os.path.splitext(input_image)[1]

  if not (str.lower(extension) == '.jpg' or str.lower(extension) == '.png'):
    raise Exception('No target histogram or image is given')

  filename = os.path.basename(os.path.splitext(input_image)[0])

  if not os.path.exists(results_dir):
    os.mkdir(results_dir)
  if not os.path.exists(f'{results_dir}/{name}/{filename}'):
    os.mkdir(f'{results_dir}/{name}/{filename}')

  latent_dim = model.GAN.G.latent_dim
  image_size = model.GAN.G.image_size
  num_layers = model.GAN.G.num_layers

  input_image = Image.open(input_image)
  input_image = input_image.resize((image_size, image_size))
  input_image = torch.unsqueeze(transform(input_image), dim=0).to(
    device=torch.cuda.current_device())
  input_image.requires_grad = False
  in_h = torch.unsqueeze(histblock(input_image), dim=0)
  in_h.requires_grad = False

  get_latents_fn = noise_list
  style = get_latents_fn(1, num_layers - 2, latent_dim)
  in_noise = image_noise(1, image_size)
  w_space = latent_to_w(model.GAN.SE, style)
  histogram = model.GAN.HE(in_h)
  h_w_space = torch.unsqueeze(histogram, dim=1)
  h_w_space = torch.cat((h_w_space, h_w_space), dim=1)
  w_styles = styles_def_to_tensor(w_space)

  x = model.GAN.GE.initial_block.expand(1, -1, -1, -1)
  styles = w_styles.transpose(0, 1)
  hists = h_w_space.transpose(0, 1)
  styles = torch.cat((styles, hists), dim=0)

  rgb = None
  if optimize_noise and latent_noise:
    noise1_list = []
    noise2_list = []
  style1_list = []
  style2_list = []
  torgb_style_list = []
  for i, (style, block) in enumerate(zip(styles, model.GAN.GE.blocks)):
    noise = in_noise
    if block.upsample is not None:
      noise = noise[:, :x.shape[2] * 2, :x.shape[3] * 2, :]
    else:
      noise = noise[:, :x.shape[2], :x.shape[3], :]
    if optimize_noise and latent_noise:
      noise1 = block.to_noise1(noise).permute((0, 3, 2, 1))
      noise2 = block.to_noise2(noise).permute((0, 3, 2, 1))
      noise1_list.append(noise1.detach().requires_grad_())
      noise2_list.append(noise2.detach().requires_grad_())

    if i < (len(model.GAN.GE.blocks) - 2):
      style1 = block.to_style1(style)
      style2 = block.to_style2(style)
      torgb_style = block.to_rgb.to_style(style)
      style1_list.append(style1.detach().requires_grad_())
      style2_list.append(style2.detach().requires_grad_())
      torgb_style_list.append(torgb_style.detach().requires_grad_())
    else:
      style1 = block.to_style1(histogram)
      style2 = block.to_style2(histogram)
      torgb_style = block.to_rgb.to_style(histogram)
      style1_list.append(torch.tensor([]).requires_grad_())
      style2_list.append(torch.tensor([]).requires_grad_())
      torgb_style_list.append(torch.tensor([]).requires_grad_())
    if latent_noise:
      x, rgb = block.forward_(x, rgb, style1, style2, torgb_style,
                              noise1=noise1,
                              noise2=noise2)
    else:
      x, rgb = block.forward_(x, rgb, style1, style2, torgb_style, inoise=noise)

  torchvision.utils.save_image(
    rgb, f'{results_dir}/{name}/{filename}/{filename}_start.jpg', nrow=1)

  if optimize_noise:
    if latent_noise:
      optimizer = optim.Adam(style1_list + style2_list + torgb_style_list +
                             noise1_list + noise2_list, lr=learning_rate)
    else:
      in_noise = in_noise.detach().requires_grad_()
      optimizer = optim.Adam(style1_list + style2_list + torgb_style_list +
                             [in_noise], lr=learning_rate)
  else:
    optimizer = optim.Adam(style1_list + style2_list + torgb_style_list,
                           lr=learning_rate)

  if optimize_noise and latent_noise:
    in_noise = None
  else:
    noise1_list = noise2_list = None

  for training_step in range(num_train_steps):

    rgb = process_image(model, histogram, style1_list, style2_list,
                        torgb_style_list, noise1_list=noise1_list,
                        noise2_list=noise2_list, in_noise=in_noise)

    if pixel_loss == 'L1':
      rec_loss = pixel_loss_weight * torch.mean(torch.abs(input_image - rgb))
    elif pixel_loss == 'L2':
      rec_loss = pixel_loss_weight * mse(input_image, rgb)
    if vgg_loss_weight:
      latent_loss = vgg_loss_weight * vgg_loss(input_image, rgb)
      loss = rec_loss + latent_loss
    else:
      loss = rec_loss
    if optimize_noise:
      if latent_noise:
        for i, (noise1, noise2) in enumerate(zip(noise1_list, noise2_list)):
          if i == 0:
            noise_loss = noise_reg_weight * (noise1.mean() ** 2 +
                                              noise2.mean() ** 2)
          else:
            noise_loss += noise_reg_weight * (noise1.mean() ** 2 +
                                              noise2.mean() ** 2)
        noise_loss = noise_loss / (len(noise1_list))
      else:
        noise_loss = noise_reg_weight * in_noise.mean() ** 2
      loss = loss + noise_loss

    else:
      noise_loss = torch.tensor(0)

    for i, (style1, style2) in enumerate(zip(style1_list, style2_list)):
      if i == 0:
        style_loss = style_reg_weight * (style1.mean() ** 2 + style2.mean()
                                         ** 2)
      elif i < (len(style1_list) - 2):
        style_loss += style_reg_weight * (
              style1.mean() ** 2 + style2.mean() ** 2)
      style_loss = style_loss / (len(style1_list) - 2)

    loss = loss + style_loss

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    if vgg_loss_weight:
      print(f'Optimization step {training_step + 1}, '
            f'rec. loss = {rec_loss.item()}, vgg loss = {latent_loss.item()}, '
            f'rec. noise reg loss = {noise_loss.item()}, style reg loss ='
            f' {style_loss.item()}')

    else:
      print(f'Optimization step {training_step + 1}, rec. loss = '
            f'{rec_loss.item()}, rec. noise reg loss = {noise_loss.item()}, '
            f'style reg loss = {style_loss.item()}')

    if (training_step + 1) % save_every == 0:

      if latent_noise:
        projected_image = process_image(model, histogram, style1_list,
                                        style2_list, torgb_style_list,
                                        noise1_list=noise1_list,
                                        noise2_list=noise2_list)
      else:
        projected_image = process_image(model, histogram, style1_list,
                                        style2_list, torgb_style_list,
                                        in_noise=in_noise)

      torchvision.utils.save_image(
        projected_image, f'{results_dir}/{name}/{filename}/{filename}_'
                         f'{training_step + 1}.jpg', nrow=1)

      if latent_noise:
        data = {'style1_list': style1_list, 'style2_list': style2_list,
                'torgb_style_list': torgb_style_list, 'noise1_list':
                  noise1_list, 'noise2_list': noise2_list}
      else:
        data = {'style1_list': style1_list, 'style2_list': style2_list,
                'torgb_style_list': torgb_style_list, 'in_noise': in_noise}

      with open(f'{results_dir}/{name}/{filename}/{filename}_'
                f'{training_step + 1}.pickle', 'wb') as handle:
        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

  if latent_noise:
    data = {'style1_list': style1_list, 'style2_list': style2_list,
            'torgb_style_list': torgb_style_list, 'noise1_list':
              noise1_list, 'noise2_list': noise2_list}
  else:
    data = {'style1_list': style1_list, 'style2_list': style2_list,
            'torgb_style_list': torgb_style_list, 'in_noise': in_noise}

  with open(f'{results_dir}/{name}/{filename}/{filename}_final.pickle',
            'wb') as handle:
    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)

  print('End of optimization.')

  with open(f'{results_dir}/{name}/{filename}/{filename}_final.pickle',
            'rb') as handle:
    data = pickle.load(handle)
    style1_list = data['style1_list']
    style2_list = data['style2_list']
    torgb_style_list = data['torgb_style_list']
    if optimize_noise and latent_noise:
      noise1_list = data['noise1_list']
      noise2_list = data['noise2_list']

      projected_image = process_image(model, histogram, style1_list,
                                      style2_list, torgb_style_list,
                                      noise1_list=noise1_list,
                                      noise2_list=noise2_list)
    elif optimize_noise:
      in_noise = data['in_noise']
      projected_image = process_image(model, histogram, style1_list,
                                      style2_list, torgb_style_list,
                                      in_noise=in_noise)
    else:
      projected_image = process_image(model, histogram, style1_list,
                                      style2_list, torgb_style_list,
                                      in_noise=in_noise)

  torchvision.utils.save_image(
    projected_image, f'{results_dir}/{name}/{filename}/'
                     f'{filename}_final.jpg', nrow=1)

def get_args():
  parser = argparse.ArgumentParser(description='Project into HistoGAN latent.')
  parser.add_argument('--results_dir', dest='results_dir',
                      default='./results_projection_to_latent')
  parser.add_argument('--models_dir', dest='models_dir', default='./models')
  parser.add_argument('--input_image', dest='input_image', default=None)
  parser.add_argument('--target_hist', dest='target_hist', default=None)
  parser.add_argument('--generate', dest='generate',
                      default=False, type=bool)
  parser.add_argument('--name', dest='name', default='histoGAN_model')
  parser.add_argument('--image_size', dest='image_size', default=256, type=int)
  parser.add_argument('--network_capacity', dest='network_capacity', default=16,
                      type=int)
  parser.add_argument('--load_from', dest='load_from', default=-1)
  parser.add_argument('--transparent', dest='transparent', default=False)
  parser.add_argument('--num_train_steps', dest='num_train_steps',
                      default=2000, type=int)
  parser.add_argument('--learning_rate', dest='learning_rate', default=1e-1,
                      type=float)
  parser.add_argument('--latent_noise', dest='latent_noise', default=False,
                      type=bool)
  parser.add_argument('--optimize_noise', dest='optimize_noise', default=False,
                      type=bool, help='Use it for noise optimization. In '
                                      'testing, use it to load saved noise '
                                      'even when training without noise '
                                      'optimization.')
  parser.add_argument('--add_noise', dest='add_noise', default=False)
  parser.add_argument('--random_styles', dest='random_styles', nargs='+',
                      default=[], type=int)
  parser.add_argument('--save_every', dest='save_every', default=100, type=int)
  parser.add_argument('--fp 16', dest='fp16', default=False)
  parser.add_argument('--fq_layers', dest='fq_layers', default=[])
  parser.add_argument('--fq_dict_size', dest='fq_dict_size', default=256,
                      type=int)
  parser.add_argument('--pixel_loss', dest='pixel_loss', default='L1',
                      help='L1 or L2')
  parser.add_argument('--vgg_loss_weight', dest='vgg_loss_weight',
                      default=0.001, type=float)
  parser.add_argument('--pixel_loss_weight', dest='pixel_loss_weight',
                      default=1.0, type=float)
  parser.add_argument('--noise_reg_weight', dest='noise_reg_weight',
                      default=0.0, type=float)
  parser.add_argument('--style_reg_weight', dest='style_reg_weight',
                      default=0.0, type=float)
  parser.add_argument('--trunc_psi', dest='trunc_psi', default=0.75,
                      type=float)
  parser.add_argument('--attn_layers', dest='attn_layers', default=[])
  parser.add_argument('--gpu', dest='gpu', default=0, type=int)
  parser.add_argument('--hist_bin', dest='hist_bin', default=64, type=int)
  parser.add_argument('--hist_insz', dest='hist_insz', default=150, type=int)
  parser.add_argument('--hist_method', dest='hist_method',
                      default='inverse-quadratic')
  parser.add_argument('--hist_resizing', dest='hist_resizing',
                      default='interpolation')
  parser.add_argument('--hist_sigma', dest='hist_sigma', default=0.02,
                      type=float)
  parser.add_argument('--upsampling_output', dest='upsampling_output',
                      default=False, type=bool,
                      help='TESTING PHASE: Applies a guided upsampling '
                           'post-processing step.')
  parser.add_argument('--upsampling_method', dest='upsampling_method',
                      default='pyramid', type=str,
                      help='TESTING PHASE: BGU or pyramid.')
  parser.add_argument('--pyramid_levels', dest='pyramid_levels',
                      default=6, type=int,
                      help='TESTING PHASE: when --upsampling_output True and '
                           '--upsampling_method is pyramid, this controls the '
                           'number of levels in the Laplacian pymraid.')
  parser.add_argument('--swapping_levels', dest='swapping_levels',
                      default=1, type=int,
                      help='TESTING PHASE: when --upsampling_output True and '
                           '--upsampling_method is pyramid, this controls the '
                           'number of levels to swap.')
  parser.add_argument('--level_blending', dest='level_blending',
                      default=False, type=bool,
                      help='TESTING PHASE: when --upsampling_output True and '
                           '--upsampling_method is pyramid, this allows to '
                           'blend between pyramid levels.')
  parser.add_argument('--post_recoloring',
                      dest='post_recoloring',
                      default=False, type=bool,
                      help='TESTING PHASE: Applies post-recoloring to '
                           'reduce artifacts. It is recommended if initial '
                           'results have some color bleeding/artifacts.')
  parser.add_argument('--aug_prob', dest='aug_prob', default=0.0, type=float,
                      help='There is no augmentation here, but if the trained '
                           'model originally was trained with aug_prob > 0.0, '
                           'this should be the same here.')

  return parser.parse_args()

if __name__ == "__main__":
  args = get_args()
  torch.cuda.set_device(args.gpu)

  assert args.pixel_loss == 'L1' or args.pixel_loss == 'L2', (
    'pixel loss should be either L1 or L2')
  assert args.vgg_loss_weight >= 0, 'vgg loss weight should be >= 0'

  project_to_latent(
    results_dir=args.results_dir,
    models_dir=args.models_dir,
    name=args.name,
    latent_noise=args.latent_noise,
    optimize_noise=args.optimize_noise,
    add_noise=args.add_noise,
    random_styles=args.random_styles,
    load_from=args.load_from,
    image_size=args.image_size,
    target_hist=args.target_hist,
    generate=args.generate,
    network_capacity=args.network_capacity,
    transparent=args.transparent,
    num_train_steps=args.num_train_steps,
    learning_rate=args.learning_rate,
    save_every=args.save_every,
    fp16=args.fp16,
    post_recoloring=args.post_recoloring,
    upsampling_output=args.upsampling_output,
    upsampling_method=args.upsampling_method,
    swapping_levels=args.swapping_levels,
    pyramid_levels=args.pyramid_levels,
    level_blending=args.level_blending,
    pixel_loss=args.pixel_loss,
    vgg_loss_weight=args.vgg_loss_weight,
    pixel_loss_weight=args.pixel_loss_weight,
    style_reg_weight=args.style_reg_weight,
    noise_reg_weight=args.noise_reg_weight,
    trunc_psi=args.trunc_psi,
    fq_layers=args.fq_layers,
    fq_dict_size=args.fq_dict_size,
    attn_layers=args.attn_layers,
    hist_method=args.hist_method,
    hist_resizing=args.hist_resizing,
    hist_sigma=args.hist_sigma,
    hist_bin=args.hist_bin,
    hist_insz=args.hist_insz,
    input_image=args.input_image,
    aug_prob=args.aug_prob
  )

ColorTransferLib/Algorithms/RHG/rehistoGAN.py

"""
 If you find this code useful, please cite our paper:

 Mahmoud Afifi, Marcus A. Brubaker, and Michael S. Brown. "HistoGAN:
 Controlling Colors of GAN-Generated and Real Images via Color Histograms."
 In CVPR, 2021.

 @inproceedings{afifi2021histogan,
  title={Histo{GAN}: Controlling Colors of {GAN}-Generated and Real Images via
  Color Histograms},
  author={Afifi, Mahmoud and Brubaker, Marcus A. and Brown, Michael S.},
  booktitle={CVPR},
  year={2021}
}
"""

from tqdm import tqdm
from .ReHistoGAN.rehistoGAN import recoloringTrainer
from .histoGAN.histoGAN import Trainer, NanException
from datetime import datetime
import torch
import argparse
from retry.api import retry_call
import os
from PIL import Image
from torchvision import transforms
import torchvision
import numpy as np
import copy
from .utils.face_preprocessing import face_extraction
from .histogram_classes.RGBuvHistBlock import RGBuvHistBlock
import cv2
import gc

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def convert_transparent_to_rgb(image):
    if image.mode == 'RGBA':
        return image.convert('RGB')
    return image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class expand_greyscale(object):
    def __init__(self, num_channels):
        self.num_channels = num_channels

    def __call__(self, tensor):
        return tensor.expand(self.num_channels, -1, -1)

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def resize_to_minimum_size(min_size, image):
    if max(*image.size) < min_size:
        return torchvision.transforms.functional.resize(image, min_size)
    return image

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def hist_interpolation(hists):
    if not torch.cuda.is_available():
        device = "cpu"
    else:
        device = "cuda"

    ratios = torch.abs(torch.rand(hists.shape[0])).to(device=device)
    ratios = ratios / torch.sum(ratios)
    out_hist = hists[0, :, :, :, :] * ratios[0]
    for i in range(hists.shape[0] - 1):
        out_hist = out_hist + hists[i + 1, :, :, :, :] * ratios[i + 1]
    return out_hist

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def process_image(model, name, input_image, target_hist, image_size=256,
                  upsampling_output=False,
                  upsampling_method='pyramid',
                  swapping_levels=1,
                  pyramid_levels=5,
                  level_blending=False,
                  post_recoloring=False,
                  sampling=True,
                  target_number=1, results_dir='./results_ReHistoGAN/',
                  hist_insz=150, hist_bin=64,
                  hist_method='inverse-quadratic', hist_resizing='sampling',
                  hist_sigma=0.02):

    if not torch.cuda.is_available():
        device = "cpu"
    else:
        device = "cuda"

    img = input_image# Image.open(input_image)
    original_img = input_image#np.array(img) / 255
    img = Image.fromarray((img*255).astype("uint8"))

    if upsampling_output:
        width, height = img.size
        if width > image_size or height > image_size:
            resizing_mode = 'upscaling'
        elif width < image_size or height < image_size:
            resizing_mode = 'downscaling'
        else:
            resizing_mode = 'none'
    else:
        resizing_mode = None
        width = None
        height = None

    if width != image_size or height != image_size:
        img = img.resize((image_size, image_size))

    now = datetime.now()
    timestamp = now.strftime("%m-%d-%Y_%H-%M-%S")

    postfix = round(np.random.rand() * 1000)
    transform = transforms.Compose([
        transforms.Lambda(convert_transparent_to_rgb),
        transforms.Resize(image_size),
        transforms.ToTensor(),
        transforms.Lambda(expand_greyscale(3))
    ])

    img = torch.unsqueeze(transform(img), dim=0).to(device=device)
    histblock = RGBuvHistBlock(insz=hist_insz, h=hist_bin, resizing=hist_resizing, method=hist_method, sigma=hist_sigma, device=device)
    transform = transforms.Compose([transforms.ToTensor()])

    img_hist = Image.fromarray((target_hist*255).astype("uint8"))#Image.open(target_hist)
    img_hist = torch.unsqueeze(transform(img_hist), dim=0).to(device=device)
    with torch.no_grad():
        h = histblock(img_hist)
        samples_name = "out"#('output-' + f'{os.path.basename(os.path.splitext(target_hist)[0])}' f'-{timestamp}-{postfix}')
        output = model.evaluate(samples_name, image_batch=img,
                        hist_batch=h,
                        resizing=resizing_mode,
                        resizing_method=upsampling_method,
                        swapping_levels=swapping_levels,
                        pyramid_levels=pyramid_levels,
                        level_blending=level_blending,
                        original_size=[width, height],
                        original_image=original_img,
                        input_image_name=input_image,
                        save_input=False,
                        post_recoloring=post_recoloring)

        return output

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def train_from_folder(
        results_dir='./results_ReHistoGAN/',
        models_dir='./Models/HistoGAN',
        name='test',
        new=False,
        load_from=-1,
        image_size=128,
        network_capacity=16,
        transparent=False,
        load_histogan_weights=True,
        batch_size=2,
        sampling=True,
        gradient_accumulate_every=8,
        num_train_steps=200000,
        learning_rate=2e-4,
        num_workers=None,
        save_every=10000,
        generate=False,
        trunc_psi=0.75,
        fp16=False,
        skip_conn_to_GAN=False,
        fq_layers=[],
        fq_dict_size=256,
        attn_layers=[],
        hist_method='inverse-quadratic',
        hist_resizing='sampling',
        hist_sigma=0.02,
        hist_bin=64,
        hist_insz=150,
        rec_loss='laplacian',
        alpha=32,
        beta=1.5,
        gamma=4,
        fixed_gan_weights=False,
        initialize_gan=False,
        variance_loss=False,
        target_hist=None,
        internal_hist=False,
        histoGAN_model_name=None,
        input_image=None,
        target_number=None,
        change_hyperparameters=False,
        change_hyperparameters_after=100000,
        upsampling_output=False,
        upsampling_method='pyramid',
        swapping_levels=1,
        pyramid_levels=6,
        level_blending=False,
        post_recoloring=False):

    model = recoloringTrainer(
        name,
        results_dir,
        models_dir,
        batch_size=batch_size,
        gradient_accumulate_every=gradient_accumulate_every,
        image_size=image_size,
        network_capacity=network_capacity,
        transparent=transparent,
        lr=learning_rate,
        num_workers=num_workers,
        save_every=save_every,
        trunc_psi=trunc_psi,
        fp16=fp16,
        fq_layers=fq_layers,
        fq_dict_size=fq_dict_size,
        attn_layers=attn_layers,
        hist_insz=hist_insz,
        hist_bin=hist_bin,
        hist_sigma=hist_sigma,
        hist_resizing=hist_resizing,
        hist_method=hist_method,
        rec_loss=rec_loss,
        fixed_gan_weights=fixed_gan_weights,
        skip_conn_to_GAN=skip_conn_to_GAN,
        initialize_gan=initialize_gan,
        variance_loss=variance_loss,
        internal_hist=internal_hist,
        change_hyperparameters=change_hyperparameters,
        change_hyperparameters_after=change_hyperparameters_after
    )
    model.load(name)

    # extension = os.path.splitext(input_image)[1]
    # if (extension == str.lower(extension) == '.jpg' or str.lower(extension) == '.png'):
    output = process_image(model, name, input_image, target_hist, image_size=256,
                    upsampling_output=upsampling_output,
                    upsampling_method=upsampling_method,
                    swapping_levels=swapping_levels,
                    pyramid_levels=pyramid_levels,
                    level_blending=level_blending,
                    post_recoloring=post_recoloring,
                    sampling=sampling,
                    target_number=target_number, results_dir=results_dir,
                    hist_insz=hist_insz, hist_bin=hist_bin,
                    hist_method=hist_method, hist_resizing=hist_resizing,
                    hist_sigma=hist_sigma)
    #print(output.shape)

    #del model
    #torch.cuda.empty_cache()

    return output

ColorTransferLib/Algorithms/RHG/upsampling/buildApplyAffineModelMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function A = buildApplyAffineModelMatrix(input_image, num_output_channels)

num_pixels = size(input_image, 1) * size(input_image, 2);

% TODO: vectorize or output the triplet format.
% Then use horzcat quickly concat the cell array.
A = sparse(0, 0);
for k = 1:size(input_image,3)
    plane = input_image(:,:,k);
    % Repeat each component num_output_channels times.
    sd = spdiags(repmat(plane(:), [num_output_channels 1]), 0, ...
        num_output_channels * num_pixels, num_output_channels * num_pixels);
    A = [A sd];
end

% Ones channel.
ones_diag = spdiags(ones(num_output_channels * num_pixels, 1), 0, ...
    num_output_channels * num_pixels, num_output_channels * num_pixels);

A = [A ones_diag];

ColorTransferLib/Algorithms/RHG/upsampling/buildDerivXMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function A = buildDerivXMatrix(grid_size)

% d/dx for every entry in the first slice except the last column.
m = grid_size(1) * (grid_size(2) - 1);
n = grid_size(1) * grid_size(2);
e = ones(m, 1);
d_dx = spdiags([-e, e], [0, grid_size(1)], m, n);

A = sparse(0, 0);

for v = 1:grid_size(5)
    for u = 1:grid_size(4)
        for k = 1:grid_size(3)
            A = blkdiag(A, d_dx);
        end
    end
end

ColorTransferLib/Algorithms/RHG/upsampling/buildAffineSliceMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Build the matrices that slices out a particular (i,j) component of the
% affine model stored in a 3D bilateral grid using trilinear interpolation.
function [w, st] = buildAffineSliceMatrix(input_image, edge_image, ...
    grid_size, i, j)

num_grid_cells = prod(grid_size);
image_width = size(input_image, 2);
image_height = size(input_image, 1);
num_pixels = image_width * image_height;
grid_width = grid_size(2);
grid_height = grid_size(1);
grid_depth = grid_size(3);

pixel_x = 0:(size(input_image, 2) - 1);
pixel_y = (0:(size(input_image, 1) - 1))';

% Convert to floating point bilateral grid coordinates:
%   x and y are shifted by a half pixel: pixels are considered to be at the
%   center of their little square.
%   Multiply (grid_width - 1) / image_width: let grid samples be defined
%   at integer edges.
bg_coord_x = (pixel_x + 0.5) * (grid_width - 1) / image_width;
bg_coord_y = (pixel_y + 0.5) * (grid_height - 1) / image_height;
bg_coord_z = edge_image * (grid_depth - 1);

bg_idx_x0 = floor(bg_coord_x);
bg_idx_y0 = floor(bg_coord_y);
bg_idx_z0_im = floor(bg_coord_z);
bg_idx_x0_im = repmat(floor(bg_coord_x), [image_height 1]);
bg_idx_y0_im = repmat(floor(bg_coord_y), [1 image_width]);

% Compute dx, dy, dz images: each pixel is the fractional distance from
% the floored (integer) bilateral grid sample.
dx = repmat(bg_coord_x - bg_idx_x0, [image_height 1]);
dy = repmat(bg_coord_y - bg_idx_y0, [1 image_width]);
dz = bg_coord_z - bg_idx_z0_im;

% Each weight_{x}{y}{z} is an image (height x width).
% Each element (i,j) contributes to exactly 8 voxels.
weight_000 = columnize((1 - dx) .* (1 - dy) .* (1 - dz));
weight_100 = columnize((    dx) .* (1 - dy) .* (1 - dz));
weight_010 = columnize((1 - dx) .* (    dy) .* (1 - dz));
weight_110 = columnize((    dx) .* (    dy) .* (1 - dz));
weight_001 = columnize((1 - dx) .* (1 - dy) .* (    dz));
weight_101 = columnize((    dx) .* (1 - dy) .* (    dz));
weight_011 = columnize((1 - dx) .* (    dy) .* (    dz));
weight_111 = columnize((    dx) .* (    dy) .* (    dz));

st_i = (1:(8 * num_pixels))';
st_bg_xx = columnize(...
    [(bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2) (bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2) (bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2) (bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2)]');
st_bg_yy = columnize(...
    [(bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 2) (bg_idx_y0_im(:) + 2) (bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 2) (bg_idx_y0_im(:) + 2)]');
st_bg_zz = columnize(...
    [(bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 2) (bg_idx_z0_im(:) + 2) (bg_idx_z0_im(:) + 2) (bg_idx_z0_im(:) + 2)]');
st_bg_uu = i * ones(8 * num_pixels, 1);
st_bg_vv = j * ones(8 * num_pixels, 1);
st_s = ones(8 * num_pixels, 1);

% Prune rows of the triplet vectors (*not* of st) where where grid indices go
% out of bounds. We only prune certain elements. The number of rows of the
% output matrix is still the same.
indices = (st_bg_xx > 0) & (st_bg_xx <= grid_width) ...
    & (st_bg_yy > 0) & (st_bg_yy <= grid_height) ...
    & (st_bg_zz > 0) & (st_bg_zz <= grid_depth);

st_i = st_i(indices);
st_bg_xx = st_bg_xx(indices);
st_bg_yy = st_bg_yy(indices);
st_bg_zz = st_bg_zz(indices);
st_bg_uu = st_bg_uu(indices);
st_bg_vv = st_bg_vv(indices);
st_s = st_s(indices);
st_j = sub2ind(grid_size, st_bg_yy, st_bg_xx, st_bg_zz, st_bg_uu, st_bg_vv);
st_m = 8 * num_pixels;
st_n = num_grid_cells;

st = sparse(st_i, st_j, st_s, st_m, st_n);

w_i = columnize(repmat(1:num_pixels, [8 1]));
w_j = (1:(8 * num_pixels))';
w_s = columnize(...
    [weight_000, weight_100, weight_010, weight_110, weight_001, weight_101 weight_011, weight_111]');
w_m = num_pixels;
w_n = 8 * num_pixels;

w = sparse(w_i, w_j, w_s, w_m, w_n);

ColorTransferLib/Algorithms/RHG/upsampling/buildDerivYMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function A = buildDerivYMatrix(grid_size)

% Derivatives down y.
ny = grid_size(1);
e = ones(ny - 1, 1);
d_dy = spdiags([-e, e], 0:1, ny - 1, ny);

A = sparse(0, 0);

for v = 1:grid_size(5)
    for u = 1:grid_size(4)
        for k = 1:grid_size(3)
            for j = 1:grid_size(2)
                A = blkdiag(A, d_dy);
            end
        end
    end
end

ColorTransferLib/Algorithms/RHG/upsampling/buildDerivZMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function A = buildDerivZMatrix(grid_size)

% d/dz for every entry in the cube except the last slice.
m = grid_size(1) * grid_size(2) * (grid_size(3) - 1);
n = grid_size(1) * grid_size(2) * grid_size(3);
e = ones(m, 1);
d_dz = spdiags([-e, e], [0, grid_size(1) * grid_size(2)], m, n);

A = sparse(0, 0);

for v = 1:grid_size(5)
    for u = 1:grid_size(4)
        A = blkdiag(A, d_dz);
    end
end

ColorTransferLib/Algorithms/RHG/upsampling/buildSecondDerivZMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function A = buildSecondDerivZMatrix(grid_size)

% d/dz for every entry in the cube except the first and last slice.
m = grid_size(1) * grid_size(2) * (grid_size(3) - 2);
n = grid_size(1) * grid_size(2) * grid_size(3);
e = ones(m, 1);
interior = spdiags([e, -2 * e, e], ...
    [0, grid_size(1) * grid_size(2), 2 * grid_size(1) * grid_size(2)], ...
    m, n);

boundary_z1 = makeBoundaryZ1(n);
boundary_zend = makeBoundaryZEnd(n);

% The matrix for a full cube.
cube = [boundary_z1; interior; boundary_zend];

% Repeat the cube for the last 2 dimensions.
A = sparse(0, 0);
for v = 1:grid_size(5)
    for u = 1:grid_size(4)
        A = blkdiag(A, cube);
    end
end

% Boundary conditions for the first slice.
% n is the number of columns for the full matrix (the number of variables
% in the grid).
function A = makeBoundaryZ1(n)
    mm = grid_size(1) * grid_size(2);
    nn = grid_size(1) * grid_size(2) * 2;
    e = ones(mm, 1);
    B = spdiags([-e, e], [0, grid_size(1) * grid_size(2)], mm, nn);
    % Concat zero columns to the right so we can vertical concat with the full
    % matrix later.
    A = [B, sparse(mm, n - nn)];
end

% Boundary conditions for the last slice.
function A = makeBoundaryZEnd(n)
    mm = grid_size(1) * grid_size(2);
    nn = grid_size(1) * grid_size(2) * 2;
    e = ones(mm, 1);
    B = spdiags([e, -e], [0, grid_size(1) * grid_size(2)], mm, nn);
    % Concat zero columns to the right so we can vertical concat with the full
    % matrix later.
    A = [sparse(mm, n - nn), B];
end

end

ColorTransferLib/Algorithms/RHG/upsampling/buildSliceMatrix.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function [w, st] = buildSliceMatrix(input_image, model_weight, grid_size)

model_weight = ones(size(input_image));

num_pixels = numel(input_image);
num_grid_cells = prod(grid_size);
image_width = size(input_image, 2);
image_height = size(input_image, 1);
grid_width = grid_size(2);
grid_height = grid_size(1);
grid_depth = grid_size(3);

pixel_x = 0:(size(input_image, 2) - 1);
pixel_y = (0:(size(input_image, 1) - 1))';
input_z = input_image;

% Convert to floating point bilateral grid coordinates:
%   x and y are shifted by a half pixel: pixels are considered to be at the
%   center of their little square.
%   Multiply (grid_width - 1) / image_width: let grid samples be defined
%   at integer edges.
bg_coord_x = (pixel_x + 0.5) * (grid_width - 1) / image_width;
bg_coord_y = (pixel_y + 0.5) * (grid_height - 1) / image_height;
bg_coord_z = input_z * (grid_depth - 1);

bg_idx_x0 = floor(bg_coord_x);
bg_idx_y0 = floor(bg_coord_y);
bg_idx_z0_im = floor(bg_coord_z);
bg_idx_x0_im = repmat(floor(bg_coord_x), [image_height 1]);
bg_idx_y0_im = repmat(floor(bg_coord_y), [1 image_width]);

% Compute dx, dy, dz images: each pixel is the fractional distance from
% the floored (integer) bilateral grid sample.
dx = repmat(bg_coord_x - bg_idx_x0, [image_height 1]);
dy = repmat(bg_coord_y - bg_idx_y0, [1 image_width]);
dz = bg_coord_z - bg_idx_z0_im;

% Each weight_{x}{y}{z} is an image (height x width).
% Each element (i,j) contributes to exactly 8 voxels.
weight_000 = columnize((1 - dx) .* (1 - dy) .* (1 - dz) .* model_weight);
weight_100 = columnize((    dx) .* (1 - dy) .* (1 - dz) .* model_weight);
weight_010 = columnize((1 - dx) .* (    dy) .* (1 - dz) .* model_weight);
weight_110 = columnize((    dx) .* (    dy) .* (1 - dz) .* model_weight);
weight_001 = columnize((1 - dx) .* (1 - dy) .* (    dz) .* model_weight);
weight_101 = columnize((    dx) .* (1 - dy) .* (    dz) .* model_weight);
weight_011 = columnize((1 - dx) .* (    dy) .* (    dz) .* model_weight);
weight_111 = columnize((    dx) .* (    dy) .* (    dz) .* model_weight);

st_i = (1:(8 * num_pixels))';
st_bg_xx = columnize(...
    [(bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2) (bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2) (bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2) (bg_idx_x0_im(:) + 1) (bg_idx_x0_im(:) + 2)]');
st_bg_yy = columnize(...
    [(bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 2) (bg_idx_y0_im(:) + 2) (bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 1) (bg_idx_y0_im(:) + 2) (bg_idx_y0_im(:) + 2)]');
st_bg_zz = columnize(...
    [(bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 1) (bg_idx_z0_im(:) + 2) (bg_idx_z0_im(:) + 2) (bg_idx_z0_im(:) + 2) (bg_idx_z0_im(:) + 2)]');
st_s = ones(8 * num_pixels, 1);

% Prune rows of the triplet vectors (*not* of st) where where grid indices go
% out of bounds. We only prune certain elements. The number of rows of the
% output matrix is still the same.
indices = (st_bg_xx > 0) & (st_bg_xx <= grid_width) ...
    & (st_bg_yy > 0) & (st_bg_yy <= grid_height) ...
    & (st_bg_zz > 0) & (st_bg_zz <= grid_depth);

st_i = st_i(indices);
st_bg_xx = st_bg_xx(indices);
st_bg_yy = st_bg_yy(indices);
st_bg_zz = st_bg_zz(indices);
st_s = st_s(indices);
st_j = sub2ind(grid_size, st_bg_yy, st_bg_xx, st_bg_zz);
st_m = 8 * num_pixels;
st_n = num_grid_cells;

st = sparse(st_i, st_j, st_s, st_m, st_n);

w_i = columnize(repmat(1:num_pixels, [8 1]));
w_j = (1:(8 * num_pixels))';
w_s = columnize(...
    [weight_000, weight_100, weight_010, weight_110, weight_001, weight_101 weight_011, weight_111]');
w_m = num_pixels;
w_n = 8 * num_pixels;

w = sparse(w_i, w_j, w_s, w_m, w_n);

ColorTransferLib/Algorithms/RHG/upsampling/columnize.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function output = columnize(V)
% treat V as a vector, return V(:) but can be called as a function.
output = V(:);

ColorTransferLib/Algorithms/RHG/upsampling/demo_temp.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

%% Local Laplacian on Parrot.
runOnFilenames('../../images/', ...
    'low_res_in.png', 'low_res_out.png', ...
    'high_res_in.png', 'high_res_ground_truth_out.png');

fprintf('Program paused, press Enter to continue.\n');
pause;
close all;

%% Local Laplacian on Parrot (grayscale).
runOnFilenames('../../images/', ...
    'low_res_in.png', 'low_res_out_gray.png', ...
    'high_res_in.png', 'high_res_ground_truth_out_gray.png');

fprintf('Program paused, press Enter to continue.\n');
pause;
close all;

ColorTransferLib/Algorithms/RHG/upsampling/BGU.m

function BGU(input_file_fs, output_fs, output_file_ds)
input_file_fs = char(input_file_fs);
output_fs = char(output_fs);
output_file_ds = char(output_file_ds);
input_fs = im2double(imread(input_file_fs));
edge_fs = rgb2luminance(input_fs); % Used to slice at full resolution.
output_ds = im2double(imread(output_file_ds));
if size(output_ds, 1) > 300 || size(output_ds, 2) > 300
    output_ds = imresize(output_ds,[300,300]);
end
input_ds = imresize(input_fs, [size(output_ds, 1), size(output_ds, 2)]);
edge_ds = rgb2luminance(input_ds); % Determines grid z at low resolution.
result = testBGU_modified(input_ds, edge_ds, output_ds, [], input_fs, edge_fs, []);

imwrite(result, output_fs);

ColorTransferLib/Algorithms/RHG/upsampling/getDefaultAffineGridSize.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% grid_size = getDefaultAffineGridSize(input_image)
%
% Get the default affine bilateral grid size.
% grid_size = round([input_height / 16, input_width / 16, 8, ...
%   output_channels, input_channels + 1]);
%
function grid_size = getDefaultAffineGridSize(input_image, output_image)

input_height = size(input_image, 1);
input_width = size(input_image, 2);
input_channels = size(input_image, 3);
output_channels = size(output_image, 3);

grid_size = round([input_height / 16, input_width / 16, 8, ...
    output_channels, input_channels + 1]);

ColorTransferLib/Algorithms/RHG/upsampling/heightWidth.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Returns [size(x, 1), size(x, 2)].
function sz = heightWidth(x)

sz = [size(x, 1), size(x, 2)];

ColorTransferLib/Algorithms/RHG/upsampling/maxn.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function mx = maxn(V)
% Max of V treated as a vector, equal to max(V(:))
mx = max(V(:));

ColorTransferLib/Algorithms/RHG/upsampling/mse.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function out = mse(a, b)
% Compute the mean squared error between A and B (treated as column
% vectors).

out = mean((a(:) - b(:)) .^ 2);

ColorTransferLib/Algorithms/RHG/upsampling/rgb2luminance.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Convert rgb to luma by taking a dot product with coeffs.
%
% rgb should be height x width x 3.
% coeffs should be a 3-vector.
function luma = rgb2luminance(rgb, coeffs)

if ismatrix(rgb)
    luma = rgb;
    return;
end

if ~exist('coeffs', 'var')
    coeffs = [0.25, 0.5, 0.25];
end

if ndims(rgb) ~= 3
    error('rgb should be height x width x 3.');
end

if numel(coeffs) ~= 3
    error('coeffs must be a 3-element vector.');
end

if abs(1 - sum(coeffs)) > 1e-6
    warning('coeffs sum to %f, which is not 1.', sum(coeffs));
end

luma = coeffs(1) * rgb(:,:,1) + coeffs(2) * rgb(:,:,2) + ...
    coeffs(3) * rgb(:,:,3);

ColorTransferLib/Algorithms/RHG/upsampling/testBGU.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Run bilateral guided upsampling pipeline on downsampled input and output, then
% compare to ground truth.
%
% input_ds: downsampled input
% edge_ds: downsampled edge
% output_ds: downsampled output
% weight_ds: downsampled weight map
% input_fs: full-size input
% edge_fs: full-size edge
%
% output_fs: ground-truth full-size output
% [optional] grid_size: affine bilateral grid size to pass through
%   [height width depth num_output_channels num_input_channels]
% [optional] lambda_s: spatial smoothness parameter to pass through
% [optional] intensity_options: intensity parameters to pass through
function output = testBGU(input_ds, edge_ds, output_ds, weight_ds, ...
    input_fs, edge_fs, output_fs, grid_size, lambda_s, intensity_options)

if any(heightWidth(input_fs) ~= heightWidth(output_fs))
    error('input_fs and output_fs need to be the same size.');
end

output.input_ds = input_ds;
output.edge_ds = edge_ds;
output.output_ds = output_ds;
output.input_fs = input_fs;
output.edge_fs = edge_fs;
output.output_fs = output_fs;

if exist('grid_size', 'var')
    output.grid_size = grid_size;
else
    output.grid_size = [];
end

if exist('lambda_s', 'var')
    output.lambda_s = lambda_s;
else
    output.lambda_s = [];
end

if exist('intensity_options', 'var')
    output.intensity_options = intensity_options;
else
    output.intensity_options = [];
end

tic;
[output.gamma, output.A, output.b, output.lambda_s, output.intensity_options] = ...
    bguFit(input_ds, edge_ds, ...
    output_ds, weight_ds, output.grid_size, output.lambda_s, output.intensity_options);
t_fit = toc;

output.grid_size = size(output.gamma);

tic;
output.result_fs = bguSlice(output.gamma, input_fs, edge_fs);
t_slice = toc;

output.t_fit = t_fit;
output.t_slice = t_slice;

fprintf('Fit took %f ms\nSlice took %f ms\n', 1000 * t_fit, 1000 * t_slice);

output.mse = mse(output.result_fs, output.output_fs);
output.maxabsdiff = maxn(abs(output.result_fs - output.output_fs));
output.psnr = psnr(output.result_fs, output.output_fs, 1);
if ndims(output.result_fs) == 3
    output.ssim = ssim(rgb2gray(output.result_fs), rgb2gray(output.output_fs));
else
    output.ssim = ssim(output.result_fs, output.output_fs);
end

fprintf('Mean squared error = %f\n', output.mse);
fprintf('Max absdiff = %f\n', output.maxabsdiff);
fprintf('PSNR (peak = 1.0) = %f\n', output.psnr);
fprintf('SSIM = %f\n', output.ssim);

ColorTransferLib/Algorithms/RHG/upsampling/showTestResults.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% showTestResults(tr)
%
% Show the test results from a run of testBGU.
function [] = showTestResults(tr)

figure;
imshow(tr.input_fs);
title('Input full size');

figure;
imshow(tr.input_ds);
title('Input downsampled');

figure;
imshow(tr.output_ds);
title('Output downsampled');

figure;
imshow(tr.result_fs);
title('Result full size');

figure;
imshow(tr.output_fs);
title('Ground Truth full size');

pos = get(gcf, 'Position');

figure;
imshow(abs(tr.output_fs - tr.result_fs));
colorbar;
title('Absolute difference');
set(gcf, 'Position', pos);

ColorTransferLib/Algorithms/RHG/upsampling/runOnFilenames.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

function [] = runOnFilenames(prefix, ...
    input_ds_filename, output_ds_filename, ...
    input_fs_filename, output_fs_gt_filename)

input_fs = im2double(imread(fullfile(prefix, input_fs_filename)));
edge_fs = rgb2luminance(input_fs); % Used to slice at full resolution.
input_ds = im2double(imread(fullfile(prefix, input_ds_filename)));
edge_ds = rgb2luminance(input_ds); % Determines grid z at low resolution.
output_ds = im2double(imread(fullfile(prefix, output_ds_filename)));
output_fs_gt = im2double(imread(fullfile(prefix, output_fs_gt_filename))); % Ground truth.

% Call driver function.
% [] is for weight_ds: we're not doing a weighted fit.
% grid_size, lambda_s, intensity_options: just use defaults.
results = testBGU(input_ds, edge_ds, output_ds, [], ...
    input_fs, edge_fs, output_fs_gt);
showTestResults(results);

ColorTransferLib/Algorithms/RHG/upsampling/bguFit.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Bilateral Guided Upsampling fits an affine bilateral grid gamma for an
%   operator f: input_image --> output_image.
%
% function [gamma, A, b, lambda_spatial, intensity_options] =
%   bguFit(input_image, edge_image, ...
%   output_image, output_weight, grid_size, lambda_spatial, intensity_options)
%
% Inputs:
%
% input_image is a double tensor: the input to the operator f.
%   Dimensions: height x width x num_input_channels
% edge_image is a double matrix: the "edges" we try to respect. For
%   example, the input luminance (try rgb2luminance or rgb2gray).
%   Dimensions: height x width (x 1)
% output_image is a double tensor: the output f(input_image).
%   Dimensions: height x width x num_output_channels
%
% [Optional] weight_image is a double tensor: whether f is defined at each
%   pixel. Weights need only be non-negative (>= 0) and can be any double.
%   Dimensions: height x width x num_output_channels
%   (same size as output_image)
%
% [Optional] grid_size is:
%   [height width depth num_output_channels num_input_channels]
%   If not specified or empty, defaults to:
%   [round(input_height / 16), round(input_width / 16), 8, ...
%    output_channels, input_channels + 1]
%
% [Optional] lambda_spatial controls spatial smoothness, and defaults to 1.
%   lambda_spatial must be positive.
%
% [Optional] intensity_options is a struct:
%   .type:
%     The type of constraint you want on the grid in the intensity
%     direction. Must be one of:
%       'none': no constraints
%       'first': constrain the first derivative dg/dz
%       'second': constrain the second derivative d2g/dz2
%     Default: 'second'
%   .value:
%     The value towards which the first or second derivative should be.
%     Note that this value is in *absolute* output units and is independent
%     of the grid spacing. I.e., if you want your derivatives to be close
%     to 1 in each bin, then set type to 'first' and value to 1, not
%     1/grid_size(3).
%       type = 'none': ignored.
%       type = 'first' or 'second': any double.
%     Default: 0
%   .lambda:
%     The strength of the constraint relative to the other terms.
%     Default: 4e-6 for first derivative, 4e-7 for second derivative.
%
% Outputs:
%
% A, b are the optional output sparse matrix and right hand side vector
% used to solve for gamma. gamma = reshape(A \ b, grid_size).
%
% lambda_spatial and intensity_options are echoed back as optional outputs
% so the test harness can ask for default parameters then retrieve them.
function [gamma, A, b, lambda_spatial, intensity_options] = ...
    bguFit(input_image, edge_image, ...
    output_image, output_weight, grid_size, lambda_spatial, intensity_options)

DEFAULT_LAMBDA_SPATIAL = 1;

DEFAULT_FIRST_DERIVATIVE_LAMBDA_Z = 4e-6; % about 0.01 for default bin sizes.
%DEFAULT_FIRST_DERIVATIVE_LAMBDA_Z = 4e-7; % about 0.001 for default bin sizes.

DEFAULT_SECOND_DERIVATIVE_LAMBDA_Z = 4e-7; % about 0.01 for default bin sizes.
%DEFAULT_SECOND_DERIVATIVE_LAMBDA_Z = 4e-8; % about 0.001 for default bin sizes.

if ~isa(input_image, 'double')
    error('input_image must be double.');
end

if ~isa(output_image, 'double')
    error('model_image must be double.');
end

if ~exist('edge_image', 'var') || isempty(edge_image) || ...
        ~ismatrix(edge_image) || ~isa(edge_image, 'double')
    error('edge_image must be a double matrix (one channel).');
end

if isempty(output_weight)
    output_weight = ones(size(output_image));
end

if ~isa(output_weight, 'double')
    error('weight must be double matrix');
end

if ndims(input_image) < 2
    error('input_image must be at least two-dimensional.');
end

if ndims(output_image) < 2
    error('output_image must be at least two-dimensional.');
end

if ~isequal(size(input_image, 1), size(output_image, 1)) || ...
    ~isequal(size(input_image, 2), size(output_image, 2))
    error('input_image and output_image must have the same width and height.');
end

if ~exist('grid_size', 'var') || isempty(grid_size)
    grid_size = getDefaultAffineGridSize(input_image, output_image);
end

if ~exist('lambda_spatial', 'var') || isempty(lambda_spatial)
    lambda_spatial = DEFAULT_LAMBDA_SPATIAL;
end

if lambda_spatial <= 0
    error('lambda_spatial must be positive.');
end

% If you pass in nothing, default to second derivative.
if ~exist('intensity_options', 'var') || isempty(intensity_options)
    intensity_options.type = 'second';
    intensity_options.value = 0;
    intensity_options.lambda = DEFAULT_SECOND_DERIVATIVE_LAMBDA_Z;
    intensity_options.enforce_monotonic = false;
end

% If you ask for a constraint but are missing some of the parameters.
if strcmp(intensity_options.type, 'first')
    if ~isfield(intensity_options, 'lambda')
        intensity_options.lambda = DEFAULT_FIRST_DERIVATIVE_LAMBDA_Z;
    end

    if ~isfield(intensity_options, 'value')
        intensity_options.value = 0;
    end

    if ~isfield(intensity_options, 'enforce_monotonic')
        intensity_options.enforce_monotonic = false;
    end
elseif strcmp(intensity_options.type, 'second')
    if ~isfield(intensity_options, 'lambda')

        intensity_options.lambda = DEFAULT_SECOND_DERIVATIVE_LAMBDA_Z;
    end

    if ~isfield(intensity_options, 'value')
        intensity_options.value = 0;
    end

    if ~isfield(intensity_options, 'enforce_monotonic')
        intensity_options.enforce_monotonic = false;
    end
else
    if ~isfield(intensity_options, 'enforce_monotonic')
        intensity_options.enforce_monotonic = false;
    end
end

input_height = size(input_image, 1);
input_width = size(input_image, 2);
grid_height = grid_size(1);
grid_width = grid_size(2);
grid_depth = grid_size(3);
affine_output_size = grid_size(4);
affine_input_size = grid_size(5);

% Size of each grid cell in pixels (# pixels per bin).
bin_size_x = input_width / grid_width;
bin_size_y = input_height / grid_height;
bin_size_z = 1 / grid_depth;

num_deriv_y_rows = (grid_height - 1) * grid_width * grid_depth ...
    * affine_output_size * affine_input_size;
num_deriv_x_rows = grid_height * (grid_width - 1) * grid_depth ...
    * affine_output_size * affine_input_size;

% Set up data term Ax = b.
%
% x is the bilateral grid. It is vectorized as a column vector of size n,
% where n = grid_height * grid_width * grid_depth * ...
%   affine_output_size * affine_input_size.
%
% The right hand side b is the *output* image, vectorized as output_image(:).
% I.e., it's a m x 1 column vector:
% [ red0 red1 ... redN green0 green1 ... greenN blue0 blue1 ... blueN ]'
%
% A is an m x n (sparse) matrix.
% It slices into the bilateral grid with linear interpolation at edge_image
% to get an affine model, then applies it.

% TODO: vectorize
% Build slice matrices for each (i,j) entry of the affine model.
weight_matrices = cell(affine_output_size, affine_input_size);
slice_matrices = cell(affine_output_size, affine_input_size);
for j = 1:affine_input_size    
    for i = 1:affine_output_size
        %fprintf('Building weight and slice matrices, i = %d, j = %d\n', i, j);
        [weight_matrices{i, j}, slice_matrices{i, j}] = ...
            buildAffineSliceMatrix(input_image, edge_image, grid_size, i, j);
    end
end

% Concat them together.
slice_matrix = sparse(0, 0);
weight_matrix = sparse(0, 0);
for j = 1:affine_input_size
    for i = 1:affine_output_size
        %fprintf('Concatenating affine slice matrices, i = %d, j = %d\n', i, j);
        slice_matrix = [slice_matrix; slice_matrices{i, j}];
        weight_matrix = blkdiag(weight_matrix, weight_matrices{i, j});
    end
end

%fprintf('Building apply affine model matrix\n');
apply_affine_model_matrix = buildApplyAffineModelMatrix(...
    input_image, affine_output_size);

% Complete slicing matrix.
%fprintf('Building full slice matrix\n');
sqrt_w = sqrt(output_weight(:)); % weighted least squares takes sqrt(w).
W_data = spdiags(sqrt_w, 0, numel(output_weight), numel(output_weight));
A_data = W_data * apply_affine_model_matrix * weight_matrix * slice_matrix;
b_data = output_image(:) .* sqrt_w;

% ----- Add deriv_y constraints -----
%fprintf('Building d/dy matrix\n');
A_deriv_y = (bin_size_x * bin_size_z / bin_size_y) * lambda_spatial * ...
    buildDerivYMatrix(grid_size);
b_deriv_y = zeros(num_deriv_y_rows, 1);

% ----- Add deriv_x constraints -----
%fprintf('Building d/dx matrix\n');
A_deriv_x = (bin_size_y * bin_size_z / bin_size_x) * lambda_spatial * ...
    buildDerivXMatrix(grid_size);
b_deriv_x = zeros(num_deriv_x_rows, 1);

% ----- Add intensity constraints -----
%fprintf('Building d/dz matrix\n');
if strcmp(intensity_options.type, 'first')
    A_intensity = (bin_size_x * bin_size_y / bin_size_z) * ...
        intensity_options.lambda * buildDerivZMatrix(grid_size);
    value = intensity_options.lambda * intensity_options.value;
    m = size(A_intensity, 1);
    b_intensity = value * ones(m, 1);
elseif strcmp(intensity_options.type, 'second')
    A_intensity = (bin_size_x * bin_size_y) / (bin_size_z * bin_size_z) * ...
        intensity_options.lambda * ...
        buildSecondDerivZMatrix(grid_size);
    value = intensity_options.lambda * intensity_options.value;
    m = size(A_intensity, 1);
    b_intensity = value * ones(m, 1);
end

% ----- Assemble final A matrix -----
%fprintf('Assembling final sparse system\n');
if ~strcmp(intensity_options.type, 'none')
    A = [A_data; A_deriv_y; A_deriv_x; A_intensity];
    b = [b_data; b_deriv_y; b_deriv_x; b_intensity];
else
    A = [A_data; A_deriv_y; A_deriv_x];
    b = [b_data; b_deriv_y; b_deriv_x];
end

% ----- Solve -----
%fprintf('Solving system\n');
gamma = A \ b;
gamma = reshape(gamma, grid_size);

ColorTransferLib/Algorithms/RHG/upsampling/testBGU_modified.m

% Copyright 2016 Google Inc.
%
% Licensed under the Apache License, Version 2.0 (the "License");
% you may not use this file except in compliance with the License.
% You may obtain a copy of the License at
%
% http ://www.apache.org/licenses/LICENSE-2.0
%
% Unless required by applicable law or agreed to in writing, software
% distributed under the License is distributed on an "AS IS" BASIS,
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
% See the License for the specific language governing permissions and
% limitations under the License.

% Run bilateral guided upsampling pipeline on downsampled input and output, then
% compare to ground truth.
%
% input_ds: downsampled input
% edge_ds: downsampled edge
% output_ds: downsampled output
% weight_ds: downsampled weight map
% input_fs: full-size input
% edge_fs: full-size edge
%
% output_fs: ground-truth full-size output
% [optional] grid_size: affine bilateral grid size to pass through
%   [height width depth num_output_channels num_input_channels]
% [optional] lambda_s: spatial smoothness parameter to pass through
% [optional] intensity_options: intensity parameters to pass through
function result = testBGU_modified(input_ds, edge_ds, output_ds, weight_ds, ...
    input_fs, edge_fs, grid_size, lambda_s, intensity_options)

if ~exist('grid_size', 'var')
    grid_size = round([input_height / 16, input_width / 16, 8, ...
    output_channels, input_channels + 1]);
end

if ~exist('lambda_s', 'var')
    lambda_s = [];
end

if ~exist('intensity_options', 'var')
    intensity_options = [];
end
[gamma, A, b, lambda_s, intensity_options] = ...
    bguFit(input_ds, edge_ds, output_ds, weight_ds, grid_size, lambda_s, intensity_options);
result =  bguSlice(gamma, input_fs, edge_fs);

ColorTransferLib/Algorithms/RHG/utils/diff_augment.py

import random

import torch
import torch.nn.functional as F

'''Source: https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py'''

def DiffAugment(x, types=[]):
    for p in types:
        for f in AUGMENT_FNS[p]:
            x = f(x)
    return x.contiguous()

def rand_brightness(x):
    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype,
                        device=x.device) - 0.5)
    return x

def rand_saturation(x):
    x_mean = x.mean(dim=1, keepdim=True)
    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype,
                                   device=x.device) * 2) + x_mean
    return x

def rand_contrast(x):
    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)
    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype,
                                   device=x.device) + 0.5) + x_mean
    return x

def rand_translation(x, ratio=0.125):
    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(
        x.size(3) * ratio + 0.5)
    translation_x = torch.randint(-shift_x, shift_x + 1,
                                  size=[x.size(0), 1, 1], device=x.device)
    translation_y = torch.randint(-shift_y, shift_y + 1,
                                  size=[x.size(0), 1, 1], device=x.device)
    grid_batch, grid_x, grid_y = torch.meshgrid(
        torch.arange(x.size(0), dtype=torch.long, device=x.device),
        torch.arange(x.size(2), dtype=torch.long, device=x.device),
        torch.arange(x.size(3), dtype=torch.long, device=x.device),
    )
    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)
    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)
    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])
    x = x_pad.permute(
        0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)
    return x

def rand_offset(x, ratio=1, ratio_h=1, ratio_v=1):
    w, h = x.size(2), x.size(3)

    imgs = []
    for img in x.unbind(dim = 0):
        max_h = int(w * ratio * ratio_h)
        max_v = int(h * ratio * ratio_v)

        value_h = random.randint(0, max_h) * 2 - max_h
        value_v = random.randint(0, max_v) * 2 - max_v

        if abs(value_h) > 0:
            img = torch.roll(img, value_h, 2)

        if abs(value_v) > 0:
            img = torch.roll(img, value_v, 1)

        imgs.append(img)

    return torch.stack(imgs)

def rand_offset_h(x, ratio=1):
    return rand_offset(x, ratio=1, ratio_h=ratio, ratio_v=0)

def rand_offset_v(x, ratio=1):
    return rand_offset(x, ratio=1, ratio_h=0, ratio_v=ratio)

def rand_cutout(x, ratio=0.5):
    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)
    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2),
                             size=[x.size(0), 1, 1], device=x.device)
    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2),
                             size=[x.size(0), 1, 1], device=x.device)
    grid_batch, grid_x, grid_y = torch.meshgrid(
        torch.arange(x.size(0), dtype=torch.long, device=x.device),
        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),
        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),
    )
    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0,
                         max=x.size(2) - 1)
    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0,
                         max=x.size(3) - 1)
    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype,
                      device=x.device)
    mask[grid_batch, grid_x, grid_y] = 0
    x = x * mask.unsqueeze(1)
    return x

AUGMENT_FNS = {
    'color': [rand_brightness, rand_saturation, rand_contrast],
    'offset': [rand_offset],
    'offset_h': [rand_offset_h],
    'offset_v': [rand_offset_v],
    'translation': [rand_translation],
    'cutout': [rand_cutout],
}

ColorTransferLib/Algorithms/RHG/utils/color_transfer_MKL.py

import numpy as np

EPS = 2.2204e-16

def color_transfer_MKL(source, target):
  assert len(source.shape) == 3, 'Images should have 3 dimensions'
  assert source.shape[-1] == 3, 'Images should have 3 channels'
  X0 = np.reshape(source, (-1, 3), 'F')
  X1 = np.reshape(target, (-1, 3), 'F')
  A = np.cov(X0, rowvar=False)
  B = np.cov(X1, rowvar=False)
  T = MKL(A, B)
  mX0 = np.mean(X0, axis=0)
  mX1 = np.mean(X1, axis=0)
  XR = (X0 - mX0) @ T + mX1
  IR = np.reshape(XR, source.shape, 'F')
  IR = np.real(IR)
  IR[IR > 1] = 1
  IR[IR < 0] = 0
  return IR

def MKL(A, B):
  Da2, Ua = np.linalg.eig(A)

  Da2 = np.diag(Da2)
  Da2[Da2 < 0] = 0
  Da = np.sqrt(Da2 + EPS)
  C = Da @ np.transpose(Ua) @ B @ Ua @ Da
  Dc2, Uc = np.linalg.eig(C)

  Dc2 = np.diag(Dc2)
  Dc2[Dc2 < 0] = 0
  Dc = np.sqrt(Dc2 + EPS)
  Da_inv = np.diag(1 / (np.diag(Da)))
  T = Ua @ Da_inv @ Uc @ Dc @ np.transpose(Uc) @ Da_inv @ np.transpose(Ua)
  return T

ColorTransferLib/Algorithms/RHG/utils/init.py

from . import face_preprocessing

ColorTransferLib/Algorithms/RHG/utils/face_preprocessing.py

import os
import dlib
import PIL.Image
import PIL.ImageFile
import numpy as np
import scipy.ndimage
import os.path as path
# Reference: https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py

def detect_face_landmarks(face_file_path=None,
                          predictor_path=None,
                          img=None):
  # References:
  # -   http://dlib.net/face_landmark_detection.py.html
  # -   http://dlib.net/face_alignment.py.html

  if predictor_path is None:
    predictor_path = './utils/shape_predictor_68_face_landmarks.dat'

  # Load all the models we need: a detector to find the faces, a shape predictor
  # to find face landmarks so we can precisely localize the face
  detector = dlib.get_frontal_face_detector()
  shape_predictor = dlib.shape_predictor(predictor_path)

  if img is None:
    # Load the image using Dlib
    print("Processing file: {}".format(face_file_path))
    img = dlib.load_rgb_image(face_file_path)

  shapes = list()

  # Ask the detector to find the bounding boxes of each face. The 1 in the
  # second argument indicates that we should upsample the image 1 time. This
  # will make everything bigger and allow us to detect more faces.
  dets = detector(img, 1)

  num_faces = len(dets)
  print("Number of faces detected: {}".format(num_faces))

  if num_faces < 1:
    raise Exception('No face found!')

  # Find the face landmarks we need to do the alignment.
  faces = dlib.full_object_detections()
  for d in dets:
    print("Left: {} Top: {} Right: {} Bottom: {}".format(
      d.left(), d.top(), d.right(), d.bottom()
    ))

    shape = shape_predictor(img, d)
    faces.append(shape)

  return faces

def recreate_aligned_images(json_data,
                            dst_dir='./temp-faces/',
                            output_size=1024,
                            transform_size=4096,
                            enable_padding=True):
    print('Recreating aligned images...')
    if dst_dir:
        os.makedirs(dst_dir, exist_ok=True)

    for item_idx, item in enumerate(json_data.values()):
        print('\r%d / %d ... ' % (item_idx, len(json_data)), end='', flush=True)

        # Parse landmarks.
        # pylint: disable=unused-variable
        lm = np.array(item['in_the_wild']['face_landmarks'])
        filename = item['in_the_wild']['file_path']
        filename = path.split(filename)[-1]

        lm_chin = lm[0:17]  # left-right
        lm_eyebrow_left = lm[17:22]  # left-right
        lm_eyebrow_right = lm[22:27]  # left-right
        lm_nose = lm[27:31]  # top-down
        lm_nostrils = lm[31:36]  # top-down
        lm_eye_left = lm[36:42]  # left-clockwise
        lm_eye_right = lm[42:48]  # left-clockwise
        lm_mouth_outer = lm[48:60]  # left-clockwise
        lm_mouth_inner = lm[60:68]  # left-clockwise

        # Calculate auxiliary vectors.
        eye_left = np.mean(lm_eye_left, axis=0)
        eye_right = np.mean(lm_eye_right, axis=0)
        eye_avg = (eye_left + eye_right) * 0.5
        eye_to_eye = eye_right - eye_left
        mouth_left = lm_mouth_outer[0]
        mouth_right = lm_mouth_outer[6]
        mouth_avg = (mouth_left + mouth_right) * 0.5
        eye_to_mouth = mouth_avg - eye_avg

        # Choose oriented crop rectangle.
        print(eye_to_mouth.shape)
        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]
        x /= np.hypot(*x)
        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)
        y = np.flipud(x) * [-1, 1]
        c = eye_avg + eye_to_mouth * 0.1
        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])
        qsize = np.hypot(*x) * 2

        # Load in-the-wild image.
        src_file = item['in_the_wild']['file_path']
        img = PIL.Image.open(src_file)

        # Shrink.
        shrink = int(np.floor(qsize / output_size * 0.5))
        if shrink > 1:
            rsize = (int(np.rint(float(img.size[0]) / shrink)),
                     int(np.rint(float(img.size[1]) / shrink)))
            img = img.resize(rsize, PIL.Image.ANTIALIAS)
            quad /= shrink
            qsize /= shrink

        # Crop.
        border = max(int(np.rint(qsize * 0.1)), 3)
        crop = (int(np.floor(min(quad[:, 0]))),
                int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),
                int(np.ceil(max(quad[:, 1]))))
        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0),
                min(crop[2] + border, img.size[0]),
                min(crop[3] + border, img.size[1]))
        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:
            img = img.crop(crop)
            quad -= crop[0:2]

        # Pad.
        pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))),
               int(np.ceil(max(quad[:, 0]))), int(np.ceil(max(quad[:, 1]))))
        pad = (max(-pad[0] + border, 0),
               max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),
               max(pad[3] - img.size[1] + border, 0))
        if enable_padding and max(pad) > border - 4:
            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))
            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]),
                                           (0, 0)), 'reflect')
            h, w, _ = img.shape
            y, x, _ = np.ogrid[:h, :w, :1]
            mask = np.maximum(
              1.0 - np.minimum(np.float32(x) / pad[0],
                               np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(
                np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))
            blur = qsize * 0.02
            img += (scipy.ndimage.gaussian_filter(
              img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)
            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)
            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)),
                                      'RGB')
            quad += pad[:2]

        # Transform.
        img = img.transform((transform_size, transform_size), PIL.Image.QUAD,
                            (quad + 0.5).flatten(), PIL.Image.BILINEAR)
        if output_size < transform_size:
            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)

        # Save aligned image.
        # dst_subdir = os.path.join(
        #   dst_dir, '%05d' % (item_idx - item_idx % 1000))
        os.makedirs(dst_dir, exist_ok=True)
        print(f'Saving {os.path.join(dst_dir, filename)}')
        #img.save(os.path.join(dst_subdir, '%05d.png' % item_idx))
        img.save(os.path.join(dst_dir, filename))

    # All done.
    print('\r%d / %d ... done' % (len(json_data), len(json_data)))

    return

def face_extraction(face_file_path):

  faces = detect_face_landmarks(face_file_path)

  img = dlib.load_rgb_image(face_file_path)

  thumbnail_size = 512
  thumbnails = dlib.get_face_chips(img, faces, size=thumbnail_size)

  # The first face which is detected:
  # NB: we assume that there is exactly one face per picture!
  f = faces[0]

  parts = f.parts()

  num_face_landmarks=68

  v = np.zeros(shape=(num_face_landmarks, 2))
  for k, e in enumerate(parts):
    v[k, :] = [e.x, e.y]

  json_data = dict()

  item_idx = 0

  json_data[item_idx] = dict()
  json_data[item_idx]['in_the_wild'] = dict()
  json_data[item_idx]['in_the_wild']['file_path'] = face_file_path
  json_data[item_idx]['in_the_wild']['face_landmarks'] = v

  recreate_aligned_images(json_data)

ColorTransferLib/Algorithms/RHG/utils/imresize.py

from __future__ import print_function
import numpy as np
from math import ceil, floor

# source: https://github.com/fatheral/matlab_imresize

def deriveSizeFromScale(img_shape, scale):
    output_shape = []
    for k in range(2):
        output_shape.append(int(ceil(scale[k] * img_shape[k])))
    return output_shape

def deriveScaleFromSize(img_shape_in, img_shape_out):
    scale = []
    for k in range(2):
        scale.append(1.0 * img_shape_out[k] / img_shape_in[k])
    return scale

def triangle(x):
    x = np.array(x).astype(np.float64)
    lessthanzero = np.logical_and((x>=-1),x<0)
    greaterthanzero = np.logical_and((x<=1),x>=0)
    f = np.multiply((x+1),lessthanzero) + np.multiply((1-x),greaterthanzero)
    return f

def cubic(x):
    x = np.array(x).astype(np.float64)
    absx = np.absolute(x)
    absx2 = np.multiply(absx, absx)
    absx3 = np.multiply(absx2, absx)
    f = np.multiply(1.5*absx3 - 2.5*absx2 + 1, absx <= 1) + np.multiply(-0.5*absx3 + 2.5*absx2 - 4*absx + 2, (1 < absx) & (absx <= 2))
    return f

def contributions(in_length, out_length, scale, kernel, k_width):
    if scale < 1:
        h = lambda x: scale * kernel(scale * x)
        kernel_width = 1.0 * k_width / scale
    else:
        h = kernel
        kernel_width = k_width
    x = np.arange(1, out_length+1).astype(np.float64)
    u = x / scale + 0.5 * (1 - 1 / scale)
    left = np.floor(u - kernel_width / 2)
    P = int(ceil(kernel_width)) + 2
    ind = np.expand_dims(left, axis=1) + np.arange(P) - 1 # -1 because indexing from 0
    indices = ind.astype(np.int32)
    weights = h(np.expand_dims(u, axis=1) - indices - 1) # -1 because indexing from 0
    weights = np.divide(weights, np.expand_dims(np.sum(weights, axis=1), axis=1))
    aux = np.concatenate((np.arange(in_length), np.arange(in_length - 1, -1, step=-1))).astype(np.int32)
    indices = aux[np.mod(indices, aux.size)]
    ind2store = np.nonzero(np.any(weights, axis=0))
    weights = weights[:, ind2store]
    indices = indices[:, ind2store]
    return weights, indices

def imresizemex(inimg, weights, indices, dim):
    in_shape = inimg.shape
    w_shape = weights.shape
    out_shape = list(in_shape)
    out_shape[dim] = w_shape[0]
    outimg = np.zeros(out_shape)
    if dim == 0:
        for i_img in range(in_shape[1]):
            for i_w in range(w_shape[0]):
                w = weights[i_w, :]
                ind = indices[i_w, :]
                im_slice = inimg[ind, i_img].astype(np.float64)
                outimg[i_w, i_img] = np.sum(np.multiply(np.squeeze(im_slice, axis=0), w.T), axis=0)
    elif dim == 1:
        for i_img in range(in_shape[0]):
            for i_w in range(w_shape[0]):
                w = weights[i_w, :]
                ind = indices[i_w, :]
                im_slice = inimg[i_img, ind].astype(np.float64)
                outimg[i_img, i_w] = np.sum(np.multiply(np.squeeze(im_slice, axis=0), w.T), axis=0)        
    if inimg.dtype == np.uint8:
        outimg = np.clip(outimg, 0, 255)
        return np.around(outimg).astype(np.uint8)
    else:
        return outimg

def imresizevec(inimg, weights, indices, dim):
    wshape = weights.shape
    if dim == 0:
        weights = weights.reshape((wshape[0], wshape[2], 1, 1))
        outimg =  np.sum(weights*((inimg[indices].squeeze(axis=1)).astype(np.float64)), axis=1)
    elif dim == 1:
        weights = weights.reshape((1, wshape[0], wshape[2], 1))
        outimg =  np.sum(weights*((inimg[:, indices].squeeze(axis=2)).astype(np.float64)), axis=2)
    if inimg.dtype == np.uint8:
        outimg = np.clip(outimg, 0, 255)
        return np.around(outimg).astype(np.uint8)
    else:
        return outimg

def resizeAlongDim(A, dim, weights, indices, mode="vec"):
    if mode == "org":
        out = imresizemex(A, weights, indices, dim)
    else:
        out = imresizevec(A, weights, indices, dim)
    return out

def imresize(I, scalar_scale=None, method='bicubic', output_shape=None, mode="vec"):
    if method is 'bicubic':
        kernel = cubic
    elif method is 'bilinear':
        kernel = triangle
    else:
        print ('Error: Unidentified method supplied')

    kernel_width = 4.0
    # Fill scale and output_size
    if scalar_scale is not None:
        scalar_scale = float(scalar_scale)
        scale = [scalar_scale, scalar_scale]
        output_size = deriveSizeFromScale(I.shape, scale)
    elif output_shape is not None:
        scale = deriveScaleFromSize(I.shape, output_shape)
        output_size = list(output_shape)
    else:
        print ('Error: scalar_scale OR output_shape should be defined!')
        return
    scale_np = np.array(scale)
    order = np.argsort(scale_np)
    weights = []
    indices = []
    for k in range(2):
        w, ind = contributions(I.shape[k], output_size[k], scale[k], kernel, kernel_width)
        weights.append(w)
        indices.append(ind)
    B = np.copy(I) 
    flag2D = False
    if B.ndim == 2:
        B = np.expand_dims(B, axis=2)
        flag2D = True
    for k in range(2):
        dim = order[k]
        B = resizeAlongDim(B, dim, weights[dim], indices[dim], mode)
    if flag2D:
        B = np.squeeze(B, axis=2)
    return B

def convertDouble2Byte(I):
    B = np.clip(I, 0.0, 1.0)
    B = 255*B
    return np.around(B).astype(np.uint8)

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/CJianCost.c

#define SQR(X)  ((X)*(X))

#include <math.h>
/* #include <stdio.h> */

/* PLEASE READ THIS HEADER, IMPORTANT INFORMATION INSIDE */
#include "memory_layout_note.h"

#ifdef WIN32
__declspec( dllexport )
#endif
double CJianCost(const double* A, const double* B,const double* CorrA, const double* CorrB, int m, int n, int dim, int numCorr, double scale1 , double* grad)
{
	int i,j,d, indi, indj; 
    int id, jd;
	double dist_ij, cross_term = 0;
    double cost_ij;
	for (i=0;i<m*dim;++i) grad[i] = 0;
	for (i=0;i<numCorr;++i)
	{
		indi = CorrA[i];
		indj = CorrB[i];
			dist_ij = 0;
			for (d=0;d<dim;++d)
			{
                id = indi*dim + d;
                jd = indj*dim + d;
				dist_ij = dist_ij + SQR( A[id] - B[jd]);
			}
            cost_ij = exp(-dist_ij/SQR(scale1));
			for (d=0;d<dim;++d){
                id = indi*dim + d;
                jd = indj*dim + d;
                grad[id] += -cost_ij*2*(A[id] - B[jd]);
            }

			cross_term += cost_ij;
	}
	for (i=0;i<m*dim;++i) {
		grad[i]/=(SQR(scale1));
	}

	return cross_term;
}

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/GaussTransform.m

% [f, grad] = GaussTransform(A,B,scale)
% The inner product between two spherical Gaussian mixtures computed using the Gauss Transform.
% The centers of the two mixtures are given in terms of two point sets A and B (of same dimension d)
% represented by an mxd matrix and an nxd matrix, respectively.
% It is assumed that all the components have the same covariance matrix represented by 
% a scale parameter (scale).  Also, in each mixture, all the components are equally weighted.

%%  $Author: bjian $
%%  $Date: 2008/04/06 03:59:15 $
%%  $Revision: 1.1 $  
function [f,g] = GaussTransform(A,B,scale)	

if exist('mex_GaussTransform','file')
    [f,g] = mex_GaussTransform(A',B',scale);
    g = g';
else
    message = ['Precompiled GaussTransform module not found.\n' ...
        'If the corresponding MEX-functions exist, run the following command:\n' ...
        'mex mex_GaussTransform.c GaussTransform.c'];
    message_id = 'MATLAB:MEXNotFound';
    error (message_id, message);

end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/memory_layout_note.h

/*
 Note:  the assumed memory layout is point-wise contiguous, 
 i.e. the j-th coordinate of the i-th point is at location  
 (i*d + j) where d is the dimensionality. Thus, the MATLAB 
 mex-function expects an input matrix where each point is a 
 d-dimensional column vector as MATLAB is column-major while 
 the input array to the NumPy should place each point as a 
 d-dimensional row vector since NumPy is row-major.
*/

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/GaussTransformCorr.m

% [f, grad] = GaussTransform(A,B,scale)
% The inner product between two spherical Gaussian mixtures computed using the Gauss Transform.
% The centers of the two mixtures are given in terms of two point sets A and B (of same dimension d)
% represented by an mxd matrix and an nxd matrix, respectively.
% It is assumed that all the components have the same covariance matrix represented by 
% a scale parameter (scale).  Also, in each mixture, all the components are equally weighted.

%%  $Author: bjian $
%%  $Date: 2008/04/06 03:59:15 $
%%  $Revision: 1.1 $  
function [f, g] = GaussTransformCorr(A,B,scale, corrA, corrB)	

if exist('mex_CJianCost','file')
    [f,g] = mex_CJianCost(A',B', corrA, corrB, scale);
    g = g';
else
    message = ['Precompiled mex_CJianCost module not found.\n' ...
        'If the corresponding MEX-functions exist, run the following command:\n' ...
        'mex mex_CJianCost.c CJianCost.c'];
    message_id = 'MATLAB:MEXNotFound';
    error (message_id, message);

end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/mex_GaussTransform.c

/*%%=====================================================================
%% Project:   Pointset Registration using Gaussian Mixture Model
%% Module:    $RCSfile: mex_GaussTransform.c,v $
%% Language:  C
%% Author:    $Author: bing.jian $
%% Date:      $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% Version:   $Revision: 109 $
%%=====================================================================*/

#include "mex.h"

double GaussTransform(double* A, double* B, int m, int n, int dim, double scale, double* grad);
void mexFunction(int nlhs,       mxArray *plhs[],
		 int nrhs, const mxArray *prhs[])
{
    /* Declare variables */ 
    int m, n, dim; 
    double *A, *B, *result, *grad, scale;

    /* Check for proper number of input and output arguments */    
    if (nrhs != 3) {
	mexErrMsgTxt("Three input arguments required.");
    } 
    if (nlhs > 2){
	mexErrMsgTxt("Too many output arguments.");
    }

    /* Check data type of input argument */
    if (!(mxIsDouble(prhs[0]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[1]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[2]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }

    /* Get the number of elements in the input argument */
    /* elements=mxGetNumberOfElements(prhs[0]); */
    /* Get the data */
    A = (double *)mxGetPr(prhs[0]);
    B = (double *)mxGetPr(prhs[1]);
    scale = mxGetScalar(prhs[2]);
  	/* Get the dimensions of the matrix input A&B. */
  	m = mxGetN(prhs[0]);
  	n = mxGetN(prhs[1]);
  	dim = mxGetM(prhs[0]);
  	if (mxGetM(prhs[1])!=dim)
  	{
  		mexErrMsgTxt("The two input point sets should have same dimension.");
  	}
    /* Allocate the space for the return argument */
    plhs[0] = mxCreateDoubleMatrix(1,1,mxREAL);
    plhs[1] = mxCreateDoubleMatrix(dim,m,mxREAL);
    result = mxGetPr(plhs[0]);
    grad = mxGetPr(plhs[1]);
    *result = GaussTransform(A, B, m, n, dim, scale, grad);

}

ColorTransferLib/Algorithms/RHG/utils/pyramid_upsampling.py

import cv2 as cv
import torch
import numpy as np
from . import imresize as resize

def pyramid_upsampling(target, reference, levels=5, swapping_levels=1,
                       blending=False):

  target = torch.squeeze(target, dim=0).cpu().detach().numpy()
  target = target.transpose(1, 2, 0)
  target[target < 0] = 0.0
  target[target > 1] = 1.0
  reference = torch.squeeze(reference, dim=0).cpu().detach().numpy()
  reference = reference.transpose(1, 2, 0)

  h, w, _ = reference.shape
  if w % (2 ** levels) == 0:
    new_size_w = w
  else:
    new_size_w = w + (2 ** levels) - w % (2 ** levels)

  if h % (2 ** levels) == 0:
    new_size_h = h
  else:
    new_size_h = h + (2 ** levels) - h % (2 ** levels)

  new_size = (new_size_h, new_size_w)
  if not ((h, w) == new_size):
    reference = resize.imresize(reference, output_shape=new_size)

  target = resize.imresize(target, output_shape=reference.shape[:2])

  target = target.astype(float)
  reference = reference.astype(float)

  # generate Gaussian pyramid for target
  G = target.copy()
  gpA = [G]
  for i in range(levels):
    G = cv.pyrDown(G)
    gpA.append(G)

  # generate Gaussian pyramid for reference
  G = reference.copy()
  gpB = [G]
  for i in range(levels):
    G = cv.pyrDown(G)
    gpB.append(G)

  # generate Laplacian Pyramid for reference
  lpB = [gpB[levels - 1]]
  for i in range(levels - 1, 0, -1):
    GE = cv.pyrUp(gpB[i])
    L = cv.subtract(gpB[i - 1], GE)
    lpB.append(L)

  # generate Laplacian Pyramid for target
  lpA = [gpA[levels - 1]]
  for i in range(levels - 1, 0, -1):
    GE = cv.pyrUp(gpA[i])
    L = cv.subtract(gpA[i - 1], GE)
    lpA.append(L)

  # now reconstruct
  for i in range(swapping_levels):
    lpB[i] = lpA[i]
  if blending:
    weights = np.linspace(0.0, 1.0, levels - swapping_levels + 1)
    for i in range(swapping_levels, levels):
      lpB[i] = (1 - weights[i]) * lpA[i] + weights[i] * lpB[i]

  output = lpB[0]
  for i in range(1, levels):
    output = cv.pyrUp(output)
    output = cv.add(output, lpB[i])

  output = torch.unsqueeze(torch.from_numpy(output.transpose(2, 0, 1)), dim=0)
  return output

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/mex_CJianCost.c

/*%%=====================================================================
%% Project:   Pointset Registration using Gaussian Mixture Model
%% Module:    $RCSfile: mex_GaussTransform.c,v $
%% Language:  C
%% Author:    $Author: bing.jian $
%% Date:      $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% Version:   $Revision: 109 $
%%=====================================================================*/

#include "mex.h"

double CJianCost(double* A, double* B, double* CorrA, double* CorrB, int m, int n, int dim, int numCorr, double scale1 , double* grad);//, double* grad);
void mexFunction(int nlhs,       mxArray *plhs[],
		 int nrhs, const mxArray *prhs[])
{
    /* Declare variables */ 
    int m, n, dim, numCorr; 
    double *A, *B, *result, scale1, *grad;
	double *CorrA, *CorrB;

    /* Check for proper number of input and output arguments */    
    if (nrhs != 5) {
	mexErrMsgTxt("Five input arguments required.");
    } 
    if (nlhs > 2){
	mexErrMsgTxt("Too many output arguments.");
    }

    /* Check data type of input argument */
    if (!(mxIsDouble(prhs[0]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[1]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[2]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }

    /* Get the number of elements in the input argument */
    /* elements=mxGetNumberOfElements(prhs[0]); */
    /* Get the data */
    A = (double *)mxGetPr(prhs[0]);
    B = (double *)mxGetPr(prhs[1]);
	CorrA = (double *)mxGetPr(prhs[2]);
	CorrB = (double *)mxGetPr(prhs[3]);
    scale1 = mxGetScalar(prhs[4]);
  	/* Get the dimensions of the matrix input A&B. */
  	m = mxGetN(prhs[0]);//Get the number of columns in the array A
  	n = mxGetN(prhs[1]);
  	dim = mxGetM(prhs[0]);
	numCorr = mxGetN(prhs[2]);
  	if (mxGetM(prhs[1])!=dim)
  	{
  		mexErrMsgTxt("The two input point sets should have same dimension.");
  	}
    /* Allocate the space for the return argument */
    plhs[0] = mxCreateDoubleMatrix(1,1,mxREAL);
    plhs[1] = mxCreateDoubleMatrix(dim,m,mxREAL);
    result = mxGetPr(plhs[0]);
    grad = mxGetPr(plhs[1]);
    *result = CJianCost(A, B,CorrA, CorrB, m, n, dim, numCorr, scale1, grad);

}

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/mg_initialiseMexFilesGT.m

mex mex_CJianCost.c CJianCost.c
mex mex_GaussTransform.c GaussTransform.c

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/Recolour/mg_recolourTarget.m

function [ finalResult ] = mg_recolourTarget( target, param, ctrl_pts, colourSpace )
%UNTITLED Summary of this function goes here
%   Detailed explanation goes here
A = ((imread(target)));
if(strcmp(colourSpace, 'LAB'))
    A = rgb2lab(A);
end
[fnrows, fncols, d] = size(A);
fullT = reshape(double(A),fnrows*fncols,3);
newFrame = mg_transform_tps_parallel(param, fullT, ctrl_pts);
if(strcmp(colourSpace, 'LAB'))
    newFrame = 255*lab2rgb(newFrame);
end
finalResult = reshape(newFrame, [fnrows fncols 3]);
finalResult = uint8(finalResult);

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/auxilliary/set_bounds.m

% prepare the parameters for optimization 
function [x0, Lb, Ub] = set_bounds(motion)
%%=====================================================================
%% $RCSfile: set_bounds.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================

switch lower(motion)
    case 'rigid2d'
        x0 = [0,0,0];    %[ translation_x, translation_y,  rotation_theta]
        MAX_DX = 80;  MAX_DY = 80;   MAX_DTHETA = pi;
        MIN_DX = -80;  MIN_DY = -80;   MIN_DTHETA = -pi;
        Lb = [MIN_DX; MIN_DY; MIN_DTHETA];
        Ub = [MAX_DX; MAX_DY; MAX_DTHETA];
    case 'rigid3d'     %[unit quaternion,  translation_xyz]
       x0 = [0,0,0, 1, 0, 0, 0];
       MAX_DX = 40;
       MAX_DY = 40;
       MAX_DZ = 40;
       MIN_DX = -40;
       MIN_DY = -40;
       MIN_DZ = -40;
       Lb = [-1 ; -1; -1; -1; MIN_DX; MIN_DY; MIN_DZ;];
       Ub = [1 ; 1; 1; 1; MAX_DX; MAX_DY; MAX_DZ;];
    case 'affine2d'
        d = 2;
        x0 = repmat([zeros(1,d) 1],1,d);  %[translation_xy, reshape(eye(2),1,4)]
        MAX_DX = 40;
        MAX_DY = 40;
        MIN_DX = -40;
        MIN_DY = -40;
        Lb = [MIN_DX; MIN_DY; -inf; -inf; -inf; -inf];
        Ub = [MAX_DX; MAX_DY; inf; inf; inf; inf];
    case 'affine3d'      
        d = 3;
        x0 = repmat([zeros(1,d) 1],1,d);  %[translation_xyz, reshape(eye(3),1,9)]
        MAX_DX = 100;
        MAX_DY = 100;
        MAX_DZ = 100;
        MIN_DX = -100;
        MIN_DY = -100;
        MIN_DZ = -100;
        Lb = [MIN_DX; MIN_DY; MIN_DZ; 0; -inf; -inf; -inf; 0; -inf; -inf; -inf; 0];
        Ub = [MAX_DX; MAX_DY; MAX_DZ; inf; inf; inf; inf; inf; inf; inf; inf; inf];
    otherwise
        error('Unknown motion type');
end;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/Recolour/mg_recolourTargetM.m

function [ finalResult ] = mg_recolourTargetM( target, targetMask, paramB, paramW, ctrl_pts, colourSpace )
%UNTITLED Summary of this function goes here
%   Detailed explanation goes here
A = ((imread(target)));
if(strcmp(colourSpace, 'LAB'))
    A = rgb2lab(A);
end
[fnrows, fncols, d] = size(A);
fullT = reshape(double(A),fnrows*fncols,3);
M = imread(targetMask);
nM = imresize(M, [fnrows, fncols]);
fullM = double(reshape(M(:,:,1),fnrows*fncols,1));
fullM = fullM./255;
newFrame = mg_transform_parallel_mask(paramB,paramW,fullM,fullT, ctrl_pts);%recolour target image in parallel
if(strcmp(colourSpace, 'LAB'))
    newFrame = 255*lab2rgb(newFrame);
end

finalResult = reshape(newFrame, [fnrows fncols 3]);
finalResult = uint8(finalResult);

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/auxilliary/determine_border.m

function [axis_limits] = determine_border(Model, Scene)
%%=====================================================================
%% Module:    $RCSfile: determine_border.m,v $
%% Language:  MATLAB
%% Author:    $Author: bing.jian $
%% Date:      $Date: 2008-11-13 16:34:29 -0500 (Thu, 13 Nov 2008) $
%% Version:   $Revision: 109 $
%%=====================================================================

dim = size(Scene,2);
axis_limits = zeros(dim,2);
for i=1:dim
    min_i = min([Scene(:,i);Model(:,i)]);
    max_i = max([Scene(:,i);Model(:,i)]);
    margin_i = (max_i-min_i)*0.05;
    axis_limits(i,:) = [min_i - margin_i max_i+margin_i];
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/auxilliary/set_ctrl_pts.m

function [ctrl_pts] = set_ctrl_pts(M,S, interval, d, colourSpace)
%%=====================================================================
%% Project:   Point Set Registration using Gaussian Mixture Model
%% Module:    $RCSfile: set_ctrl_pts.m,v $
%% Language:  MATLAB
%% Author:    $Author: bing.jian $
%% Date:      $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% Version:   $Revision: 109 $
%% i this the functions
%%=====================================================================

if(strcmp(colourSpace, 'CIELab'))

    x_min = 0;
    x_max = 100;
    y_min = -100;
    y_max = 100;
        if(d ==2)
            [x,y] = ndgrid(linspace(x_min,x_max,interval), linspace(y_min,y_max,interval));
            n_pts = interval*interval;
            ctrl_pts = [reshape(x,n_pts,1) reshape(y,n_pts,1)];    
        else
            z_min = -110;
            z_max = 100;
            [x,y,z] = ndgrid(linspace(x_min,x_max,interval), linspace(y_min,y_max,interval),linspace(z_min,z_max,interval));
            n_pts = interval*interval*interval;
            ctrl_pts = [reshape(x,n_pts,1) reshape(y,n_pts,1) reshape(z,n_pts,1)];    
        end
else
    x_min = 0;
    x_max = 255;
    y_min = 0;
    y_max = 255;
        if(d ==2)
            [x,y] = ndgrid(linspace(x_min,x_max,interval), linspace(y_min,y_max,interval));
            n_pts = interval*interval;
            ctrl_pts = [reshape(x,n_pts,1) reshape(y,n_pts,1)];    
        else
            z_min = 0;
            z_max = 255;
            [x,y,z] = ndgrid(linspace(x_min,x_max,interval), linspace(y_min,y_max,interval),linspace(z_min,z_max,interval));
            n_pts = interval*interval*interval;
            ctrl_pts = [reshape(x,n_pts,1) reshape(y,n_pts,1) reshape(z,n_pts,1)];    
        end
 end
 end

ColorTransferLib/Algorithms/RHG/utils/vggloss.py

import torch
import torchvision

class VGGPerceptualLoss(torch.nn.Module):
  def __init__(self, resize=True, device='cuda'):
    super(VGGPerceptualLoss, self).__init__()
    blocks = []
    blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())
    blocks.append(
      torchvision.models.vgg16(pretrained=True).features[4:9].eval())
    blocks.append(
      torchvision.models.vgg16(pretrained=True).features[9:16].eval())
    blocks.append(
      torchvision.models.vgg16(pretrained=True).features[16:23].eval())
    for bl in blocks:
      for p in bl:
        p.requires_grad = False
    self.blocks = torch.nn.ModuleList(blocks)
    self.blocks.to(device=device)
    self.transform = torch.nn.functional.interpolate
    self.mean = torch.nn.Parameter(
      torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)).to(device=device)
    self.std = torch.nn.Parameter(
      torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)).to(device=device)
    self.resize = resize

  def forward(self, input, target, feature_layers=[0, 1, 2, 3],
              style_layers=[]):
    input = (input - self.mean) / self.std
    target = (target - self.mean) / self.std
    if self.resize:
      input = self.transform(input, mode='bilinear', size=(224, 224),
                             align_corners=False)
      target = self.transform(target, mode='bilinear', size=(224, 224),
                              align_corners=False)
    loss = 0.0
    x = input
    y = target
    for i, block in enumerate(self.blocks):
      x = block(x)
      y = block(y)
      if i in feature_layers:
        loss += torch.nn.functional.l1_loss(x, y)
      if i in style_layers:
        act_x = x.reshape(x.shape[0], x.shape[1], -1)
        act_y = y.reshape(y.shape[0], y.shape[1], -1)
        gram_x = act_x @ act_x.permute(0, 2, 1)
        gram_y = act_y @ act_y.permute(0, 2, 1)
        loss += torch.nn.functional.mse_loss(gram_x, gram_y)
    return loss

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/clustering/mg_applyKMeans.m

function [ cluster_center ] = mg_applyKMeans(image,nColors)
    %apply the k means algorithm to the image to find k colours that represent
    %it well 
    [n,m,d] = size(image); 
    image = imresize(image, [300 350]);
    %disp(size(image))
    ab = double(image);
    nrows = size(ab,1);
    ncols = size(ab,2);

    if(d == 3)
        ab = reshape(ab,nrows*ncols,d);
        [cluster_idx, cluster_center] = kmeans(ab, nColors, 'distance','sqEuclidean','MaxIter', 1000);

    %    [cluster_idx, cluster_center, clusters] = kmeans_reduced(ab, nColors);
    %    disp(["Amount ot Cluster = ", num2str(clusters)])
    %end

    %function [c_idx, c_center, clusters] = kmeans_reduced(im,colos)
    %    try            
    %        [c_idx, c_center] = kmeans(im, colos, 'distance','sqEuclidean','MaxIter', 1000);
    %        clusters = colos
    %    catch
    %        [c_idx, c_center, clusters] = kmeans_reduced(im,colos-1);
    %    end
    %end
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/GaussTransform/GaussTransform.c

/*%%=====================================================================
%% Project:   Pointset Registration using Gaussian Mixture Model
%% Module:    $RCSfile: GaussTransform.c,v $
%% Language:  C
%% Author:    $Author: bing.jian $
%% Date:      $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% Version:   $Revision: 109 $
%%=====================================================================*/

#define SQR(X)  ((X)*(X))

#include <math.h>
//#include <stdio.h> 

/* PLEASE READ THIS HEADER, IMPORTANT INFORMATION INSIDE */
#include "memory_layout_note.h"

#ifdef WIN32
__declspec( dllexport )
#endif
double GaussTransform(const double* A, const double* B,  int m, int n, int dim, double scale, double* grad)
{
	//FILE *ptr_file;
	//ptr_file =fopen("output5.txt", "w");
	int i,j,d; 
    int id, jd;
	double dist_ij, cross_term = 0;
    double cost_ij;
	for (i=0;i<m*dim;++i) grad[i] = 0;
	for (i=0;i<m;++i)
	{
		for (j=0;j<n;++j)
		{
			dist_ij = 0;
			for (d=0;d<dim;++d)
			{
                id = i*dim + d;
                jd = j*dim + d;
				dist_ij = dist_ij + SQR( A[id] - B[jd]);
			}
            cost_ij = exp(-dist_ij/SQR(scale));
			//fprintf(ptr_file,"cost_ij %f \n", cost_ij);
            for (d=0;d<dim;++d){
                id = i*dim + d;
                jd = j*dim + d;
                grad[id] += -cost_ij*2*(A[id] - B[jd]);
            }
			cross_term += cost_ij;
		}
		/* printf("cross_term = %.3f\n", cross_term); */
	}
	for (i=0;i<m*dim;++i) {
		grad[i]/=(m*n*SQR(scale));
	}
	//fclose(ptr_file);
	return cross_term/(m*n);
}

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/gmmreg_L2_corr.m

%function [param, tt] = GMMReg(model, scene, scale, motion, display, init_param);
%   'model' and 'scene'  are two point sets
%   'scale' is a free scalar parameter
%   'motion':  the transformation model, can be
%         ['rigid2d', 'rigid3d', 'affine2d', 'affine3d']
%         The default motion model is 'rigid2d' or 'rigid3d' depending on
%         the input dimension
%   'display': display the intermediate steps or not.
%   'init_param':  initial parameter

function [param, transformed_model, history, config] = gmmreg_L2_corr(config, corrA, corrB)
%%=====================================================================
%% $Author: bing.jian $
%% $Date: 2009-02-10 07:13:49 +0000 (Tue, 10 Feb 2009) $
%% $Revision: 121 $
%%=====================================================================

% todo: use the statgetargs() in statistics toolbox to process parameter name/value pairs
% Set up shared variables with OUTFUN
history.x = [ ];
history.fval = [ ];
if nargin<1
    error('Usage: gmmreg_L2_corr(config)');
end
[n,d] = size(config.model); % number of points in model set
if (d~=2)&&(d~=3)
    error('The current program only deals with 2D or 3D point sets.');
end

if(config.iter >1)
    options = optimset(  'LargeScale','off','GradObj','on', 'TolFun',1*10^(-3), 'TolX',1e-3, 'TolCon', 1e-3);
else
    options = optimset(  'LargeScale','off','GradObj','on', 'TolFun',1*10^(-10), 'TolX',1e-10, 'TolCon', 1e-10);
end
%options = optimset(options, 'outputfcn',@outfun);
options = optimset(options, 'MaxFunEvals', config.max_iter, 'MaxIter', config.max_iter);
options = optimset(options, 'GradObj', 'on');

switch lower(config.functionType)
    case 'tps'
        scene = config.scene;
        scale = config.scale;
        alpha = config.alpha;
        beta = config.beta;

        [n,d] = size(config.ctrl_pts);
        [m,d] = size(config.model);
        [K,U] = compute_kernel(config.ctrl_pts, config.model);
        Pm = [ones(m,1) config.model];
        Pn = [ones(n,1) config.ctrl_pts];
        PP = null(Pn');  % or use qr(Pn)
        basis = [Pm U*PP];
        kernel = PP'*K*PP;

        init_tps = config.init_tps;  % it should always be of size d*(n-d-1)
        if isempty(config.init_affine)
            % for your convenience, [] implies default affine
            config.init_affine = repmat([zeros(1,d) 1],1,d);
        end
        if config.opt_affine % optimize both affine and tps
            init_affine = [ ];
            x0 = [config.init_affine init_tps(end+1-d*(n-d-1):end)];
        else % optimize tps only
            init_affine = config.init_affine;
            x0 = init_tps(end+1-d*(n-d-1):end);
        end
        tic
        param = fminunc(@(x)gmmreg_L2_tps_costfunc_corr(x, init_affine, basis, kernel, scene, scale, alpha, beta, n, d, corrA, corrB), x0,  options);
        toc
        transformed_model = mg_transform_tps_parallel(param, config.model, config.ctrl_pts);
        if config.opt_affine
            config.init_tps = param(end+1-d*(n-d-1):end);
            config.init_affine = param(1:d*(d+1));
        else
            config.init_tps = param;
        end
    otherwise
        x0 = config.init_param;
        tic
        param = fmincon(@gmmreg_L2_costfunc_corr, x0, [ ],[ ],[ ],[ ], config.Lb, config.Ub, [ ], options, config, corrA, corrB);
        toc
        transformed_model = transform_pointset(config.model, config.functionType, param);
        config.init_param = param;
end

    function stop = outfun(x,optimValues,state,varargin)
     stop = false;
     switch state
         case 'init'
             if config.display>0
               set(gca,'FontSize',16);
             end
         case 'iter'
               history.fval = [history.fval; optimValues.fval];
               history.x = [history.x; reshape(x,1,length(x))];
               if config.display>0
                   hold off
                   switch lower(config.functionType)
                       case 'tps'
                           transformed = transform_pointset(config.model, config.functionType, x, config.ctrl_pts,init_affine);
                       otherwise
                           transformed = transform_pointset(config.model, config.functionType, x);
                   end
                   dist = L2_distance(transformed,config.scene,config.scale);
                   DisplayPoints(transformed,config.scene,d);
                   title(sprintf('L2distance: %f',dist));
                   drawnow;
               end
         case 'done'
              %hold off
         otherwise
     end
    end

end

function [dist] = L2_distance(model, scene, scale)
    dist = GaussTransform(model,model,scale) + GaussTransform(scene,scene,scale) - 2*GaussTransform(model,scene,scale);
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/clustering/mg_quantImage.m

function [X] = mg_quantImage( RGB, n)
%apply matlab's rgb2ind to the RGB values given in 'RGB' to find n colors.
%[im,quant] = rgb2ind(RGB, n);
[im,quant] = rgb2ind(RGB);
X = unique(255*quant, 'rows'); 

end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/gmmreg_rbf_L2.m

function [param, transformed_model, history, config] = gmmreg_rbf_L2(config)
%%=====================================================================
%% This function is based on gmmreg_L2, written by Jian et al and available for download here:https://github.com/bing-jian/gmmreg
%% these steps are detailed further in 'Robust Point Set Registration
%% Using Gaussian Mixture Models'
%%=====================================================================

history.x = [ ];
history.fval = [ ];
if nargin<1
    error('Usage: gmmreg_L2(config)');
end
[n,d] = size(config.model); % number of points in model set
if (d~=2)&&(d~=3)
    error('The current program only deals with 2D or 3D point sets.');
end

% Restrict the number of iterations for the first few annealling steps, if
% there are too many iterations when the scale is too large, a bad solution
% may be reached.
if(config.iter >1)
    options = optimset('GradObj','on', 'TolFun',1e-3, 'TolX',1e-3);
else
    options = optimset( 'GradObj','on', 'TolFun',1*10^(-10), 'TolX',1e-10);
end
%if(config.iter >1)
%    options = optimset( 'LargeScale','off','GradObj','on', 'TolFun',1e-3, 'TolX',1e-3, 'TolCon', 1e-3);
%else
%   options = optimset(  'LargeScale','off','GradObj','on', 'TolFun',1*10^(-10), 'TolX',1e-10, 'TolCon', 1e-10);
%end

%options = optimset(options, 'outputfcn',@outfun, 'PlotFcns', @optimplotfval );
options = optimset(options, 'MaxFunEvals', config.max_iter, 'MaxIter', config.max_iter);
options = optimset(options, 'GradObj', 'on');
options = optimset(options, 'Display', 'off');

switch lower(config.functionType)
    %when an rbf transformation is chosen
    case 'rbf'
        scene = config.scene;
        scale = config.scale;
        alpha = config.alpha;
        beta = config.beta;

        [n,d] = size(config.ctrl_pts);
        [m,d] = size(config.model);
        [U, K] = compute_rbf_kernel(config.ctrl_pts, config.model, config.kernel, config.kernelParam);
        Pm = [ones(m,1) config.model];
        Pn = [ones(n,1) config.ctrl_pts];
        %PP = null(Pn');  % or use qr(Pn)
        basis = [Pm U];
        %kernel = PP'*K*PP;

        init_rbf = config.init_rbf;  % it should always be of size d*(n-d-1)
        if isempty(config.init_affine)
            % for your convenience, [] implies default affine
            config.init_affine = repmat([zeros(1,d) 1],1,d);
        end
        if config.opt_affine % optimize both affine and tps
            init_affine = [ ];
            x0 = [config.init_affine init_rbf(1:end)];
        else % optimize tps only
            init_affine = config.init_affine;
            x0 = init_rbf(end+1-d*(n-d-1):end);
        end
       [ param,fval,exitflag,output,grad,hessian]  = fminunc(@(x)gmmreg_L2_rbf_costfunc(x, init_affine, basis, scene, scale, alpha, beta, n, d, K), x0,  options);
        transformed_model = mg_transform_rbf_parallel(param, config.model, config.ctrl_pts, config.kernel, config.kernelParam);
        if config.opt_affine
            config.init_rbf = param((d*(d+1))+1:end);
            config.init_affine = param(1:d*(d+1));
        else
            config.init_rbf = param;
        end
   %when an affine transformation is chosen
   case 'affine'
        x0 = config.init_param;
        param = fmincon(@gmmreg_L2_costfunc, x0, [ ],[ ],[ ],[ ], config.Lb, config.Ub, [ ], options, config);
        transformed_model = transform_pointset(config.model, 'affine3d', param);
        config.init_param = param;
    %when a tps transformation is chosen    
    case 'tps'
        scene = config.scene;
        scale = config.scale;
        alpha = config.alpha;
        beta = config.beta;

        [n,d] = size(config.ctrl_pts);
        [m,d] = size(config.model);
        [K,U] = compute_kernel(config.ctrl_pts, config.model);
        Pm = [ones(m,1) config.model];
        Pn = [ones(n,1) config.ctrl_pts];
        PP = null(Pn');  
        basis = [Pm U*PP];
        kernel = PP'*K*PP;

        init_tps = config.init_tps;  
        if isempty(config.init_affine)
            config.init_affine = repmat([zeros(1,d) 1],1,d);
        end
        if config.opt_affine % optimize both affine and tps
            init_affine = [ ];
            x0 = [config.init_affine init_tps(end+1-d*(n-d-1):end)];
        else % optimize tps only
            init_affine = config.init_affine;
            x0 = init_tps(end+1-d*(n-d-1):end);
        end

        %minimise the cost function 
        [param] = fminunc(@(x)gmmreg_L2_tps_costfunc(x, init_affine, basis, kernel, scene, scale, alpha, beta, n, d), x0,  options);

        transformed_model = mg_transform_tps_parallel(param, config.model, config.ctrl_pts);
        if config.opt_affine
            config.init_tps = param(end+1-d*(n-d-1):end);
            config.init_affine = param(1:d*(d+1));
            #config.init_affine
        else
            config.init_tps = param;
        end

end

    function stop = outfun(x,optimValues,state,varargin)
     stop = false;
     switch state
         case 'init'
             if config.display>0
               set(gca,'FontSize',16);
             end
         case 'iter'
               history.fval = [history.fval; optimValues.fval];
               history.x = [history.x; reshape(x,1,length(x))];
               if config.display>0
                   hold off
                   switch lower(config.functionType)
                       case 'tps'
                           transformed = transform_pointset(config.model, config.functionType, x, config.ctrl_pts,init_affine);
                       otherwise
                           transformed = transform_pointset(config.model, config.functionType, x);
                   end
                   dist = L2_distance(transformed,config.scene,config.scale);
                   DisplayPoints(transformed,config.scene,d);
                   title(sprintf('L2distance: %f',dist));
                   drawnow;
               end
         case 'done'
              %hold off
         otherwise
     end
    end

end

function [dist] = L2_distance(model, scene, scale)
    dist = GaussTransform(model,model,scale) + GaussTransform(scene,scene,scale) - 2*GaussTransform(model,scene,scale);
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/mg_initialize_config_corr.m

function [config] = mg_initialize_config_corr(model, scene,colourSpace, varargin)

config.model = model;
config.scene = scene;
% estimate the scale from the covariance matrix
[n,d] = size(model);
config.scale = power(det(model'*model/n), 1/(2^d));
config.display = 0;
config.init_param = [ ];
config.max_iter = 10000;
config.normalize = 0;
config.functionType = 'TPS';
config.AnnSteps = 1; %found that the best value it 1. 
config.scale = (2^(config.AnnSteps-1))*(config.scale);
switch lower(config.functionType)
    case 'tps'
        interval = 5;%found that the best value it 5. 
        config.ctrl_pts =  set_ctrl_pts(model, scene, interval, d, colourSpace);%set control points in a regular grid spanning the colour space 
        config.alpha = 1 - 0.003;
        config.beta = 0.003; % this value controls the strength of the regulsrisation term, we found 0.003 gives the best results when correspondences used
        config.opt_affine = 1;
        [n,d] = size(config.ctrl_pts); % number of points in model set
        config.init_tps = zeros(n-d-1,d);
        init_affine = repmat([zeros(1,d) 1],1,d);
        config.init_param = [init_affine zeros(1, d*n-d*(d+1))];
        config.init_affine = [ ];
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/compute_rbf_kernel.m

%% compute the kernel and basis matrix for rbf
function [U, K] = compute_rbf_kernel(ctrl_pts,landmarks, kernel, kernelParam)
%%=====================================================================
%% $RCSfile: compute_rbf_kernel.m,v $
%% $Author: mairead grogan $

%%=====================================================================
[n,d] = size(ctrl_pts);
[m,d] = size(landmarks);
U = zeros(m,n);
K = zeros(n,n);

for i=1:m
    for j=1:n
        r = norm(landmarks(i,1:3) - ctrl_pts(j,1:3));
        U(i,j) =  mg_apply_kernel(r, kernel, kernelParam);
    end
end

for i=1:n
    for j=1:n
        r = norm(ctrl_pts(i,1:3) - ctrl_pts(j,1:3));
        K(i,j) =   mg_apply_kernel(r, kernel, kernelParam);
    end
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/compute_kernel.m

%% compute the kernel and basis matrix for thin-plate splines
%% reference:  Landmark-based Image Analysis, Karl Rohr, p195 
function [K,U] = compute_kernel(ctrl_pts,landmarks, lambda)
%%=====================================================================
%% $RCSfile: compute_kernel.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-30 23:09:59 +0000 (Sun, 30 Nov 2008) $
%% $Revision: 116 $
%%=====================================================================

[n,d] = size(ctrl_pts);
if (nargin<3)
    lambda = 0;
end
%K = zeros(n);
K = lambda*ones(n); %% K only depends on ctrl_pts and lambda
switch d
  case 2
    for i=1:n
        for j=1:n
            r = norm(ctrl_pts(i,1:2) - ctrl_pts(j,1:2));
            if (r>0)
                    K(i,j) =   r*r*log(r);
            end
        end
    end
  case 3
    for i=1:n
        for j=1:n
            r = norm(ctrl_pts(i,1:3) - ctrl_pts(j,1:3));
            K(i,j) =   -r;
        end
    end
end

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
if (nargin>=2)
    [m,d] = size(landmarks);
    %U = zeros(m,n);
    U = lambda*ones(m,n);
    switch d
        case 2
            for i=1:m
                for j=1:n
                    r = norm(landmarks(i,1:2) - ctrl_pts(j,1:2));
                    if (r>0)
                        U(i,j) =   r*r*log(r);
                    end
                end
            end
        case 3
            for i=1:m
                for j=1:n
                    r = norm(landmarks(i,1:3) - ctrl_pts(j,1:3));
                    U(i,j) =   -r;
                end
            end
    end
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_costfunc_corr.m

function [f,g] = gmmreg_L2_costfunc_corr(param, config, corrA, corrB)
%%=====================================================================
%% $RCSfile: gmmreg_L2_costfunc.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================
model = config.model;
scene = config.scene;
motion = config.motion;
scale = config.scale;
[transformed_model] = transform_pointset(model, motion, param);
switch lower(config.motion)
    case 'rigid2d'
        [f, grad] = rigid_costfunc(transformed_model, scene, scale);
        grad = grad';
        g(1) = sum(grad(1,:));
        g(2) = sum(grad(2,:));
        grad = grad*model;
        theta = param(3);
        r = [-sin(theta) -cos(theta);
             cos(theta)  -sin(theta)];
        g(3) = sum(sum(grad.*r));
    case 'rigid3d'
       [f,grad] = rigid_costfunc(transformed_model, scene, scale);
        [r,gq] = quaternion2rotation(param(1:4));
        grad = grad';
        gm = grad*model; 
        g(1) = sum(sum(gm.*gq{1}));
        g(2) = sum(sum(gm.*gq{2}));
        g(3) = sum(sum(gm.*gq{3}));
        g(4) = sum(sum(gm.*gq{4}));        
        g(5) = sum(grad(1,:));
        g(6) = sum(grad(2,:));
        g(7) = sum(grad(3,:));
    case 'affine2d'
        [f,grad] = general_costfunc(transformed_model, scene, scale);
        grad = grad';
        g(1) = sum(grad(1,:));
        g(2) = sum(grad(2,:));
        g(3:6) = reshape(grad*model,1,4);
    case 'affine3d'
        [f,grad] = general_costfunc(transformed_model, scene, scale, corrA, corrB);
        grad = grad';
        g(1) = sum(grad(1,:));
        g(2) = sum(grad(2,:));
        g(3) = sum(grad(3,:));
        g(4:12) = reshape(grad*model,1,9);
    otherwise
        error('Unknown motion type');
end;

function [f, g] = rigid_costfunc(A, B, scale)
[f, g] =  GaussTransform(A,B,scale);
f = -f; g = -g;

function [f, g] = general_costfunc_corr(A, B, scale, corrA, corrB)
[f1, g1] = GaussTransformCorr(A,A,scale, corrA, corrB);
[f2, g2] = GaussTransformCorr(A,B,scale, corrA, corrB);
f =  f1 - 2*f2;
g = 2*g1 - 2*g2;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_costfunc.m

function [f,g] = gmmreg_L2_costfunc(param, config)
%%=====================================================================
%% $RCSfile: gmmreg_L2_costfunc.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================
model = config.model;
scene = config.scene;
scale = config.scale;
[transformed_model] = transform_pointset(model, 'affine3d', param);

[f,grad] = general_costfunc(transformed_model, scene, scale);
grad = grad';
g(1) = sum(grad(1,:));
g(2) = sum(grad(2,:));
g(3) = sum(grad(3,:));
g(4:12) = reshape(grad*model,1,9);

function [f, g] = rigid_costfunc(A, B, scale)
[f, g] =  GaussTransform(A,B,scale);
f = -f; g = -g;

function [f, g] = general_costfunc(A, B, scale)
[f1, g1] = GaussTransform(A,A,scale);
[f2, g2] = GaussTransform(A,B,scale);
f =  f1 - 2*f2;
g = 2*g1 - 2*g2;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_tps_costfunc.m

function [energy, grad] = gmmreg_L2_tps_costfunc(param, init_affine, basis,  kernel, scene, scale, alpha, beta, n, d)
%%=====================================================================
%% This function is based on gmmreg_L2_tps_costfunc, written by Jian et al and available for download here:https://github.com/bing-jian/gmmreg
%% these steps are detailed further in 'Robust Point Set Registration
%% Using Gaussian Mixture Models'
%%=====================================================================

if isempty(init_affine)
    %% if init_affine is given as [ ], then it means the affine matrix is 
    %% part of parameter and will be updated during optimization as well.
    %% In this case, the length of parameter should be n*d
    affine_param = reshape(param(1:d*(d+1)),d,d+1);
    affine_param = affine_param';
    tps_param = reshape(param(d*(d+1)+1:d*n),d,n-d-1);
    tps_param = tps_param';
else
    %% if a non-empty init_affine is given, then it will be treated as
    %% a fixed affine matrix.
    %% In this case, the length of parameter should be (n-d-1)*d
    tps_param = reshape(param(1:d*n-d*(d+1)),d,n-d-1);
    tps_param = tps_param';
    affine_param = reshape(init_affine,d,d+1);
    affine_param = affine_param';
end
after_tps = basis*[affine_param;tps_param];
bending = trace(tps_param'*kernel*tps_param);
[energy,grad] = general_costfunc(after_tps, scene, scale);
energy = alpha*energy + beta * bending;
grad = alpha*basis'*grad;
grad(d+2:n,:) = grad(d+2:n,:) + 2*beta*kernel*tps_param;
if isempty(init_affine) 
    %% In this case, the length of gradient should be n*d    
    grad = grad';
    grad = reshape(grad,1,d*n);
else 
    %% In this case, the length of parameter should be (n-d-1)*d    
    grad(1:d+1,:) = [ ];
    grad = grad';
    grad = reshape(grad,1,d*(n-d-1));
end

function [f, g] = general_costfunc(A, B, scale)
[f1, g1] = GaussTransform(A,A,scale);
[f2, g2] = GaussTransform(A,B,scale);
f =  f1 - 2*f2;
g = 2*g1 - 2*g2;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/quaternion2rotation.m

% convert quaternion to rotation matrix
% Note:  [0,0,0,1] --> eye(3);
%
% See also: transform_by_rigid3d
function [R, g] = quaternion2rotation(q)
%%=====================================================================
%% $RCSfile: quaternion2rotation.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================

x = q(1); y = q(2); z=q(3); r = q(4);
x2 = q(1) * q(1);
y2 = q(2) * q(2);
z2 = q(3) * q(3);
r2 = q(4) * q(4);

R(1,1) = r2 + x2 - y2 - z2;		% fill diagonal terms
R(2,2) = r2 - x2 + y2 - z2;
R(3,3) = r2 - x2 - y2 + z2;

xy = q(1) * q(2);
yz = q(2) * q(3);
zx = q(3) * q(1);
rx = q(4) * q(1);
ry = q(4) * q(2);
rz = q(4) * q(3);

R(1,2) = 2 * (xy + rz);			% fill off diagonal terms
R(1,3) = 2 * (zx - ry);
R(2,3) = 2 * (yz + rx);
R(2,1) = 2 * (xy - rz);
R(3,1) = 2 * (zx + ry);
R(3,2) = 2 * (yz - rx);

ss = (x2+y2+z2+r2);
R = R/ss;

if (nargout>1)
    ssss = ss*ss;
    % derivative of R(1,1) = r2 + x2 - y2 - z2;
    g1(1,1) = 4*x*(y2+z2)/ssss; g2(1,1) = -4*y*(x2+r2)/ssss;
    g3(1,1) = -4*z*(x2+r2)/ssss; g4(1,1) = 4*r*(y2+z2)/ssss;
    % derivative of R(2,2) = r2 - x2 + y2 - z2; 
    g1(2,2) = -4*x*(y2+r2)/ssss; g2(2,2) = 4*y*(x2+z2)/ssss;
    g3(2,2) = -4*z*(y2+r2)/ssss; g4(2,2) = 4*r*(x2+z2)/ssss;
    % derivative of R(3,3) = r2 - x2 - y2 + z2;
    g1(3,3) = -4*x*(z2+r2)/ssss; g2(3,3) = -4*y*(r2+z2)/ssss;
    g3(3,3) = 4*z*(x2+y2)/ssss; g4(3,3) = 4*r*(x2+y2)/ssss;

    % fill off diagonal terms
    % derivative of R(1,2) = 2 * (xy + rz);			
    g1(1,2) = 2*y/ss - 2*x*R(1,2)/ssss;
    g2(1,2) = 2*x/ss - 2*y*R(1,2)/ssss;
    g3(1,2) = 2*r/ss - 2*z*R(1,2)/ssss;
    g4(1,2) = 2*z/ss - 2*r*R(1,2)/ssss;
    % derivative of R(1,3) = 2 * (zx - ry);
    g1(1,3) = 2*z/ss - 2*x*R(1,3)/ssss;
    g2(1,3) = -2*r/ss - 2*y*R(1,3)/ssss;
    g3(1,3) = 2*x/ss - 2*z*R(1,3)/ssss;
    g4(1,3) = -2*y/ss - 2*r*R(1,3)/ssss;
    % derivative of R(2,3) = 2 * (yz + rx);
    g1(2,3) = 2*r/ss - 2*x*R(2,3)/ssss;
    g2(2,3) = 2*z/ss - 2*y*R(2,3)/ssss;
    g3(2,3) = 2*y/ss - 2*z*R(2,3)/ssss;
    g4(2,3) = 2*x/ss - 2*r*R(2,3)/ssss;
    % derivative of R(2,1) = 2 * (xy - rz);
    g1(2,1) = 2*y/ss - 2*x*R(2,1)/ssss;
    g2(2,1) = 2*x/ss - 2*y*R(2,1)/ssss;
    g3(2,1) = -2*r/ss - 2*z*R(2,1)/ssss;
    g4(2,1) = -2*z/ss - 2*r*R(2,1)/ssss;
    % derivative of R(3,1) = 2 * (zx + ry);
    g1(3,1) = 2*z/ss - 2*x*R(3,1)/ssss;
    g2(3,1) = 2*r/ss - 2*y*R(3,1)/ssss;
    g3(3,1) = 2*x/ss - 2*z*R(3,1)/ssss;
    g4(3,1) = 2*y/ss - 2*r*R(3,1)/ssss;
    % derivative of R(3,2) = 2 * (yz - rx);
    g1(3,2) = -2*r/ss - 2*x*R(3,2)/ssss;
    g2(3,2) = 2*z/ss - 2*y*R(3,2)/ssss;
    g3(3,2) = 2*y/ss - 2*z*R(3,2)/ssss;
    g4(3,2) = -2*x/ss - 2*r*R(3,2)/ssss;

    g{1} = g1; g{2} = g2; g{3} = g3; g{4} = g4;
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_affine2d.m

% [result] = transform_by_affine2d(pointset, param)
% perform a 2D affine transform on a pointset and
% return the transformed pointset
% the param is expected in the order of 
%  [tx ty a11 a21 a12 a22] or
%  [tx a11 a12 ]
%  [ty a21 a22 ]
%
% See also: transform_by_affine3d, transform_by_rigid2d
function [result] = transform_by_affine2d(pointset, param)
%%=====================================================================
%% $RCSfile: transform_by_affine2d.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================
n = size(pointset,1);
A = reshape(param,2,3);   A = A';
result =  [ones(n,1)  pointset(:,1:2)]*A;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_rbf_costfunc.m

function [energy, grad] = gmmreg_L2_rbf_costfunc(param, init_affine, basis, scene, scale, alpha, beta, n, d, kernel)
%%=====================================================================
%% $RCSfile: gmmreg_L2_tps_costfunc.m,v $
%% $Author: bing.jian $
%% $Date: 2009-02-10 07:13:49 +0000 (Tue, 10 Feb 2009) $
%% $Revision: 121 $
%%=====================================================================

if isempty(init_affine)
    %% if init_affine is given as [ ], then it means the affine matrix is 
    %% part of parameter and will be updated during optimization as well.
    %% In this case, the length of parameter should be n*d
    affine_param = reshape(param(1:d*(d+1)),d,d+1);
    affine_param = affine_param';
    rbf_param = reshape(param(d*(d+1)+1:end),d,n);
    rbf_param = rbf_param';
else
    %% if a non-empty init_affine is given, then it will be treated as
    %% a fixed affine matrix.
    %% In this case, the length of parameter should be (n-d-1)*d
    rbf_param = reshape(param(1:d*n-d*(d+1)),d,n-d-1);
    rbf_param = rbf_param';
    affine_param = reshape(init_affine,d,d+1);
    affine_param = affine_param';
end
after_rbf = basis*[affine_param;rbf_param];
bending = trace(rbf_param'*kernel*rbf_param);
[energy,grad] = general_costfunc(after_rbf, scene, scale);
energy = alpha*energy + beta * bending;
grad = alpha*basis'*grad;
grad(d+2:(n+(d+1)),:) = grad(d+2:(n+(d+1)),:) + 2*beta*kernel*rbf_param;
if isempty(init_affine) 
    %% In this case, the length of gradient should be n*d    
    grad = grad';
    grad = reshape(grad,1,d*(n+ 4));
else 
    %% In this case, the length of parameter should be (n-d-1)*d    
    grad(1:d+1,:) = [ ];
    grad = grad';
    grad = reshape(grad,1,d*(n-d-1));
end

function [f, g] = general_costfunc(A, B, scale)
[f1, g1] = GaussTransform(A,A,scale);
[f2, g2] = GaussTransform(A,B,scale);
f =  f1 - 2*f2;
g = 2*g1 - 2*g2;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_affine3d.m

% [result] = transform_by_affine3d(pointset, param)
% perform a 3D affine transform on a pointset and
% return the transformed pointset
% the param is expected in the order of 
%  [tx ty tz a11 a21 a31 a12 a22 a32 a13 a23 a33] or
%  [tx a11 a12 a13]
%  [ty a21 a22 a23]
%  [tz a31 a32 a33]
%
% See also: transform_by_affine2d, transform_by_rigid3d
function [result] = transform_by_affine3d(pointset, param)
%%=====================================================================
%% $RCSfile: transform_by_affine3d.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================
n = size(pointset,1);
A = reshape(param,3,4);   A = A'; 
result =  [ones(n,1)  pointset(:,1:3)]*A;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/gmmreg_L2_tps_costfunc_corr.m

function [energy, grad] = gmmreg_L2_tps_costfunc_corr(param, init_affine, basis,  kernel, scene, scale, alpha, beta, n, d, corrA, corrB)
%%=====================================================================
%% $RCSfile: gmmreg_L2_tps_costfunc.m,v $
%% $Author: bing.jian $
%% $Date: 2009-02-10 07:13:49 +0000 (Tue, 10 Feb 2009) $
%% $Revision: 121 $
%%=====================================================================

if isempty(init_affine)
    %% if init_affine is given as [ ], then it means the affine matrix is 
    %% part of parameter and will be updated during optimization as well.
    %% In this case, the length of parameter should be n*d
    affine_param = reshape(param(1:d*(d+1)),d,d+1);
    affine_param = affine_param';
    tps_param = reshape(param(d*(d+1)+1:d*n),d,n-d-1);
    tps_param = tps_param';
else
    %% if a non-empty init_affine is given, then it will be treated as
    %% a fixed affine matrix.
    %% In this case, the length of parameter should be (n-d-1)*d
    tps_param = reshape(param(1:d*n-d*(d+1)),d,n-d-1);
    tps_param = tps_param';
    affine_param = reshape(init_affine,d,d+1);
    affine_param = affine_param';
end
after_tps = basis*[affine_param;tps_param];
bending = trace(tps_param'*kernel*tps_param);
[energy, grad] = general_costfunc(after_tps, scene, scale, corrA, corrB);
energy = alpha*energy + beta * bending;
grad = alpha*basis'*grad;
grad(d+2:n,:) = grad(d+2:n,:) + 2*beta*kernel*tps_param;
if isempty(init_affine) 
    %% In this case, the length of gradient should be n*d    
    grad = grad';
    grad = reshape(grad,1,d*n);
else 
    %% In this case, the length of parameter should be (n-d-1)*d    
    grad(1:d+1,:) = [ ];
    grad = grad';
    grad = reshape(grad,1,d*(n-d-1));
end

function [f, g] = general_costfunc(A, B, scale, corrA, corrB)
[f1, g1] = GaussTransformCorr(A,A,scale, corrA, corrA);
[f2, g2] = GaussTransformCorr(A,B,scale, corrA, corrB);
%f = -2*f2;
%g = -2*f2;
f =  f1 - 2*f2;
g = 2*g1 - 2*g2;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/mg_initialize_config.m

function [config] = mg_initialize_config(model, scene, colourSpace)

config.model = model;
config.scene = scene;
% estimate the scale from the covariance matrix
[n,d] = size(model);
config.scale = power(det(model'*model/n), 1/(2^d));
config.display = 0;
config.init_param = [ ];
config.max_iter = 10000;
config.normalize = 0;
config.functionType = 'TPS';
config.AnnSteps = 5; %found that the best value it 5. 
config.scale = (2^(config.AnnSteps-1))*(config.scale);
switch lower(config.functionType)
    case 'tps'
        interval = 5;%found that the best value it 5. 
        config.ctrl_pts =  set_ctrl_pts(model, scene, interval, d, colourSpace);%set control points in a regular grid spanning the colour space 
        config.alpha = 1 - 0.000003;
        config.beta = 0.000003; % this value controls the strength of the regulsrisation term, we found 0.000003 gives the best results. 
        config.opt_affine = 1;
        [n,d] = size(config.ctrl_pts); % number of points in model set
        config.init_tps = zeros(n-d-1,d);
        init_affine = repmat([zeros(1,d) 1],1,d);
        config.init_param = [init_affine zeros(1, d*n-d*(d+1))];
        config.init_affine = [ ];
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/tps_compute_kernel.m

%% compute the kernel and basis matrix for thin-plate splines
%% reference:  Landmark-based Image Analysis, Karl Rohr, p195
function [K] = tps_compute_kernel(x,z)
%%=====================================================================
%% $RCSfile: tps_compute_kernel.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-30 23:09:59 +0000 (Sun, 30 Nov 2008) $
%% $Revision: 116 $
%%=====================================================================
[n, d] = size (x);
[m, d] = size (z);

% calc. the K matrix.
% 2D: K = r^2 * log r
% 3D: K = -r
K = zeros (n,m);

for i=1:d
  tmp = x(:,i) * ones(1,m) - ones(n,1) * z(:,i)';
  tmp = tmp .* tmp;
  K = K + tmp;
end;

if d == 2
  mask = K < 1e-10; % to avoid singularity.
  K = 0.5 * K .* log(K + mask) .* (K>1e-10);
else
  K = - sqrt(K);
end;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_rigid2d.m

% [result] = transform_by_rigid2d(pointset, param)
% perform a 2D rigid transform on a pointset and
% return the transformed pointset
% Note that here 2D rigid transform is parametrized by [translation_x,
% translation_y, rotation_angle].
%
% See also: transform_by_rigid3d
function [result] = transform_by_rigid2d(pointset, param)
%%=====================================================================
%% $RCSfile: transform_by_rigid2d.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-30 23:09:59 +0000 (Sun, 30 Nov 2008) $
%% $Revision: 116 $
%%=====================================================================
n = size(pointset,1);
% parameters of 2D-rigid transform
delta_x = param(1);   % translation in x-dimension
delta_y = param(2);   % translation in y-dimension
d_theta = param(3);   % rotation in radius
% form the rotation matrix
r = [cos(d_theta) -sin(d_theta);
     sin(d_theta)  cos(d_theta)];
% first rotate
result(:,1:2) = pointset(:,1:2) * r' ;
% than translate
result(:,1:2) = result(:,1:2) + ones(n,1)*[delta_x delta_y];

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_rigid3d.m

% [result] = transform_by_rigid3d(pointset, param)
% perform a 3D rigid transform on a pointset and
% return the transformed pointset
% Note that here 3D rigid transform is parametrized by 7 numbers
% [quaternion(1 by 4) translation(1 by 3)]
%
% See also: quaternion2rotation, transform_by_rigid2d,
% transform_by_affine3d

function [result] = transform_by_rigid3d(pointset, param)
%%=====================================================================
%% $RCSfile: transform_by_rigid3d.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-30 23:09:59 +0000 (Sun, 30 Nov 2008) $
%% $Revision: 116 $
%%=====================================================================
n = size(pointset,1);
r = quaternion2rotation(param(1:4));
t = ones(n,1)*param(5:7);
result = pointset*r' + t;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_by_tps.m

%% Perform thin-plate spline warping
%% Input:
%%       landmarks:   source pts stored in nxd matrix.  
%%       parameters:  parameters in nxd matrix where first (d+1) rows are
%%       affine parameters corresponding to <1,x,y>
%% Output:
%%       warped_pts:  target pts in nxd matrix
%%       energy:      bending energy

function [warped_pts, bending_energy] = transform_by_tps(param, landmarks, ctrl_pts)
%%=====================================================================
%% $RCSfile: transform_by_tps.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-30 23:09:59 +0000 (Sun, 30 Nov 2008) $
%% $Revision: 116 $
%%=====================================================================
if (nargin==2)
    [n,d] = size(landmarks);
    [B,lambda] = compute_basis(landmarks);
    warped_pts = B*param;
    tps_param = param(d+2:n,:);
    bending_energy = trace(tps_param'*diag(lambda)*tps_param);
else
    [m,d] = size(landmarks);
    [n,d] = size(ctrl_pts);
    [K,U] = compute_kernel(ctrl_pts,landmarks);
    Pm = [ones(m,1) landmarks];
    Pn = [ones(n,1) ctrl_pts];
    PP = null(Pn'); B = [Pm U*PP]; 
    warped_pts = B*param;
    tps_param = param(d+2:n,:);
    bending_energy = trace(tps_param'*PP'*K*PP*tps_param);
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/MATLAB/registration/transform_pointset.m

% Perform a spatial tranformation on a given pointset
% motion:  the motion model represented by string, can be
%         'rigid2d',    'rigid3d', 'affine2d',  'affine3d', 'tps'
% parameter: a row vector
function [transformed_pointset] = transform_pointset(pointset, motion, parameter, varargin)
%%=====================================================================
%% $RCSfile: transform_pointset.m,v $
%% $Author: bing.jian $
%% $Date: 2008-11-13 21:34:29 +0000 (Thu, 13 Nov 2008) $
%% $Revision: 109 $
%%=====================================================================

switch lower(motion)
    case 'rigid2d'
        transformed_pointset = transform_by_rigid2d(pointset, parameter);
    case 'rigid3d'
        transformed_pointset = transform_by_rigid3d(pointset, parameter);
    case 'affine2d'
        transformed_pointset = transform_by_affine2d(pointset, parameter);
    case 'affine3d'
        transformed_pointset = transform_by_affine3d(pointset, parameter);
    case 'tps'
        ctrl_pts = varargin{1};
        init_affine = varargin{2};
        [n,d] = size(ctrl_pts);
        p = reshape([init_affine parameter],d,n); p = p'; 
        transformed_pointset = transform_by_tps(p, pointset, ctrl_pts);
    otherwise
        error('Unknown motion type');
end;

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mex_mgRecolourParallel_1.cpp

#include "mex.h"
#include <omp.h>
#include <math.h>
#include <iostream>

void mexFunction(int nlhs, mxArray* plhs[], int nrhs, const mxArray* prhs[])
{
	double *pix, *A, *Nv, *ctrl, *recolour;    
	int num_pix, num_ctrl;   

	if (nrhs != 4) {
	mexErrMsgTxt("Four input arguments required.");
    } 
    if (nlhs > 1){
	mexErrMsgTxt("Too many output arguments.");
    }

	if (!(mxIsDouble(prhs[0]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[1]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[2]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
	 if (!(mxIsDouble(prhs[3]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }

	//inputs   
	pix = (double *)mxGetPr(prhs[0]);
	A = (double *)mxGetPr(prhs[1]);
	Nv = (double *)mxGetPr(prhs[2]);
	ctrl = (double *)mxGetPr(prhs[3]);

	//figure out dimensions   
	num_pix = mxGetN(prhs[0]);
	num_ctrl = mxGetN(prhs[3]);  

	//associate outputs   
	plhs[0] = mxCreateDoubleMatrix(3,num_pix, mxREAL);    
	recolour = mxGetPr(plhs[0]);

	//std::cout << "FUCK" << std::endl;
	//std::cout << A[3] << std::endl;
	//std::cout << A[6] << std::endl;
	//std::cout << A[9] << std::endl;
	//for(int k = 0; k < 375; k++) {
	// 	if(k%3 == 0){
	//		std::cout << std::endl;
	//	}
	//	std::cout << Nv[k] << ", ";
	//}
	//std::cout << "FUCKEND" << std::endl;

	long int i, tmp_indx0,tmp_indx1,tmp_indx2;
	int j;
	double norm;
	int nThreads = omp_get_max_threads();
	#pragma omp parallel for shared(pix, A, Nv, ctrl, num_ctrl) private(j,norm,tmp_indx0, tmp_indx1, tmp_indx2)
	for(i = 0; i < num_pix; i++)
		{
			tmp_indx0 = 3*i;
			tmp_indx1 = 3*i+1;
			tmp_indx2 = 3*i+2;
			recolour[tmp_indx0] = A[0] + A[3]*pix[tmp_indx0] + A[6]*pix[tmp_indx1] + A[9]*pix[tmp_indx2]; // t+Ax1
			recolour[tmp_indx1] = A[1] + A[4]*pix[tmp_indx0] + A[7]*pix[tmp_indx1] + A[10]*pix[tmp_indx2]; //t+Ax2
			recolour[tmp_indx2] = A[2] + A[5]*pix[tmp_indx0] + A[8]*pix[tmp_indx1] + A[11]*pix[tmp_indx2]; //t+Ax3
			//recolour[tmp_indx0] = A[3]*pix[tmp_indx0] + A[6]*pix[tmp_indx1] + A[9]*pix[tmp_indx2]; // t+Ax1
			//recolour[tmp_indx1] = A[4]*pix[tmp_indx0] + A[7]*pix[tmp_indx1] + A[10]*pix[tmp_indx2]; //t+Ax2
			//recolour[tmp_indx2] = A[5]*pix[tmp_indx0] + A[8]*pix[tmp_indx1] + A[11]*pix[tmp_indx2]; //t+Ax3
			//recolour[tmp_indx0] = A[0] + pix[tmp_indx0];
			//recolour[tmp_indx1] = A[1] + pix[tmp_indx1];
			//recolour[tmp_indx2] = A[2] + pix[tmp_indx2];

			for(j = 0; j < num_ctrl; j++)
			{
				norm = -sqrt((pix[tmp_indx0] - ctrl[3*j])*(pix[tmp_indx0] - ctrl[3*j]) + (pix[tmp_indx1] - ctrl[(3*j)+1])*(pix[tmp_indx1] - ctrl[(3*j)+1]) + (pix[tmp_indx2] - ctrl[(3*j)+2])*(pix[tmp_indx2] - ctrl[(3*j)+2])); //|| Xi - cj||
				recolour[tmp_indx0] += norm*(Nv[j*3]); //|| Xi - cj||*Nvj1
				recolour[tmp_indx1] += norm*(Nv[(j*3) + 1]); //|| Xi - cj||*Nvj2
				recolour[tmp_indx2] += norm*(Nv[(j*3) + 2]);//|| Xi - cj||*Nvj3

			}

		}

	mexPrintf("nThreads = %i\n",nThreads);mexEvalString("drawnow");
}

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mex_mgRecolourParallel_Mask.cpp

#include "mex.h"
#include <omp.h>
#include <math.h>

void mexFunction(int nlhs, mxArray* plhs[], int nrhs, const mxArray* prhs[])
{
	double *pix, *A1, *W1, *A2, *W2, *mask, *ctrl, *recolour;    
	int num_pix, num_ctrl;   

	if (nrhs != 7) {
	mexErrMsgTxt("Seven input arguments required.");
    } 
    if (nlhs > 1){
	mexErrMsgTxt("Too many output arguments.");
    }

	if (!(mxIsDouble(prhs[0]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[1]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[2]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
	 if (!(mxIsDouble(prhs[3]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
	  if (!(mxIsDouble(prhs[4]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
	   if (!(mxIsDouble(prhs[5]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
	    if (!(mxIsDouble(prhs[6]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }

	//inputs   
	pix = (double *)mxGetPr(prhs[0]);
	A1 = (double *)mxGetPr(prhs[1]);
	W1 = (double *)mxGetPr(prhs[2]);
	A2 = (double *)mxGetPr(prhs[3]);
	W2 = (double *)mxGetPr(prhs[4]);
	mask = (double *)mxGetPr(prhs[5]);
	ctrl = (double *)mxGetPr(prhs[6]);

	//figure out dimensions   
	num_pix = mxGetN(prhs[0]);
	num_ctrl = mxGetN(prhs[6]);  

	//associate outputs   
	plhs[0] = mxCreateDoubleMatrix(3,num_pix, mxREAL);    
	recolour = mxGetPr(plhs[0]);

	long int i, tmp_indx0,tmp_indx1,tmp_indx2;
	int j;
	double norm, knorm, k1, k2; 
	double W[375];
	double A[12];
	int nThreads = omp_get_max_threads();
	#pragma omp parallel for shared(pix, A1, W1, A2, W2, mask, ctrl, num_ctrl) private(j,norm, knorm, tmp_indx0, tmp_indx1, tmp_indx2, k1,k2, A, W)
	for(i = 0; i < num_pix; i++)
		{
			// Create new variables A that is k1*A1 + k2*A2
			k1 = 1 - mask[i];
			k2 = mask[i];
			A[0] = k1*A1[0] + k2*A2[0];		A[6] = k1*A1[6] + k2*A2[6];
			A[1] = k1*A1[1] + k2*A2[1];		A[7] = k1*A1[7] + k2*A2[7];
			A[2] = k1*A1[2] + k2*A2[2];		A[8] = k1*A1[8] + k2*A2[8];
			A[3] = k1*A1[3] + k2*A2[3];		A[9] = k1*A1[9] + k2*A2[9];
			A[4] = k1*A1[4] + k2*A2[4];		A[10] = k1*A1[10] + k2*A2[10];
			A[5] = k1*A1[5] + k2*A2[5];		A[11] = k1*A1[11] + k2*A2[11];

			// Do computation
			tmp_indx0 = 3*i;
			tmp_indx1 = 3*i+1;
			tmp_indx2 = 3*i+2;
			recolour[tmp_indx0] = A[0] + A[3]*pix[tmp_indx0] + A[6]*pix[tmp_indx1] + A[9]*pix[tmp_indx2]; // t+Ax1
			recolour[tmp_indx1] = A[1] + A[4]*pix[tmp_indx0] + A[7]*pix[tmp_indx1] + A[10]*pix[tmp_indx2]; //t+Ax2
			recolour[tmp_indx2] = A[2] + A[5]*pix[tmp_indx0] + A[8]*pix[tmp_indx1] + A[11]*pix[tmp_indx2]; //t+Ax3

			for(j = 0; j < num_ctrl; j++)
			{
				// Create new variables W that is k1*W1 + k2*W2
				W[j*3] =  k1*W1[j*3] + k2*W2[j*3];		
				W[(j*3) + 1] = k1*W1[(j*3) + 1] + k2*W2[(j*3) + 1];		
				W[(j*3) + 2] = k1*W1[(j*3) + 2] + k2*W2[(j*3) + 2];		

				norm = sqrt((pix[tmp_indx0] - ctrl[3*j])*(pix[tmp_indx0] - ctrl[3*j]) + (pix[tmp_indx1] - ctrl[(3*j)+1])*(pix[tmp_indx1] - ctrl[(3*j)+1]) + (pix[tmp_indx2] - ctrl[(3*j)+2])*(pix[tmp_indx2] - ctrl[(3*j)+2])); //|| Xi - cj||
				knorm = -norm;
				recolour[tmp_indx0] += knorm*(W[j*3]); //|| Xi - cj||*Wj1
				recolour[tmp_indx1] += knorm*(W[(j*3) + 1]); //|| Xi - cj||*Wj2
				recolour[tmp_indx2] += knorm*(W[(j*3) + 2]);//|| Xi - cj||*Wj3

			}
		}

	mexPrintf("nThreads = %i\n",nThreads);mexEvalString("drawnow");
}

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mex_mgRecolourParallelTPS.cpp

#include "mex.h"
#include <omp.h>
#include <math.h>

void mexFunction(int nlhs, mxArray* plhs[], int nrhs, const mxArray* prhs[])
{
	double *pix, *A, *W, *ctrl, *recolour;    
	int num_pix, num_ctrl;   

	if (nrhs != 4) {
	mexErrMsgTxt("Four input arguments required.");
    } 
    if (nlhs > 1){
	mexErrMsgTxt("Too many output arguments.");
    }

	if (!(mxIsDouble(prhs[0]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[1]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
    if (!(mxIsDouble(prhs[2]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }
	 if (!(mxIsDouble(prhs[3]))) {
      mexErrMsgTxt("Input array must be of type double.");
    }

	//inputs   
	pix = (double *)mxGetPr(prhs[0]);
	A = (double *)mxGetPr(prhs[1]);
	W = (double *)mxGetPr(prhs[2]);
	ctrl = (double *)mxGetPr(prhs[3]);

	//figure out dimensions   
	num_pix = mxGetN(prhs[0]);
	num_ctrl = mxGetN(prhs[3]);  

	//associate outputs   
	plhs[0] = mxCreateDoubleMatrix(3,num_pix, mxREAL);    
	recolour = mxGetPr(plhs[0]);

	long int i, tmp_indx0,tmp_indx1,tmp_indx2;
	int j;
	double norm, knorm;
	int nThreads = omp_get_max_threads();
	#pragma omp parallel for shared(pix, A, W, ctrl, num_ctrl) private(j,norm, knorm, tmp_indx0, tmp_indx1, tmp_indx2)
	for(i = 0; i < num_pix; i++)
		{
			tmp_indx0 = 3*i;
			tmp_indx1 = 3*i+1;
			tmp_indx2 = 3*i+2;
			//recolour[tmp_indx0] = A[0] + A[3]*pix[tmp_indx0] + A[6]*pix[tmp_indx1] + A[9]*pix[tmp_indx2]; // t+Ax1
			//recolour[tmp_indx1] = A[1] + A[4]*pix[tmp_indx0] + A[7]*pix[tmp_indx1] + A[10]*pix[tmp_indx2]; //t+Ax2
			//recolour[tmp_indx2] = A[2] + A[5]*pix[tmp_indx0] + A[8]*pix[tmp_indx1] + A[11]*pix[tmp_indx2]; //t+Ax3
			recolour[tmp_indx0] = pix[tmp_indx0];
			recolour[tmp_indx1] = pix[tmp_indx1];
			recolour[tmp_indx2] = pix[tmp_indx2];

			/*
			for(j = 0; j < num_ctrl; j++)
			{
				norm = sqrt((pix[tmp_indx0] - ctrl[3*j])*(pix[tmp_indx0] - ctrl[3*j]) + (pix[tmp_indx1] - ctrl[(3*j)+1])*(pix[tmp_indx1] - ctrl[(3*j)+1]) + (pix[tmp_indx2] - ctrl[(3*j)+2])*(pix[tmp_indx2] - ctrl[(3*j)+2])); //|| Xi - cj||
				knorm = -norm;
				recolour[tmp_indx0] += knorm*(W[j*3]); //|| Xi - cj||*Wj1
				recolour[tmp_indx1] += knorm*(W[(j*3) + 1]); //|| Xi - cj||*Wj2
				recolour[tmp_indx2] += knorm*(W[(j*3) + 2]);//|| Xi - cj||*Wj3

			}
			*/
		}

	mexPrintf("nThreads = %i\n",nThreads);mexEvalString("drawnow");
}

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mgRecolourPixels.m

function [result] = mgRecolourPixels(pix, A, Nv, ctrl)	
if(size(pix,2)==3)
    if exist('mex_mgRecolourParallel_1','file')
        [result] = mex_mgRecolourParallel_1(pix', A', Nv', ctrl');
    else
        message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
            'mex -g  mex_mgRecolourParallel_1.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
        message_id = 'MATLAB:MEXNotFound';
        error (message_id, message);
    end
else
    if exist('mex_mgRecolourParallel2d','file')
        [result] = mex_mgRecolourParallel2d(pix', A', Nv', ctrl');
    else
        message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
            'mex -g  mex_mgRecolourParallel2d.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
        message_id = 'MATLAB:MEXNotFound';
        error (message_id, message);
    end

end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mgRecolourPixels_Mask.m

function [result] = mgRecolourPixels_Mask(pix, A1, Nv1, A2, Nv2, Mask,  ctrl)	
if(size(pix,2)==3)
    if exist('mex_mgRecolourParallel_Mask','file')
        [result] = mex_mgRecolourParallel_Mask(pix', A1', Nv1', A2', Nv2', Mask', ctrl');
    else
        message = ['Precompiled mgrecolourParallel_Mask module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
            'mex -g  mex_mgRecolourParallel_Mask.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
        message_id = 'MATLAB:MEXNotFound';
        error (message_id, message);
    end
else
    %To Do

end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mgRecolourPixelsRBF.m

function [result] = mgRecolourPixels(pix, A, w, ctrl, kernel, varargin)	
switch(kernel)
    case 'Gaussian' 
        b = varargin{1};
        if exist('mex_mgRecolourParallelGaussian','file')
            [result] = mex_mgRecolourParallelGaussian(pix', A', w', ctrl', b);
        else
            message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
            'mex -g  mex_mgRecolourParallelGaussian.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
            message_id = 'MATLAB:MEXNotFound';
            error (message_id, message);
        end

    case 'TPS'
        if exist('mex_mgRecolourParallelTPS','file')
            [result] = mex_mgRecolourParallelTPS(pix', A', w', ctrl');
        else
            message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
           'mex -g  mex_mgRecolourParallelTPS.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
            message_id = 'MATLAB:MEXNotFound';
            error (message_id, message);
        end

    case 'Multiquadric'
        b = varargin{1};
        if exist('mex_mgRecolourParallelMQ','file')
            [result] = mex_mgRecolourParallelMQ(pix', A', w', ctrl',b);
        else
            message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
            'mex -g  mex_mgRecolourParallelMQ.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
            message_id = 'MATLAB:MEXNotFound';
            error (message_id, message);
        end
    case 'Inversequadric'
        b = varargin{1};
        if exist('mex_mgRecolourParallelIQ','file')
            [result] = mex_mgRecolourParallelIQ(pix', A', w', ctrl',b);
        else
            message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
             'mex -g  mex_mgRecolourParallelIQ.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
            message_id = 'MATLAB:MEXNotFound';
            error (message_id, message);
        end
    case 'InverseMultiquadric'
        b = varargin{1};
        if exist('mex_mgRecolourParallelIM','file')
            [result] = mex_mgRecolourParallelIM(pix', A', w', ctrl',b);
        else
            message = ['Precompiled mgrecolourParallel module not found.\n' ...
            'If the corresponding MEX-functions exist, run the following command:\n' ...
            'mex -g  mex_mgRecolourParallelIM.cpp COMPFLAGS="/openmp $COMPFLAGS"'];
            message_id = 'MATLAB:MEXNotFound';
            error (message_id, message);
        end
end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mg_initialiseMexFilesOMP.m

mex -g  mex_mgRecolourParallel_1.cpp
mex -g  mex_mgRecolourParallel_Mask.cpp
mex -g  mex_mgRecolourParallelTPS.cpp

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/README.txt

%%-------------------------------------------------------------------
%%
%% Author: Mairead Grogan
%% Date: April 2017
%% 
%%-------------------------------------------------------------------

This zip file contains the code for the L2 based colour transfer method 
described in 'L2 Divergence for robust colour transfer' published in Computer Vision and Image Understanding 2019. If using this code, please cite this paper. 

It contains code sourced from https://github.com/bing-jian/gmmreg  written by
Jian et al. in support of the paper 'Robust Point Set Registration Using Gaussian Mixture Models'. 
The web page for this paper can be found here: https://code.google.com/p/gmmreg/

The demo.m file is a script which shows how to run the colour transfer algorithm for both images with and without correspondeces. 
It implements the functions ctfunction.m (colour transfer applied to target and palette images without correspondences) and ctfunction_corr.m (colour transfer apllied to target and palette images with correspondences).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

BEFORE RUNNING THE CODE: Building the mex files - 

1. Change the Matlab directory to the folder 'L2RegistrationForCT/MATLAB/GaussTransform' and run the script 'mg_initialiseMexFilesGT.m' to initialise the mex files. 
2. Change the Matlab directory to the folder 'L2RegistrationForCT/OpenMPCode' and run the script 'mg_initialiseMexFilesOMP.m' to initialise the mex files for recolouring that use open MP. 

This code uses Open MP within the mex files. Please ensure that you are using a C/C++ compiler which supports OpenMP. See: https://uk.mathworks.com/matlabcentral/answers/237411-can-i-make-use-of-openmp-in-my-matlab-mex-files

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

FOLDER LAYOUT

L2RegistrationForCT/data/ - contains the image files for processing.

L2RegistrationForCT/Results/ - contains the result files after processing.

L2RegistrationForCT/OpenMPCode/ - contains the mex files and Matlab files used for recolouring the target image using the estimated colour transfer function. This step is parallelised using OpenMP. 

L2RegistrationForCT/MATLAB/  - Files in the 'MATLAB' directory are organised as follows:

	gmmreg_rbf_L2.m and gmmreg_L2_corr.m
	    The main entry into the MATLAB implementation fro images without and wih correspondences.

	mg_initialize_config.m and mg_initialize_config_corr.m
	    Generate the configuration struct used in gmmreg_rbf_L2.m and gmmreg_L2_corr.m.

	auxiliary/
	    Some supporting functions.

	clustering/
		Functions that apply the KMeans or MVQ clustering algorithms.

	GaussTransform/
	    MEX-files for implementing the GaussTransform with and without correspondences.

	registration/
	    Functions used in the Matlab implementation of the GMMReg algorithm, 
	    requiring 'GaussTransform' and the optimization toolbox.

Please address any queries to Mairead Grogan at mgrogan@tcd.ie

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/candelete.txt

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/ctfunction.m

%function [param, config] = ctfunction(paletteFile, targetFile, varargin);
function finalResult = ctfunction(he2, he1, varargin);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ctfunction(paletteFile, targetFile, clusterFun, nColors,colourSpace )
% Computes the colour transfer result between a target image and palette
% image that are aligned, eg. pixels at the same location in both images
% are correspondences.
%
% paletteFile:  The name of the palette file to be processed. The colour
%               distribution of the palette file will be mapped to the target image. 
% targetFile:   The name of the target file to be processed.The colour
%               distribution of the palette file will be mapped to the target image.
% clusterFun:   'KMeans' or 'MVQ'(default). The clustering function used.
% nColors:      The number of clusters computed by the clustering
%               function. The default is 50.
% colourSpace:  The colour space in which the registration is performed. The default is 'RGB'. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

if(nargin < 2)
    %Three input arguements must be given
    error('Two input arguements are required: palette image, target image.');
elseif(nargin == 2)
    %If a clustering function is not specified, use Matlab's MVQ fucntion.
    %If the number of clusters to be used is not specified, use 50. 
    clusterFun = 'MVQ';
    nColors = 50;
    colourSpace = 'RGB';
elseif(nargin == 3)
    clusterFun = varargin{1};
    nColors = 50;
    colourSpace = 'RGB';
elseif(nargin == 4)
    clusterFun = varargin{1};
    nColors = varargin{2};
    colourSpace = 'RGB';
elseif(nargin == 5)
    clusterFun = varargin{1};
    nColors = varargin{2};
    colourSpace = varargin{3};
end
addpath(genpath('../L2RegistrationForCT'));

close all;

%read target image and reshape
%he1 = imread(targetFile);
if(strcmp(colourSpace, 'CIELab') && strcmp(clusterFun, 'KMeans'))
        he1	= rgb2lab(he1);
elseif(strcmp(colourSpace, 'CIELab') && strcmp(clusterFun, 'MVQ'))
    disp('MVQ clustering cannot be applied in CIELAB space in this implementation. Using RGB space instead.');
    colourSpace = 'RGB';
end
dfull1 = double(he1);
fnrows = size(dfull1,1);
fncols = size(dfull1,2);
full_transform = reshape(dfull1,fnrows*fncols,3);

%read palette image
%he2 = imread(paletteFile);
if(strcmp(colourSpace, 'CIELab'))
        he2	= rgb2lab(he2);
end

%cluster target and palette image to get most dominant colours
disp('Clustering of palette and target image started...');
switch(clusterFun)
    case 'KMeans'
        X = mg_applyKMeans(he1,nColors);
        Y = mg_applyKMeans(he2,nColors);
    case 'MVQ'
        X = mg_quantImage(he1, nColors);
        Y = mg_quantImage(he2, nColors);
end
disp('Clustering of palette and target image finished.');

%initialise some parameters used to control the registration 
[config] = mg_initialize_config(X,Y, colourSpace);
disp('Registration of colours started...');
%register the colours using an annealling scheme, estimate a TPS transformation 
for i = 1:config.AnnSteps
    config.iter = (config.AnnSteps-i+1);
    [param, transformed_model, history, config] = gmmreg_rbf_L2(config);
    if(i ~= config.AnnSteps)
        config.scale = .5*config.scale;
    end
end
disp('Registration of colours finished.');

%apply the colour transformation to the target image to get the result
%image
disp('Applying colour transfer to target image...');
fullTransform = mg_transform_tps_parallel(param, full_transform, config.ctrl_pts);
disp('Finished.');
if(strcmp(colourSpace, 'CIELab'))
       fullTransform = lab2rgb(fullTransform);
       ind1 = find(fullTransform < 0);
       fullTransform(ind1) = 0;
       ind2 = find(fullTransform > 1);
       fullTransform(ind2) = 1;
       fullTransform = 255*fullTransform;
end

finalResult = reshape(fullTransform, [fnrows fncols 3]);
finalResult = uint8(finalResult);
%imshow(finalResult);

%sve the result
%disp('Saving the result in Results/result.png');
%imwrite(finalResult, 'Results/nocorrresult.png', 'png' );

end

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/ct_apply.m

function outp = ct_apply(val);
disp(val)
outp = 9.7
end
%% No correspondences [ctfunction(paletteFile, targetFile, clusterFun, nColors,colourSpace )]
%palette1 = 'data/images/kandinsky.jpg';
%target1 = 'data/images/northern_lights.jpg';
#disp(brot)

palette1 = 'data/images/hyacinth.jpg';
target1 = 'data/images/rose.jpg';
res = ctfunction(palette1,target1, 'KMeans', 50, 'RGB'); %this will apply colour transfer in RGB space with 50 Kmeans clusters selected from the target and palette images. (used to generate results in paper)
disp('Saving the result in Results/result.png');
imwrite(res, 'Results/nocorrresult.png', 'png' );

%end

% Some more samples showing how to set the parameters of ctfunction 
%ctfunction(palette1,target1, 'MVQ'); %this will apply colour transfer in RGB space using MVQ clustering, and 50 clusters as default (quicker than KMeans).
%ctfunction(palette1,target1, 'KMeans'); %this will apply colour transfer in RGB space using KMeans clustering, and 50 clusters as default.
%ctfunction(palette1,target1,'MVQ', 30); %this will apply colour transfer with MVQ clustering, computing 30 clusters in the traget and palette images.
%ctfunction(palette1,target1,'KMeans', 50, 'CIELab'); %this will apply colour transfer with KMeans clustering, computing 50 clusters in the target and palette images in CIELab space.

%% Correspondences [ctfunction_corr(paletteFile, targetFile, colourSpace, numCorr)]
%palette2 = 'reference_aligned.png';
%target2 = 'input.png';
%ctfunction_corr(palette2,target2);%this will apply colour transfer in RGB space using 5000 correspondences (assumed that the target and palette images are aligned.)

%Some more samples showing how to set the parameters of ctfunction_corr 
%ctfunction_corr(palette2,target2,'CIELab');
%ctfunction_corr(palette2,target2,'RGB', 50000); % set to 50000 to recreate results in paper.

%% display results
%close all;
%creensize = get(0,'ScreenSize');
%sz = [576, 1024];
%figure('Position', [ ceil((screensize(3)-sz(2))/2), ceil((screensize(4)-sz(1))/2), sz(2), sz(1)]);
%subplot('Position',[0.01  0.4850 0.3200 .47]); 
%imshow(imread(target1)); title('Target Image'); 

%subplot('Position',[0.3400  0.4850 0.3200 .47]); 
%imshow(imread(palette1)); title('Palette Image'); 

%subplot('Position',[0.67  0.4850 0.3200 .47]);
%imshow(imread('Results/nocorrresult.png')); title('Result after colour transfer');   

%subplot('Position',[0.01  0.01 0.3200 .47]); 
%imshow(imread(target2)); title('Target Image'); 

%subplot('Position',[0.3400  0.01 0.3200 .47]); 
%imshow(imread(palette2)); title('Palette Image'); 

%subplot('Position',[0.67  0.01 0.3200 .47]);
%imshow(imread('Results/corrresult.png')); title('Result after colour transfer'); 

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/ctfunction_corr.m

function [param, config] = ctfunction_corr(paletteFile, targetFile, varargin);
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ctfunction_corr(paletteFile, targetFile, colourSpace, numCorr)
% Computes the colour transfer result between a target image and palette
% image that are aligned, eg. pixels at the same location in both images
% are correspondences.
%
% paletteFile:  The name of the palette file to be processed. The colour
%               distribution of the palette file will be mapped to the target image. 
% targetFile:   The name of the target file to be processed.The colour
%               distribution of the palette file will be mapped to the target image.
% colourSpace:  'RGB' or 'CIELab' . The colour space in which the registration is performed. 
% numCorr:      The number of correspondences used to compute the colour transfer function, eg. 50
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

if(nargin < 2)
    %Two input arguements must be given
    error('Two input arguements are required: palette image, target image.');
elseif(nargin == 2)
    colourSpace = 'RGB';
    numCorr = 5000;
elseif(nargin == 3)
    colourSpace = varargin{1};
    numCorr = 5000;
elseif(nargin == 4)
    colourSpace = varargin{1};
    numCorr = varargin{2};
end
addpath(genpath('../L2RegistrationForCT'));

close all;

%read target image and reshape
he1 = imread(targetFile);
if(strcmp(colourSpace, 'CIELab'))
        he1	= rgb2lab(he1);
end
dfull1 = double(he1);
fnrows = size(dfull1,1);
fncols = size(dfull1,2);
ab1 = reshape(dfull1,fnrows*fncols,3);

he2 = imread(paletteFile);
if(strcmp(colourSpace, 'CIELab'))
        he2	= rgb2lab(he2);
end
ab2 = double(he2);
nrows = size(ab2,1);
ncols = size(ab2,2);
ab2 = reshape(ab2,nrows*ncols,3);
%ind = randperm(nrows*ncols,3*round((nrows*ncols)/100));
ind = randperm(nrows*ncols,numCorr);
X = ab1(ind,:); Y = ab2(ind,:);
[n,d] = size(X);

%initialise some parameters used to control the registration 
[config] = mg_initialize_config_corr(X,Y, colourSpace);
disp('Registration of colours started...');
%register the colours using an annealling scheme, estimate a TPS transformation 
for i = 1:config.AnnSteps
    config.iter = (config.AnnSteps-i+1);
    [param, transformed_model, history, config] = gmmreg_L2_corr(config, 0:(n-1), 0:(n-1));
    if(i ~= config.AnnSteps)
        config.scale = .5*config.scale;
    end
end
disp('Registration of colours finished.');

%apply the colour transformation to the target image to get the result
%image
disp('Applying colour transfer to target image...');
fullTransform = mg_transform_tps_parallel(param, ab1, config.ctrl_pts);
disp('Finished.');
if(strcmp(colourSpace, 'CIELab'))
       fullTransform = lab2rgb(fullTransform);
       ind1 = find(fullTransform < 0);
       fullTransform(ind1) = 0;
       ind2 = find(fullTransform > 1);
       fullTransform(ind2) = 1;
       fullTransform = 255*fullTransform;

end
finalResult = reshape(fullTransform, [fnrows fncols 3]);
finalResult = uint8(finalResult);
imshow(finalResult);

%sve the result
imwrite(finalResult, 'Results/corrresult.png', 'png' );

end

ColorTransferLib/Algorithms/TPS/init.py

from . import TPS

ColorTransferLib/Algorithms/TPS/TPS.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import time
import os
from copy import deepcopy
from sys import platform

from ColorTransferLib.Utils.Helper import check_compatibility

if platform == "linux" or platform == "linux2":
    # linux
    os.environ["OCTAVE_EXECUTABLE"] = "/usr/bin/octave-cli"
elif platform == "darwin":
    # OS X
    os.environ["OCTAVE_EXECUTABLE"] = "/opt/homebrew/bin/octave-cli"
elif platform == "win32":
    # Windows...
    pass

from oct2py import octave

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: L2 Divergence for robust colour transfer
#   Author: MairÃ©ad Grogan, Rozenn Dahyot
#   Published in: Computer Vision and Image Understanding
#   Year of Publication: 2019
#
# Abstract:
#   Optimal Transport (OT) is a very popular framework for performing colour transfer in images and videos. We have
#   proposed an alternative framework where the cost function used for inferring a parametric transfer function is
#   defined as the robust L2 divergence between two probability density functions Grogan and Dahyot (2015). In this
#   paper, we show that our approach combines many advantages of state of the art techniques and outperforms many
#   recent algorithms as measured quantitatively with standard quality metrics, and qualitatively using perceptual
#   studies Grogan and Dahyot (2017). Mathematically, our formulation is presented in contrast to the OT cost function
#   that shares similarities with our cost function. Our formulation, however, is more flexible as it allows colour
#   correspondences that may be available to be taken into account and performs well despite potential occurrences of
#   correspondence outlier pairs. Our algorithm is shown to be fast, robust and it easily allows for user interaction
#   providing freedom for artists to fine tune the recoloured images and videos Grogan et al. (2017).
#
# Info:
#   Name: TpsColorTransfer
#   Identifier: TPS
#   Link: https://doi.org/10.1016/j.cviu.2019.02.002
#   Source: https://github.com/groganma/gmm-colour-transfer
#
# Implementation Details:
#   Usage of Octave to run the Matlab-Scripts
#   Clustering is done using KMeans because MVQ does not work in Octave
#   Internal image resizing (mg applyK-Means.m) to 300x350px for clustering
#   Remove largescale and TolCon option in gmmregrbfl2.m because unrecognized
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class TPS:
    identifier = "TpsColorTransfer"
    title = "L2 Divergence for robust colour transfer"
    year = 2019
    compatibility = {
        "src": ["Image", "Mesh"],
        "ref": ["Image", "Mesh"]
    }

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # HOST METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_info():
        info = {
            "identifier": "TpsColorTransfer",
            "title": "L2 Divergence for robust colour transfer",
            "year": 2019,
            "abstract": "Optimal Transport (OT) is a very popular framework for performing colour transfer in images "
                        "and videos. We have proposed an alternative framework where the cost function used for "
                        "inferring a parametric transfer function is defined as the robust L2 divergence between two "
                        "probability density functions Grogan and Dahyot (2015). In this paper, we show that our "
                        "approach combines many advantages of state of the art techniques and outperforms many recent "
                        "algorithms as measured quantitatively with standard quality metrics, and qualitatively using "
                        "perceptual studies Grogan and Dahyot (2017). Mathematically, our formulation is presented in "
                        "contrast to the OT cost function that shares similarities with our cost function. Our "
                        "formulation, however, is more flexible as it allows colour correspondences that may be "
                        "available to be taken into account and performs well despite potential occurrences of "
                        "correspondence outlier pairs. Our algorithm is shown to be fast, robust and it easily allows "
                        "for user interaction providing freedom for artists to fine tune the recoloured images and "
                        "videos Grogan et al. (2017).",
            "types": ["Image"]
        }

        return info
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(src, ref, opt):
        start_time = time.time()
        # NOTE: sudo apt-get install liboctave-dev
        # NOTE: pkg install -forge image
        # NOTE: pkg install -forge statistics

        # check if method is compatible with provided source and reference objects
        output = check_compatibility(src, ref, TPS.compatibility)

        if output["status_code"] == -1:
            output["response"] = "Incompatible type."
            return output

        # Preprocessing
        # NOTE RGB space needs multiplication with 255
        src_img = src.get_raw() * 255
        ref_img = ref.get_raw() * 255
        out_img = deepcopy(src)

        # mex -g  mex_mgRecolourParallel_1.cpp COMPFLAGS="/openmp $COMPFLAGS"
        octave.addpath(octave.genpath('.'))
        #octave.addpath(octave.genpath('module/Algorithms/TpsColorTransfer/L2RegistrationForCT'))
        octave.eval('pkg load image')
        octave.eval('pkg load statistics')

        outp = octave.ctfunction(ref_img, src_img, opt.cluster_method, opt.cluster_num, opt.colorspace)
        outp = outp.astype(np.float32)
        out_img.set_raw(outp)
        output = {
            "status_code": 0,
            "response": "",
            "object": out_img,
            "process_time": time.time() - start_time
        }

        return output

ColorTransferLib/ColorTransfer.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import warnings
import importlib
import sys
import json
import os
import copy

# Suppresses the following warning: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 
# 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to 
# True in Numba 0.59.0.
warnings.filterwarnings("ignore", message=".*The 'nopython' keyword.*")

# Suppresses the following warning: 
# (1) UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.
# warnings.warn("nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.")
# (2) UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in  the future, please use 
# 'weights' instead.
warnings.filterwarnings("ignore", message=".*deprecated.*")

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' # 0: DEBUG, 1: INFO, 2: WARNING, 3: ERROR

from ColorTransferLib.Utils.BaseOptions import BaseOptions
from ColorTransferLib.Utils.Helper import get_methods, get_metrics

available_methods = get_methods()
available_metrics = get_metrics()

for m in available_methods:
    exec(m + " = getattr(importlib.import_module('ColorTransferLib.Algorithms."+m+"."+m+"'), '"+m+"')")
for m in available_metrics:
    exec(m + " = getattr(importlib.import_module('ColorTransferLib.Evaluation."+m+"."+m+"'), '"+m+"')")

# Useful for preventing the status prints from the algorithms which were integrated from public repositories
VAR_BLOCKPRINT = False
# Disable
def blockPrint():
    sys.stdout = open(os.devnull, 'w')
# Restore
def enablePrint():
    sys.stdout = sys.__stdout__

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Proxy class for all color transfer algorithms within this project. This class allows the call of the algorithms with
# different kind of input data without preprocessing. The following input data can be processed:
#
# -----------------------------------------
# | Source                 | Reference    |
# -----------------------------------------
# | Image                  | Image        |
# | Image                  | Pointcloud   |
# | Image                  | Mesh         |
# | Pointcloud             | Image        |
# | Pointcloud             | Pointcloud   |
# | Pointcloud             | Mesh         |
# | Video                  | Image        |
# | Video                  | Pointcloud   |
# | Video                  | Mesh         |
# | Volumetric Video       | Image        |
# | Volumetric Video       | Pointcloud   |
# | Volumetric Video       | Mesh         |
# -----------------------------------------
#
# The following approaches are currently supported:
# global - 2001 - Color Transfer between Images
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class ColorTransfer:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, src, ref, approach):
        self.__src = src
        self.__ref = ref

        # check the types of the objects

        self.__out = None
        self.__approach = approach

        with open(os.path.dirname(os.path.abspath(__file__)) + "/Options/" + approach + ".json", 'r') as f:
            options = json.load(f)
            self.__options = BaseOptions(options)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def apply(self):
        if VAR_BLOCKPRINT: blockPrint()
        self.__out = globals()[self.__approach].apply(self.__src, self.__ref, self.__options)
        if VAR_BLOCKPRINT: enablePrint()
        return self.__out

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def set_option(self, key, value):
        if not key in self.__options.get_keys():
            raise RuntimeError("\033[91m" + "Error: Key <" + key + "> does not exist!" + "\033[0m")
        else:
            self.__options.set_option(key, value)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def set_options(self, options):
        self.__options.set_options(options)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_available_methods():
        av_methods = []
        for met in available_methods:
            dirname = os.path.dirname(__file__)
            filename = os.path.join(dirname, "Options/" + met + ".json")

            with open(filename, 'r') as f:
                options = json.load(f)

            av_m = {
                "name": met,
                "options": options,
                "abstract": getattr(sys.modules[__name__], met).get_info()["abstract"],
                "title": getattr(sys.modules[__name__], met).get_info()["title"],
                "year": getattr(sys.modules[__name__], met).get_info()["year"],
                "types": getattr(sys.modules[__name__], met).get_info()["types"]
            }
            av_methods.append(av_m)
        return av_methods

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Proxy class for all color transfer evlauation metric within this project. This class allows the call of the algorithms with
# different kind of input data without preprocessing.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class ColorTransferEvaluation():
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, src, ref, out):
        self.__src = src
        self.__ref = ref
        self.__out = out

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def apply(self, approach):
        ss = copy.deepcopy(self.__src)
        rr = copy.deepcopy(self.__ref)
        oo = copy.deepcopy(self.__out)
        self.__outeval = globals()[approach].apply(ss, rr, oo)
        return self.__outeval
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def get_available_metrics():
        return available_metrics

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/OpenMPCode/mg_transform_tps_parallel.m

function [warped_pts, bending_energy] = mg_transform_tps_parallel(param, landmarks, ctrl_pts)
%%=====================================================================
%Function created by Mairead Grogan October 2015
% Computes pixel recolouring in parallel for TPS transformation
%%=====================================================================
    [n,d] = size(ctrl_pts);
    param = reshape([param],d,n); param = param'; 
    Pn = [ones(n,1) ctrl_pts];
    PP = null(Pn'); 
    Nv = PP*param((d+2):end, :);
    %param(1:(d+1), 1:d)
    warped_pts = mgRecolourPixels(landmarks, param(1:(d+1), 1:d), Nv, ctrl_pts);
    %disp("FUCK")
    warped_pts = warped_pts';

ColorTransferLib/Algorithms/init.py

from . import BCC
from . import CAM
from . import DPT
from . import EB3
from . import FUZ
from . import GLO
from . import HIS
from . import MKL
from . import NST
from . import PDF
from . import PSN
from . import RHG
from . import TPS
from . import CCS
from . import GPC

ColorTransferLib/Evaluation/BA/BA.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import math
import numpy as np

from ColorTransferLib.ImageProcessing.Image import Image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Bhattacharyya distance (BA)
# ...
#
# Source: https://docs.opencv.org/3.4/d8/dc8/tutorial_histogram_comparison.html
#
# Range [0, ??] -> 0 means perfect similarity
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class BA:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[1]
        ref = args[2]
        bins=[10,10,10]
        histo1 = src.get_color_statistic_3D(bins=bins, normalized=True)
        histo2 = ref.get_color_statistic_3D(bins=bins, normalized=True)

        num_bins = np.prod(bins)

        histo1_m = np.mean(histo1)
        histo2_m = np.mean(histo2)

        ba_l = 1 / np.sqrt(histo1_m * histo2_m * math.pow(num_bins, 2))
        ba_r = np.sum(np.sqrt(np.multiply(histo1, histo2)))
        ba = math.sqrt(1 - ba_l * ba_r)

        return round(ba, 4)

ColorTransferLib/Evaluation/BA/init.py

from . import BA

ColorTransferLib/Algorithms/TPS/L2RegistrationForCT/demo_test.m

%% No correspondences [ctfunction(paletteFile, targetFile, clusterFun, nColors,colourSpace )]
%palette1 = 'data/images/kandinsky.jpg';
%target1 = 'data/images/northern_lights.jpg';
palette1 = 'data/hyacinth.jpg';
target1 = 'data/rose.jpg';
ctfunction(palette1,target1, 'KMeans', 50, 'RGB'); %this will apply colour transfer in RGB space with 50 Kmeans clusters selected from the target and palette images. (used to generate results in paper)

% Some more samples showing how to set the parameters of ctfunction 
%ctfunction(palette1,target1, 'MVQ'); %this will apply colour transfer in RGB space using MVQ clustering, and 50 clusters as default (quicker than KMeans).
%ctfunction(palette1,target1, 'KMeans'); %this will apply colour transfer in RGB space using KMeans clustering, and 50 clusters as default.
%ctfunction(palette1,target1,'MVQ', 30); %this will apply colour transfer with MVQ clustering, computing 30 clusters in the traget and palette images.
%ctfunction(palette1,target1,'KMeans', 50, 'CIELab'); %this will apply colour transfer with KMeans clustering, computing 50 clusters in the target and palette images in CIELab space.

%% Correspondences [ctfunction_corr(paletteFile, targetFile, colourSpace, numCorr)]
%palette2 = 'reference_aligned.png';
%target2 = 'input.png';
%ctfunction_corr(palette2,target2);%this will apply colour transfer in RGB space using 5000 correspondences (assumed that the target and palette images are aligned.)

%Some more samples showing how to set the parameters of ctfunction_corr 
%ctfunction_corr(palette2,target2,'CIELab');
%ctfunction_corr(palette2,target2,'RGB', 50000); % set to 50000 to recreate results in paper.

%% display results
%close all;
%creensize = get(0,'ScreenSize');
%sz = [576, 1024];
%figure('Position', [ ceil((screensize(3)-sz(2))/2), ceil((screensize(4)-sz(1))/2), sz(2), sz(1)]);
%subplot('Position',[0.01  0.4850 0.3200 .47]); 
%imshow(imread(target1)); title('Target Image'); 

%subplot('Position',[0.3400  0.4850 0.3200 .47]); 
%imshow(imread(palette1)); title('Palette Image'); 

%subplot('Position',[0.67  0.4850 0.3200 .47]);
%imshow(imread('Results/nocorrresult.png')); title('Result after colour transfer');   

%subplot('Position',[0.01  0.01 0.3200 .47]); 
%imshow(imread(target2)); title('Target Image'); 

%subplot('Position',[0.3400  0.01 0.3200 .47]); 
%imshow(imread(palette2)); title('Palette Image'); 

%subplot('Position',[0.67  0.01 0.3200 .47]);
%imshow(imread('Results/corrresult.png')); title('Result after colour transfer'); 

ColorTransferLib/Evaluation/BRISQUE/init.py

from . import BRISQUE

ColorTransferLib/Evaluation/CF/CF.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import math
import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Colorfulness (CF)
# ...
#
# Source: Measuring colorfulness in natural images
#
# Range [...]
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class CF:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb2rgyb(img):
        rg = img[:,:,0] - img[:,:,1]
        yb = 0.5 * (img[:,:,0] + img[:,:,1]) - img[:,:,2]
        return rg, yb

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        img = args[2]
        rg, yb = CF.rgb2rgyb(img.get_raw() * 255)

        mu_rg = np.mean(rg)
        mu_yb = np.mean(yb)

        sig_rg = np.std(rg)
        sig_yb = np.std(yb)

        sig_rgyb = math.sqrt(sig_rg ** 2 + sig_yb ** 2)
        mu_rgyb = math.sqrt(mu_rg ** 2 + mu_yb ** 2)

        M = sig_rgyb + 0.3 * mu_rgyb

        return round(M, 4)

ColorTransferLib/Evaluation/CSS/init.py

from . import CSS

ColorTransferLib/Evaluation/CSS/CSS.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Selective color transfer with multi-source images
#   Author: Yao Xiang, Beiji Zou, Hong Li
#   Published in: Pattern Recognition Letters
#   Year of Publication: 2009
#
# Abstract:
#   A novel method is proposed to transfer selective colors from a set of source images to a target image. An improved 
#   EM method is presented to model regional color distribution of the target image by Gaussian Mixture Model (GMM), 
#   then, trained by this model, appropriate reference colors are automatically selected from the given source images 
#   to color each target region. The generated compelling results prove that our proposed method is applicable to 
#   colorize grayscale images and color transfer between chromatic images. A new objective metric considering 
#   colorfulness and structural similarity is also proposed to evaluate the quality of the transferred image, which 
#   verifies good performance of our method.
#
# Info:
#   Name: Colorfulness Structure Similarity
#   Shortname: CSS
#   Identifier: CSS
#   Link: https://doi.org/10.1016/j.patrec.2009.01.004
#   Range: [0, 1] with 1 = best quality
#
# Implementation Details:
#   we stick to the original kernel size of 11x11 for the GSSIM
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class CSS:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def iso2Dgauss(size=11, sig=1.5, normalized=True):
        kernel = np.zeros((size,size))
        shift = size // 2
        for x in range(size): 
            for y in range(size): 
                x_s = x - shift
                y_s = y - shift
                kernel[y, x] = (1.0 / (2 * math.pi * math.pow(sig, 2))) * np.exp( -(math.pow(x_s,2)+math.pow(y_s,2)) / (2*math.pow(sig,2)) )

        if normalized:
            kernel /= np.sum(kernel)
        return kernel

    # ------------------------------------------------------------------------------------------------------------------
    # the CSS uses a 8x8 windows intead of a 11x11
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def ssim(src, ref):
        kernel_gaus = CSS.iso2Dgauss()

        src_img = src
        ref_img = ref

        hi, wi, ci = src_img.shape

        k1 = 0.01
        k2 = 0.03
        L = 1.0
        N = 11
        M = src_img.shape[0] * src_img.shape[1]
        c1 = (k1 * L) ** 2
        c2 = (k2 * L) ** 2
        c3 = c2 / 2.0
        alp = 1.0
        bet = 1.0
        gam = 1.0
        w_r = 1.0/3.0
        w_g = 1.0/3.0
        w_b = 1.0/3.0

        mu_src = cv2.filter2D(src_img, -1, kernel_gaus)
        mu_ref = cv2.filter2D(ref_img, -1, kernel_gaus)

        src_pad = np.pad(src_img, ((5, 5), (5, 5), (0, 0)), 'reflect')
        ref_pad = np.pad(ref_img, ((5, 5), (5, 5), (0, 0)), 'reflect')

        sig_src = np.zeros_like(src_img)
        sig_ref = np.zeros_like(ref_img)
        cov = np.zeros_like(src_img)

        # Version 3
        # src_pad_ext.shape = (512, 512, 1, 11, 11, 3)
        src_pad_ext = np.lib.stride_tricks.sliding_window_view(src_pad, (11,11,3)).squeeze()
        ref_pad_ext = np.lib.stride_tricks.sliding_window_view(ref_pad, (11,11,3)).squeeze()

        # mu_src.shape = (512, 512, 3)
        # mu_src_win_ext.shape = (512, 512, 11, 11, 3)
        mu_src_win = np.expand_dims(mu_src, (2,3))
        mu_src_win_ext = np.tile(mu_src_win, (1, 1, 11, 11, 1))
        mu_ref_win = np.expand_dims(mu_ref, (2,3))
        mu_ref_win_ext = np.tile(mu_ref_win, (1, 1, 11, 11, 1))

        kernel_gaus_3d = np.concatenate((np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2)), 2)

        kernel_gaus_3d = np.expand_dims(kernel_gaus_3d, (0,1))
        kernel_gaus_3d_ext = np.tile(kernel_gaus_3d, (hi, wi, 1, 1, 1))

        src_pad_ext_norm = src_pad_ext - mu_src_win_ext
        ref_pad_ext_norm = ref_pad_ext - mu_ref_win_ext
        sig_src = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        sig_ref = np.sum(kernel_gaus_3d_ext * ref_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        cov = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm * ref_pad_ext_norm, axis=(2,3))

        l = (2 * mu_src * mu_ref + c1) / (mu_src ** 2 + mu_ref ** 2 + c1)
        c = (2 * sig_src * sig_ref + c2) / (sig_src ** 2 + sig_ref ** 2 + c2)
        s = (cov + c3) / (sig_src * sig_ref + c3)

        ssim_local = l ** alp * c ** bet * s ** gam
        mssim = np.sum(ssim_local, axis=(0,1)) / M
        mssim = mssim[0] * w_r + mssim[1] * w_g + mssim[2] * w_b

        return l, c, s

    # ------------------------------------------------------------------------------------------------------------------
    # 
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gssim(src, ref):
        src_img = src
        ref_img = ref

        l, _, _ = CSS.ssim(src_img, ref_img)

        src_h, src_w, src_c = src_img.shape
        ref_h, ref_w, ref_c = ref_img.shape

        # apply Sobel filter for each channel
        src_edge_mag = np.zeros((src_h, src_w, 0))
        ref_edge_mag = np.zeros((ref_h, ref_w, 0))
        for c in range(3):
            src_grad_x = cv2.Sobel(src_img[:,:,c], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            src_grad_y = cv2.Sobel(src_img[:,:,c], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            src_sobel_mag = np.sqrt(src_grad_x ** 2 + src_grad_y ** 2)
            src_edge_mag = np.concatenate((src_edge_mag, np.expand_dims(src_sobel_mag, 2)), 2)

            ref_grad_x = cv2.Sobel(ref_img[:,:,c], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            ref_grad_y = cv2.Sobel(ref_img[:,:,c], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            ref_sobel_mag = np.sqrt(ref_grad_x ** 2 + ref_grad_y ** 2)
            ref_edge_mag = np.concatenate((ref_edge_mag, np.expand_dims(ref_sobel_mag, 2)), 2)

        _, c, s = CSS.ssim(src_edge_mag, ref_edge_mag)

        M = src_img.shape[0] * src_img.shape[1]
        alp = bet = gam = 1.0
        w_r = w_g = w_b = 1.0/3.0
        gssim_local = l ** alp * c ** bet * s ** gam
        mgssim = np.sum(gssim_local, axis=(0,1)) / M
        mgssim = mgssim[0] * w_r + mgssim[1] * w_g + mgssim[2] * w_b

        return mgssim
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb2rgyb(img):
        rg = img[:,:,0] - img[:,:,1]
        yb = 0.5 * (img[:,:,0] + img[:,:,1]) - img[:,:,2]
        return rg, yb

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def colorfulness(img):
        rg, yb = CSS.rgb2rgyb(img * 255)

        mu_rg = np.mean(rg)
        mu_yb = np.mean(yb)

        sig_rg = np.std(rg)
        sig_yb = np.std(yb)

        sig_rgyb = math.sqrt(sig_rg ** 2 + sig_yb ** 2)
        mu_rgyb = math.sqrt(mu_rg ** 2 + mu_yb ** 2)

        C = sig_rgyb + 0.3 * mu_rgyb

        return C

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[1]
        out = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()
        out_img = out.get_raw()

        # calculate colorfulness similarity
        ref_cf = CSS.colorfulness(ref_img)
        out_cf = CSS.colorfulness(out_img)

        cs = np.abs(out_cf - ref_cf)

        # calculate GSSIM
        ss = CSS.gssim(out_img, src_img)

        # calculate colorfulness structure similarity
        A4 = 1.0
        css_val = (ss + A4) / (cs + A4)

        return round(css_val, 4)

ColorTransferLib/Evaluation/CF/init.py

from . import CF

ColorTransferLib/Evaluation/CTQM/CTQM.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Novel multi-color transfer algorithms and quality measure
#   Author: Karen Panetta, Long Bao, Sos Agaian
#   Published in: IEEE Transactions on Consumer Electronics
#   Year of Publication: 2016
#
# Abstract:
#   In this paper, two new image multi-color transfer algorithms for still images and image sequences are proposed. 
#   These methods can be used to capture the artistic ambience or "mood" of the source image and transfer that same 
#   color ambience to the target image. The performance and effectiveness of these new algorithms are demonstrated 
#   through simulations and comparisons to other state of the art methods, including Alla's, Reinhard's and Pitie's 
#   methods. These algorithms are straight-forward, automatic, and suitable for various practical recoloring 
#   applications, including coloring, color correction, animation and color restoration for imaging tools and consumer 
#   products. This work is also useful for fast implementation of special effects for the entertainment industry and 
#   reduces manual labor costs for these types of tasks. Another contribution of this paper is the introduction of a 
#   new color transfer quality measure. The new measure is highly consistent with human perception, even compared to 
#   other current color transfer quality measures such as Xiao's measure and Xiang's measure.
#
# Info:
#   Name: Color Transfer Quality Measure
#   Shortname: CTQM
#   Identifier: CTQM
#   Link: https://doi.org/10.1109/TCE.2016.7613196
#   Range: [0, ??]
#
# Implementation Details:
#   nan values can appear -> are ignored for evaluation
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class CTQM:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def iso2Dgauss(size=11, sig=1.5, normalized=True):
        kernel = np.zeros((size,size))
        shift = size // 2
        for x in range(size): 
            for y in range(size): 
                x_s = x - shift
                y_s = y - shift
                kernel[y, x] = (1.0 / (2 * math.pi * math.pow(sig, 2))) * np.exp( -(math.pow(x_s,2)+math.pow(y_s,2)) / (2*math.pow(sig,2)) )

        if normalized:
            kernel /= np.sum(kernel)
        return kernel

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def ssim(src, ref):
        kernel_gaus = CTQM.iso2Dgauss()

        src_img = src
        ref_img = ref

        hi, wi = src_img.shape

        k1 = 0.01
        k2 = 0.03
        L = 1.0
        N = 11
        M = src_img.shape[0] * src_img.shape[1]
        c1 = (k1 * L) ** 2
        c2 = (k2 * L) ** 2
        c3 = c2 / 2.0
        alp = 1.0
        bet = 1.0
        gam = 1.0

        mu_src = cv2.filter2D(src_img, -1, kernel_gaus)
        mu_ref = cv2.filter2D(ref_img, -1, kernel_gaus)

        src_pad = np.pad(src_img, ((5, 5), (5, 5)), 'reflect')
        ref_pad = np.pad(ref_img, ((5, 5), (5, 5)), 'reflect')

        sig_src = np.zeros_like(src_img)
        sig_ref = np.zeros_like(ref_img)
        cov = np.zeros_like(src_img)

        # Version 3
        # src_pad_ext.shape = (512, 512, 1, 11, 11, 3)
        src_pad_ext = np.lib.stride_tricks.sliding_window_view(src_pad, (11,11)).squeeze()
        ref_pad_ext = np.lib.stride_tricks.sliding_window_view(ref_pad, (11,11)).squeeze()

        # mu_src.shape = (512, 512)
        # mu_src_win_ext.shape = (512, 512, 11, 11)
        mu_src_win = np.expand_dims(mu_src, (2,3))
        mu_src_win_ext = np.tile(mu_src_win, (1, 1, 11, 11))
        mu_ref_win = np.expand_dims(mu_ref, (2,3))
        mu_ref_win_ext = np.tile(mu_ref_win, (1, 1, 11, 11))

        kernel_gaus = np.expand_dims(kernel_gaus, (0,1))
        kernel_gaus_ext = np.tile(kernel_gaus, (hi, wi, 1, 1))

        src_pad_ext_norm = src_pad_ext - mu_src_win_ext
        ref_pad_ext_norm = ref_pad_ext - mu_ref_win_ext
        sig_src = np.sum(kernel_gaus_ext * src_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        sig_ref = np.sum(kernel_gaus_ext * ref_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        cov = np.sum(kernel_gaus_ext * src_pad_ext_norm * ref_pad_ext_norm, axis=(2,3))

        l = (2 * mu_src * mu_ref + c1) / (mu_src ** 2 + mu_ref ** 2 + c1)
        c = (2 * sig_src * sig_ref + c2) / (sig_src ** 2 + sig_ref ** 2 + c2)
        s = (cov + c3) / (sig_src * sig_ref + c3)

        #ssim_local = l ** alp * c ** bet * s ** gam
        #mssim = np.sum(ssim_local, axis=(0,1)) / M

        return l, c, s
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gradient_img(img):
        grad_x = cv2.Sobel(img[:,:], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
        grad_y = cv2.Sobel(img[:,:], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
        sobel_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)
        return sobel_mag

    # ------------------------------------------------------------------------------------------------------------------
    # R1 regions: preserved edge pixel region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R1_region(ssim_loc, src_edge, ref_edge, T1):
        R1 = ssim_loc[:,:][(src_edge[:,:] > T1) & (ref_edge[:,:] > T1)]
        return R1

    # ------------------------------------------------------------------------------------------------------------------
    # R2 regions: changed edge pixel region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R2_region(ssim_loc, src_edge, ref_edge, T1):
        R2 = ssim_loc[:,:][(src_edge[:,:] > T1) & (ref_edge[:,:] <= T1) | 
                                (ref_edge[:,:] > T1) & (src_edge[:,:] <= T1)]
        return R2

    # ------------------------------------------------------------------------------------------------------------------
    # R3 regions: Smooth region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R3_region(ssim_loc, src_edge, ref_edge, T1, T2):
        R3 = ssim_loc[:,:][(src_edge[:,:] < T2) & (ref_edge[:,:] > T1)]
        return R3

    # ------------------------------------------------------------------------------------------------------------------
    # R4 regions: Rexture region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R4_region(ssim_loc, src_edge, ref_edge, T1, T2):
        R4 = ssim_loc[:,:][~((src_edge[:,:] > T1) & (ref_edge[:,:] > T1)) &
                                ~((src_edge[:,:] > T1) & (ref_edge[:,:] <= T1) | (ref_edge[:,:] > T1) & (src_edge[:,:] <= T1)) &
                                ~((src_edge[:,:] < T2) & (ref_edge[:,:] > T1))]
        return R4

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def ivegssim(src, ref):
        src_img = src
        ref_img = ref

        alp = bet = gam = 1.0

        # apply Sobel filter for each channel
        src_edge_mag = CTQM.gradient_img(src_img)
        ref_edge_mag = CTQM.gradient_img(ref_img)

        l, _, _ = CTQM.ssim(src_img, ref_img)
        _, c, s = CTQM.ssim(np.log(src_edge_mag+1), np.log(ref_edge_mag+1))
        ssim_local = l ** alp * c ** bet * s ** gam

        T1 = 0.12 * np.max(src_edge_mag)
        T2 = 0.06 * np.max(src_edge_mag)

        R1 = CTQM.R1_region(ssim_local, src_edge_mag, ref_edge_mag, T1)
        R2 = CTQM.R2_region(ssim_local, src_edge_mag, ref_edge_mag, T1)
        R3 = CTQM.R3_region(ssim_local, src_edge_mag, ref_edge_mag, T1, T2)
        R4 = CTQM.R4_region(ssim_local, src_edge_mag, ref_edge_mag, T1, T2)

        # setting of weights
        #           w1      w2      w3      w4
        # if R1={}  0.00    0.50    0.25    0.25
        # if R2={}  0.50    0.00    0.25    0.25
        # else      0.25    0.25    0.25    0.25
        ivssim_val = 0.0
        w3 = w4 = 0.25

        if R1.shape[0] == 0:
            w1 = 0.0
            w2 = 0.5
        elif R2.shape[0] == 0:
            w1 = 0.5
            w2 = 0.0
        else:
            w1 = 0.25
            w2 = 0.25

        if R1.shape[0] != 0.0:
            ivssim_val += np.sum(R1) / R1.shape[0] * w1
        if R2.shape[0] != 0.0:
            ivssim_val += np.sum(R2) / R2.shape[0] * w2
        if R3.shape[0] != 0.0:
            ivssim_val += np.sum(R3) / R3.shape[0] * w3
        if R4.shape[0] != 0.0:
            ivssim_val += np.sum(R4) / R4.shape[0] * w4

        return ivssim_val

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb2rgyb(img):
        rg = img[:,:,0] - img[:,:,1]
        yb = 0.5 * (img[:,:,0] + img[:,:,1]) - img[:,:,2]
        return rg, yb

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def colorfulness(img):
        rg, yb = CTQM.rgb2rgyb(img * 255)

        mu_rg = np.mean(rg)
        mu_yb = np.mean(yb)

        sig_rg = np.std(rg)
        sig_yb = np.std(yb)

        rg_val = np.log(sig_rg ** 2 / np.abs(mu_rg) ** 0.2)
        yb_val = np.log(sig_yb ** 2 / np.abs(mu_yb) ** 0.2)

        cf = 0.2 * rg_val * yb_val

        return cf
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def colormapsimilarity(ref, out):
        # round a and b channel to integers
        ref = np.around(ref).astype(np.int32)
        out = np.around(out).astype(np.int32)

        # reshape from (512, 512, 2) to (262144, 2)
        ref = ref.reshape(ref.shape[0] * ref.shape[1], ref.shape[2])
        out = out.reshape(out.shape[0] * out.shape[1], out.shape[2])

        # get normalized 2D histogram of a and b channel
        bins = [125, 125]
        #print(ref.shape)
        ref_histo = np.asarray(np.histogramdd(ref, bins)[0])
        out_histo = np.asarray(np.histogramdd(out, bins)[0])

        ref_histo /= np.sum(ref_histo)
        out_histo /= np.sum(out_histo)

        cms = -np.sum(np.abs(out_histo-ref_histo))

        return cms

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[1]
        out = args[2]
        # RGB to CIELab
        src_lab = cv2.cvtColor(src.get_raw(), cv2.COLOR_RGB2LAB)
        ref_lab = cv2.cvtColor(ref.get_raw(), cv2.COLOR_RGB2LAB)
        out_lab = cv2.cvtColor(out.get_raw(), cv2.COLOR_RGB2LAB)

        # Caluclate 4-EGSSIM of L channel
        # multiple with 2.55 to get range [0, 255] instead of [0, 100]
        ivegssim_val = CTQM.ivegssim(out_lab[:,:,0], src_lab[:,:,0])

        # Calculate Colorfulness
        cf_val = CTQM.colorfulness(out.get_raw())

        # Calculate Color Map Similarity
        cms_val = CTQM.colormapsimilarity(ref_lab[:,:,1:3], out_lab[:,:,1:3])

        # Calculate CTQM
        wo = 1.0
        ws = 100.0
        wm = 1.0

        ctqm_val = wo * cf_val + wm * cms_val + ws *ivegssim_val

        return round(ctqm_val, 4)

ColorTransferLib/Evaluation/Corr/Corr.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Corr
# ...
#
# Source: https://docs.opencv.org/3.4/d8/dc8/tutorial_histogram_comparison.html
#
# Range [-1, 1]
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Corr:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[1]
        ref = args[2]
        bins=[10,10,10]
        histo1 = src.get_color_statistic_3D(bins=bins, normalized=True)
        histo2 = ref.get_color_statistic_3D(bins=bins, normalized=True)

        histo1_m = np.mean(histo1)
        histo2_m = np.mean(histo2)

        histo1_shift = histo1 - histo1_m
        histo2_shift = histo2 - histo2_m

        nomi = np.sum(np.multiply(histo1_shift, histo2_shift))
        denom = np.sqrt(np.multiply(np.sum(np.power(histo1_shift, 2)),np.sum(np.power(histo2_shift, 2))))

        corr = nomi / denom

        return round(corr, 4)

ColorTransferLib/Evaluation/CTQM/init.py

from . import CTQM

ColorTransferLib/Evaluation/FSIM/init.py

from . import FSIM

ColorTransferLib/Evaluation/BRISQUE/BRISQUE.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import pyiqa
import torch

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: No-Reference Image Quality Assessment in the Spatial Domain
#   Author: Anish Mittal, Anush Krishna Moorthy, Alan Conrad Bovik
#   Published in: IEEE Transactions on Image Processing
#   Year of Publication: 2012
#
# Abstract:
#   We propose a natural scene statistic-based distortion-generic blind/no-reference (NR) image quality assessment 
#   (IQA) model that operates in the spatial domain. The new model, dubbed blind/referenceless image spatial quality 
#   evaluator (BRISQUE) does not compute distortion-specific features, such as ringing, blur, or blocking, but instead 
#   uses scene statistics of locally normalized luminance coefficients to quantify possible losses of â€œnaturalnessâ€ in 
#   the image due to the presence of distortions, thereby leading to a holistic measure of quality. The underlying 
#   features used derive from the empirical distribution of locally normalized luminances and products of locally 
#   normalized luminances under a spatial natural scene statistic model. No transformation to another coordinate frame 
#   (DCT, wavelet, etc.) is required, distinguishing it from prior NR IQA approaches. Despite its simplicity, we are 
#   able to show that BRISQUE is statistically better than the full-reference peak signal-to-noise ratio and the 
#   structural similarity index, and is highly competitive with respect to all present-day distortion-generic NR IQA 
#   algorithms. BRISQUE has very low computational complexity, making it well suited for real time applications. 
#   BRISQUE features may be used for distortion-identification as well. To illustrate a new practical application of 
#   BRISQUE, we describe how a nonblind image denoising algorithm can be augmented with BRISQUE in order to perform 
#   blind image denoising. Results show that BRISQUE augmentation leads to performance improvements over 
#   state-of-the-art methods. A software release of BRISQUE is available online: 
#   http://live.ece.utexas.edu/research/quality/BRISQUE_release.zip for public use and evaluation.
#
# Info:
#   Name: Blind/Referenceless Image Spatial Quality Evaluator
#   Shortname: BRISQUE
#   Identifier: BRISQUE
#   Link: https://doi.org/10.1109/TIP.2012.2214050
#   Range: [0, 100] with 100 = perfect quality
#
# Implementation Details:
#   from https://github.com/spmallick/learnopencv
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# ... (BRISQUE)
# 
#
# Source: https://github.com/spmallick/learnopencv
#
# Range []
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class BRISQUE:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        out = args[2]
        img = out.get_raw()

        img_ten = torch.from_numpy(img)
        img_ten = torch.swapaxes(img_ten, 1, 2)
        img_ten = torch.swapaxes(img_ten, 0, 1)
        img_ten = img_ten.unsqueeze(0)

        device = torch.device("cpu")
        iqa_metric = pyiqa.create_metric('brisque', device=device)
        score_nr = iqa_metric(img_ten)

        score = float(score_nr.cpu().detach().numpy())

        return round(score, 4)

ColorTransferLib/Evaluation/FSIM/FSIM.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import numpy as np
import phasepack.phasecong as PC

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: FSIM: A Feature Similarity Index for Image Quality Assessment
#   Author: Lin Zhang, Lei Zhang, Xuanqin Mou, David Zhang
#   Published in: IEEE Transactions on Image Processing 
#   Year of Publication: 2011
#
# Abstract:
#   Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with 
#   subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. 
#   In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that 
#   human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase 
#   congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary 
#   feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' 
#   perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and 
#   GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we 
#   use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six 
#   benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations 
#   than state-of-the-art IQA metrics.

# Info:
#   Name: Feature Similarity Index
#   Shortname: FSIM
#   Identifier: FSIM
#   Link: https://doi.org/10.1109/TIP.2011.2109730
#   Range: [0, 1]
#
# Implementation Details:
#   For calculating the phase congruency, the 'phasepack'-library from https://github.com/alimuldal/phasepack was used, 
#   which uses a butterworth filter instead of a gaussian.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class FSIM:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb2yiq(img):
        T_mat = np.array([[0.299, 0.587, 0.114], [0.596, -0.274, -0.322], [0.211, -0.523, 0.312]])
        img_yiq = np.dot(img.reshape(-1, 3), T_mat.transpose()).reshape(img.shape)
        return img_yiq

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def phase_congruency(img, c):
        # mult 2 leads to the gabor_scales = [1/6, 1/12, 1/24, 1/48]
        pc2d = PC(img[:, :, c], nscale=4, norient=4, minWaveLength=6, mult=2, sigmaOnf=0.5978)
        pc2d = np.sum(np.asarray(pc2d[4]), axis=0)
        return pc2d

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gradient_magnitude(img, c):
        src_sharr_x = cv2.Sobel(img[:, :, c], cv2.CV_64F, 1, 0, ksize=3)
        src_sharr_y = cv2.Sobel(img[:, :, c], cv2.CV_64F, 0, 1, ksize=3)
        src_sharr = np.sqrt(np.power(src_sharr_x, 2) + np.power(src_sharr_y, 2))
        return src_sharr

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def similarity_measure(src_pc2d, ref_pc2d, T1):
        SPC = (2 * src_pc2d * ref_pc2d + T1) / (np.power(src_pc2d, 2) + np.power(ref_pc2d, 2) + T1)
        return SPC

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def combine_similarity_measures(SPC, SG):
        alpha = beta = 1.0
        SL = np.power(SPC, alpha) * np.power(SG, beta)
        return SL 

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def fsim_idx(src_pc2d, ref_pc2d, SL, S_C):
        PC_m = np.maximum(src_pc2d, ref_pc2d)
        fsim_val = np.sum(SL * np.power(S_C, 0.03) * PC_m) / np.sum(PC_m)
        return fsim_val 

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        # 1. RGB -> YIQ
        src_yiq = FSIM.rgb2yiq(src_img)
        ref_yiq = FSIM.rgb2yiq(ref_img)

        # 1. calculate phase congruency (PC)
        src_pc2d_y = FSIM.phase_congruency(src_yiq, 0)
        ref_pc2d_y = FSIM.phase_congruency(ref_yiq, 0)

        # 2. calculate gradient magnitude (GM)
        src_sharr_y = FSIM.gradient_magnitude(src_yiq, 0)
        ref_sharr_y = FSIM.gradient_magnitude(ref_yiq, 0)

        # 3. Similarity measure (PC)
        SPC_y = FSIM.similarity_measure(src_pc2d_y, ref_pc2d_y, 0.85)

        # 4. Similarity measure (GM)
        SG_y = FSIM.similarity_measure(src_sharr_y, ref_sharr_y, 160.0)

        # 5. Combined similarity measure
        SL_y = FSIM.combine_similarity_measures(SPC_y, SG_y)

        # 4. Similarity measure (Chromatic)
        S_i = FSIM.similarity_measure(src_yiq[:,:,1], ref_yiq[:,:,1], 200.0)
        S_q = FSIM.similarity_measure(src_yiq[:,:,2], ref_yiq[:,:,2], 200.0)
        S_C = S_i * S_q

        # 6. FSIM
        fsim_val = FSIM.fsim_idx(src_pc2d_y, ref_pc2d_y, SL_y, S_C)

        return round(fsim_val, 4)

ColorTransferLib/Evaluation/GSSIM/init.py

from . import GSSIM

ColorTransferLib/Evaluation/IVEGSSIM/IVEGSSIM.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Structural similarity index measure (SSIM)
# Measuring the perceived quality of an image regarding an original (uncompressed and distortion-free) image.
#
# Source: Image quality assessment: from error visibility to structural similarity
#
# Range [-1, 1]
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class IVEGSSIM:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def iso2Dgauss(size=11, sig=1.5, normalized=True):
        kernel = np.zeros((size,size))
        shift = size // 2
        for x in range(size): 
            for y in range(size): 
                x_s = x - shift
                y_s = y - shift
                kernel[y, x] = (1.0 / (2 * math.pi * math.pow(sig, 2))) * np.exp( -(math.pow(x_s,2)+math.pow(y_s,2)) / (2*math.pow(sig,2)) )

        if normalized:
            kernel /= np.sum(kernel)
        return kernel

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def ssim(src, ref):
        kernel_gaus = IVEGSSIM.iso2Dgauss()

        src_img = src
        ref_img = ref

        hi, wi, ci = src_img.shape

        k1 = 0.01
        k2 = 0.03
        L = 1.0
        N = 11
        M = src_img.shape[0] * src_img.shape[1]
        c1 = (k1 * L) ** 2
        c2 = (k2 * L) ** 2
        c3 = c2 / 2.0
        alp = 1.0
        bet = 1.0
        gam = 1.0
        w_r = 1.0/3.0
        w_g = 1.0/3.0
        w_b = 1.0/3.0

        mu_src = cv2.filter2D(src_img, -1, kernel_gaus)
        mu_ref = cv2.filter2D(ref_img, -1, kernel_gaus)

        src_pad = np.pad(src_img, ((5, 5), (5, 5), (0, 0)), 'reflect')
        ref_pad = np.pad(ref_img, ((5, 5), (5, 5), (0, 0)), 'reflect')

        sig_src = np.zeros_like(src_img)
        sig_ref = np.zeros_like(ref_img)
        cov = np.zeros_like(src_img)

        # Version 3
        # src_pad_ext.shape = (512, 512, 1, 11, 11, 3)
        src_pad_ext = np.lib.stride_tricks.sliding_window_view(src_pad, (11,11,3)).squeeze()
        ref_pad_ext = np.lib.stride_tricks.sliding_window_view(ref_pad, (11,11,3)).squeeze()

        # mu_src.shape = (512, 512, 3)
        # mu_src_win_ext.shape = (512, 512, 11, 11, 3)
        mu_src_win = np.expand_dims(mu_src, (2,3))
        mu_src_win_ext = np.tile(mu_src_win, (1, 1, 11, 11, 1))
        mu_ref_win = np.expand_dims(mu_ref, (2,3))
        mu_ref_win_ext = np.tile(mu_ref_win, (1, 1, 11, 11, 1))

        kernel_gaus_3d = np.concatenate((np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2)), 2)

        kernel_gaus_3d = np.expand_dims(kernel_gaus_3d, (0,1))
        kernel_gaus_3d_ext = np.tile(kernel_gaus_3d, (hi, wi, 1, 1, 1))

        src_pad_ext_norm = src_pad_ext - mu_src_win_ext
        ref_pad_ext_norm = ref_pad_ext - mu_ref_win_ext
        sig_src = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        sig_ref = np.sum(kernel_gaus_3d_ext * ref_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        cov = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm * ref_pad_ext_norm, axis=(2,3))

        l = (2 * mu_src * mu_ref + c1) / (mu_src ** 2 + mu_ref ** 2 + c1)
        c = (2 * sig_src * sig_ref + c2) / (sig_src ** 2 + sig_ref ** 2 + c2)
        s = (cov + c3) / (sig_src * sig_ref + c3)

        ssim_local = l ** alp * c ** bet * s ** gam
        mssim = np.sum(ssim_local, axis=(0,1)) / M
        mssim = mssim[0] * w_r + mssim[1] * w_g + mssim[2] * w_b

        return l, c, s
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gradient_img(img):
        img_h, img_w, img_c = img.shape
        edge_mag = np.zeros((img_h, img_w, 0))
        for c in range(3):
            grad_x = cv2.Sobel(img[:,:,c], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            grad_y = cv2.Sobel(img[:,:,c], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            sobel_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)
            edge_mag = np.concatenate((edge_mag, np.expand_dims(sobel_mag, 2)), 2)
        return edge_mag

    # ------------------------------------------------------------------------------------------------------------------
    # R1 regions: preserved edge pixel region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R1_region(ssim_loc, src_edge, ref_edge, T1):
        #ssim_r1 = ()
        R1 = ()
        for c in range(3):
            R1_c = ssim_loc[:,:,c][(src_edge[:,:,c] > T1) & (ref_edge[:,:,c] > T1)]
            R1 = R1 + (R1_c,)
        return R1

    # ------------------------------------------------------------------------------------------------------------------
    # R2 regions: changed edge pixel region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R2_region(ssim_loc, src_edge, ref_edge, T1):
        R2 = ()
        for c in range(3):
            R2_c = ssim_loc[:,:,c][(src_edge[:,:,c] > T1) & (ref_edge[:,:,c] <= T1) | 
                                   (ref_edge[:,:,c] > T1) & (src_edge[:,:,c] <= T1)]
            R2 = R2 + (R2_c,)
        return R2

    # ------------------------------------------------------------------------------------------------------------------
    # R3 regions: Smooth region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R3_region(ssim_loc, src_edge, ref_edge, T1, T2):
        R3 = ()
        for c in range(3):
            R3_c = ssim_loc[:,:,c][(src_edge[:,:,c] < T2) & (ref_edge[:,:,c] > T1)]
            R3 = R3 + (R3_c,)
        return R3

    # ------------------------------------------------------------------------------------------------------------------
    # R4 regions: Rexture region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R4_region(ssim_loc, src_edge, ref_edge, T1, T2):
        R4 = ()
        for c in range(3):
            R4_c = ssim_loc[:,:,c][~((src_edge[:,:,c] > T1) & (ref_edge[:,:,c] > T1)) &
                                   ~((src_edge[:,:,c] > T1) & (ref_edge[:,:,c] <= T1) | (ref_edge[:,:,c] > T1) & (src_edge[:,:,c] <= T1)) &
                                   ~((src_edge[:,:,c] < T2) & (ref_edge[:,:,c] > T1))]
            R4 = R4 + (R4_c,)
        return R4

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        alp = bet = gam = 1.0

        src_img = src.get_raw()
        ref_img = ref.get_raw()

        # apply Sobel filter for each channel
        src_edge_mag = IVEGSSIM.gradient_img(src_img)
        ref_edge_mag = IVEGSSIM.gradient_img(ref_img)

        l, _, _ = IVEGSSIM.ssim(src_img, ref_img)
        _, c, s = IVEGSSIM.ssim(np.log(src_edge_mag+1), np.log(ref_edge_mag+1))
        ssim_local = l ** alp * c ** bet * s ** gam

        T1 = 0.12 * np.max(src_edge_mag)
        T2 = 0.06 * np.max(src_edge_mag)

        R1 = IVEGSSIM.R1_region(ssim_local, src_edge_mag, ref_edge_mag, T1)
        R2 = IVEGSSIM.R2_region(ssim_local, src_edge_mag, ref_edge_mag, T1)
        R3 = IVEGSSIM.R3_region(ssim_local, src_edge_mag, ref_edge_mag, T1, T2)
        R4 = IVEGSSIM.R4_region(ssim_local, src_edge_mag, ref_edge_mag, T1, T2)

        # setting of weights
        #           w1      w2      w3      w4
        # if R1={}  0.00    0.50    0.25    0.25
        # if R2={}  0.50    0.00    0.25    0.25
        # else      0.25    0.25    0.25    0.25
        ivssim_val = np.zeros(3)
        w3 = w4 = 0.25
        for c in range(3):
            if R1[c].shape[0] == 0:
                 w1 = 0.0
                 w2 = 0.5
            elif R2[c].shape[0] == 0:
                 w1 = 0.5
                 w2 = 0.0
            else:
                 w1 = 0.25
                 w2 = 0.25

            if R1[c].shape[0] != 0.0:
                ivssim_val[c] += np.sum(R1[c]) / R1[c].shape[0] * w1
            if R2[c].shape[0] != 0.0:
                ivssim_val[c] += np.sum(R2[c]) / R2[c].shape[0] * w2
            if R3[c].shape[0] != 0.0:
                ivssim_val[c] += np.sum(R3[c]) / R3[c].shape[0] * w3
            if R4[c].shape[0] != 0.0:
                ivssim_val[c] += np.sum(R4[c]) / R4[c].shape[0] * w4

        w_r = w_g = w_b = 1.0/3.0
        mivssim = ivssim_val[0] * w_r + ivssim_val[1] * w_g + ivssim_val[2] * w_b

        return round(mivssim, 4)

ColorTransferLib/Evaluation/HI/HI.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: -
#   Author: -
#   Published in: - 
#   Year of Publication: -
#
# Abstract:
#   -

# Info:
#   Name: Histogram Intersection
#   Shortname: HI
#   Identifier: HI
#   Link: -
#   Range: [0, 1] with 1 = perfect similarity
#
# Misc:
#   Formula from https://docs.opencv.org/3.4/d8/dc8/tutorial_histogram_comparison.html
#
# Implementation Details:
#   usage of $10x10x10$ bins
#   in RGB color space
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class HI:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[1]
        ref = args[2]
        bins=[10,10,10]
        histo1 = src.get_color_statistic_3D(bins=bins, normalized=True)
        histo2 = ref.get_color_statistic_3D(bins=bins, normalized=True)
        minimum = np.minimum(histo1, histo2)
        intersection = np.sum(minimum)
        return round(intersection, 4)

ColorTransferLib/Evaluation/HI/init.py

from . import HI

ColorTransferLib/Evaluation/IVEGSSIM/init.py

from . import IVEGSSIM

ColorTransferLib/Evaluation/LPIPS/LPIPS.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import lpips
import torch
import sys
import os

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
#   Author: Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang
#   Published in: IEEE Computer Vision and Pattern Recognition Conference
#   Year of Publication: 2018
#
# Abstract:
#   While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the 
#   underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, 
#   such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. 
#   Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification 
#   has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called 
#   "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new 
#   dataset of human perceptual similarity judgments. We systematically evaluate deep features across different 
#   architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous 
#   metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG 
#   features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or 
#   even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep 
#   visual representations.
#
# Info:
#   Name: Learned Perceptual Image Patch Simiality
#   Shortname: LPIPS
#   Identifier: LPIPS
#   Link: https://doi.org/10.1109/TIP.2011.2109730
#   Source: https://github.com/richzhang/PerceptualSimilarity
#   Range: [0, ?] with 0 = perfect similarity
#
# Implementation Details:
#   Implementation from https://github.com/richzhang/PerceptualSimilarity
#   AlexNet was used
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class LPIPS:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        # image should be RGB, IMPORTANT: normalized to [-1,1]
        src_img_norm = torch.from_numpy(src_img * 2 - 1)
        ref_img_norm = torch.from_numpy(ref_img * 2 - 1)

        src_img_norm = torch.swapaxes(src_img_norm, 1, 2)
        src_img_norm = torch.swapaxes(src_img_norm, 0, 1)
        src_img_norm = src_img_norm.unsqueeze(0)

        ref_img_norm = torch.swapaxes(ref_img_norm, 1, 2)
        ref_img_norm = torch.swapaxes(ref_img_norm, 0, 1)
        ref_img_norm = ref_img_norm.unsqueeze(0)

        # prevent printing
        old_stdout = sys.stdout # backup current stdout
        sys.stdout = open(os.devnull, "w")

        loss_fn_alex = lpips.LPIPS(net='alex') # best forward scores
        lp = loss_fn_alex(src_img_norm, ref_img_norm).detach().numpy().squeeze()

        sys.stdout = old_stdout # reset old stdout

        return round(float(lp), 4)

ColorTransferLib/Evaluation/IVSSIM/IVSSIM.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np
import sys

#sys.path.insert(0, '/home/potechius/Projects/VSCode/ColorTransferLib/')

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Structural similarity index measure (SSIM)
# Measuring the perceived quality of an image regarding an original (uncompressed and distortion-free) image.
#
# Source: Image quality assessment: from error visibility to structural similarity
#
# Range [-1, 1]
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class IVSSIM:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def iso2Dgauss(size=11, sig=1.5, normalized=True):
        kernel = np.zeros((size,size))
        shift = size // 2
        for x in range(size): 
            for y in range(size): 
                x_s = x - shift
                y_s = y - shift
                kernel[y, x] = (1.0 / (2 * math.pi * math.pow(sig, 2))) * np.exp( -(math.pow(x_s,2)+math.pow(y_s,2)) / (2*math.pow(sig,2)) )

        if normalized:
            kernel /= np.sum(kernel)
        return kernel

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def ssim(src, ref):
        kernel_gaus = IVSSIM.iso2Dgauss()

        src_img = src
        ref_img = ref

        hi, wi, ci = src_img.shape

        k1 = 0.01
        k2 = 0.03
        L = 1.0
        N = 11
        M = src_img.shape[0] * src_img.shape[1]
        c1 = (k1 * L) ** 2
        c2 = (k2 * L) ** 2
        c3 = c2 / 2.0
        alp = 1.0
        bet = 1.0
        gam = 1.0
        w_r = 1.0/3.0
        w_g = 1.0/3.0
        w_b = 1.0/3.0

        mu_src = cv2.filter2D(src_img, -1, kernel_gaus)
        mu_ref = cv2.filter2D(ref_img, -1, kernel_gaus)

        src_pad = np.pad(src_img, ((5, 5), (5, 5), (0, 0)), 'reflect')
        ref_pad = np.pad(ref_img, ((5, 5), (5, 5), (0, 0)), 'reflect')

        sig_src = np.zeros_like(src_img)
        sig_ref = np.zeros_like(ref_img)
        cov = np.zeros_like(src_img)

        # Version 3
        # src_pad_ext.shape = (512, 512, 1, 11, 11, 3)
        src_pad_ext = np.lib.stride_tricks.sliding_window_view(src_pad, (11,11,3)).squeeze()
        ref_pad_ext = np.lib.stride_tricks.sliding_window_view(ref_pad, (11,11,3)).squeeze()

        # mu_src.shape = (512, 512, 3)
        # mu_src_win_ext.shape = (512, 512, 11, 11, 3)
        mu_src_win = np.expand_dims(mu_src, (2,3))
        mu_src_win_ext = np.tile(mu_src_win, (1, 1, 11, 11, 1))
        mu_ref_win = np.expand_dims(mu_ref, (2,3))
        mu_ref_win_ext = np.tile(mu_ref_win, (1, 1, 11, 11, 1))

        kernel_gaus_3d = np.concatenate((np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2)), 2)

        kernel_gaus_3d = np.expand_dims(kernel_gaus_3d, (0,1))
        kernel_gaus_3d_ext = np.tile(kernel_gaus_3d, (hi, wi, 1, 1, 1))

        src_pad_ext_norm = src_pad_ext - mu_src_win_ext
        ref_pad_ext_norm = ref_pad_ext - mu_ref_win_ext
        sig_src = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        sig_ref = np.sum(kernel_gaus_3d_ext * ref_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        cov = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm * ref_pad_ext_norm, axis=(2,3))

        l = (2 * mu_src * mu_ref + c1) / (mu_src ** 2 + mu_ref ** 2 + c1)
        c = (2 * sig_src * sig_ref + c2) / (sig_src ** 2 + sig_ref ** 2 + c2)
        s = (cov + c3) / (sig_src * sig_ref + c3)

        ssim_local = l ** alp * c ** bet * s ** gam
        mssim = np.sum(ssim_local, axis=(0,1)) / M
        mssim = mssim[0] * w_r + mssim[1] * w_g + mssim[2] * w_b

        return l, c, s
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def gradient_img(img):
        img_h, img_w, img_c = img.shape
        edge_mag = np.zeros((img_h, img_w, 0))
        for c in range(3):
            grad_x = cv2.Sobel(img[:,:,c], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            grad_y = cv2.Sobel(img[:,:,c], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            sobel_mag = np.sqrt(grad_x ** 2 + grad_y ** 2)
            edge_mag = np.concatenate((edge_mag, np.expand_dims(sobel_mag, 2)), 2)
        return edge_mag

    # ------------------------------------------------------------------------------------------------------------------
    # R1 regions: preserved edge pixel region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R1_region(ssim_loc, src_edge, ref_edge, T1):
        R1 = ()
        for c in range(3):
            R1_c = ssim_loc[:,:,c][(src_edge[:,:,c] > T1) & (ref_edge[:,:,c] > T1)]
            R1 = R1 + (R1_c,)
        return R1

    # ------------------------------------------------------------------------------------------------------------------
    # R2 regions: changed edge pixel region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R2_region(ssim_loc, src_edge, ref_edge, T1):
        R2 = ()
        for c in range(3):
            R2_c = ssim_loc[:,:,c][(src_edge[:,:,c] > T1) & (ref_edge[:,:,c] <= T1) | 
                                   (ref_edge[:,:,c] > T1) & (src_edge[:,:,c] <= T1)]
            R2 = R2 + (R2_c,)
        return R2

    # ------------------------------------------------------------------------------------------------------------------
    # R3 regions: Smooth region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R3_region(ssim_loc, src_edge, ref_edge, T1, T2):
        R3 = ()
        for c in range(3):
            R3_c = ssim_loc[:,:,c][(src_edge[:,:,c] < T2) & (ref_edge[:,:,c] > T1)]
            R3 = R3 + (R3_c,)
        return R3

    # ------------------------------------------------------------------------------------------------------------------
    # R4 regions: Rexture region
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def R4_region(ssim_loc, src_edge, ref_edge, T1, T2):
        R4 = ()
        for c in range(3):
            R4_c = ssim_loc[:,:,c][~((src_edge[:,:,c] > T1) & (ref_edge[:,:,c] > T1)) &
                                   ~((src_edge[:,:,c] > T1) & (ref_edge[:,:,c] <= T1) | (ref_edge[:,:,c] > T1) & (src_edge[:,:,c] <= T1)) &
                                   ~((src_edge[:,:,c] < T2) & (ref_edge[:,:,c] > T1))]
            R4 = R4 + (R4_c,)
        return R4

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        alp = bet = gam = 1.0

        l, c, s = IVSSIM.ssim(src_img, ref_img)
        ssim_local = l ** alp * c ** bet * s ** gam

        src_img = src.get_raw()
        ref_img = ref.get_raw()

        # apply Sobel filter for each channel
        src_edge_mag = IVSSIM.gradient_img(src_img)
        ref_edge_mag = IVSSIM.gradient_img(ref_img)

        T1 = 0.12 * np.max(src_edge_mag)
        T2 = 0.06 * np.max(src_edge_mag)

        R1 = IVSSIM.R1_region(ssim_local, src_edge_mag, ref_edge_mag, T1)
        R2 = IVSSIM.R2_region(ssim_local, src_edge_mag, ref_edge_mag, T1)
        R3 = IVSSIM.R3_region(ssim_local, src_edge_mag, ref_edge_mag, T1, T2)
        R4 = IVSSIM.R4_region(ssim_local, src_edge_mag, ref_edge_mag, T1, T2)

        # setting of weights
        #           w1      w2      w3      w4
        # if R1={}  0.00    0.50    0.25    0.25
        # if R2={}  0.50    0.00    0.25    0.25
        # else      0.25    0.25    0.25    0.25
        ivssim_val = np.zeros(3)
        w3 = w4 = 0.25
        for c in range(3):
            if R1[c].shape[0] == 0:
                 w1 = 0.0
                 w2 = 0.5
            elif R2[c].shape[0] == 0:
                 w1 = 0.5
                 w2 = 0.0
            else:
                 w1 = 0.25
                 w2 = 0.25

            ivssim_val[c] = 0
            if R1[c].shape[0] != 0:
                ivssim_val[c] += np.sum(R1[c]) / R1[c].shape[0] * w1
            if R2[c].shape[0] != 0:
                ivssim_val[c] += np.sum(R2[c]) / R2[c].shape[0] * w2
            if R3[c].shape[0] != 0:
                ivssim_val[c] += np.sum(R3[c]) / R3[c].shape[0] * w3
            if R4[c].shape[0] != 0:
                ivssim_val[c] += np.sum(R4[c]) / R4[c].shape[0] * w4

        w_r = w_g = w_b = 1.0/3.0
        mivssim = ivssim_val[0] * w_r + ivssim_val[1] * w_g + ivssim_val[2] * w_b

        return round(mivssim, 4)

ColorTransferLib/Evaluation/MSE/MSE.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np
from ColorTransferLib.ImageProcessing.Image import Image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Mean Square Error (MSE)
# 
#
# Source: 
#
# Range []
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class MSE:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        num_pix = src_img.shape[0] * src_img.shape[1]

        mse_c = np.sum(np.power(np.subtract(src_img, ref_img), 2), axis=(0,1)) / num_pix
        mse = np.sum(mse_c) / 3

        return round(mse, 4)

ColorTransferLib/Evaluation/Corr/init.py

from . import Corr

ColorTransferLib/Evaluation/IVSSIM/init.py

from . import IVSSIM

ColorTransferLib/Evaluation/MSE/init.py

from . import MSE

ColorTransferLib/Evaluation/GSSIM/GSSIM.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Structural similarity index measure (SSIM)
# Measuring the perceived quality of an image regarding an original (uncompressed and distortion-free) image.
#
# Source: Image quality assessment: from error visibility to structural similarity
#
# Range [-1, 1]
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class GSSIM:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def iso2Dgauss(size=11, sig=1.5, normalized=True):
        kernel = np.zeros((size,size))
        shift = size // 2
        for x in range(size): 
            for y in range(size): 
                x_s = x - shift
                y_s = y - shift
                kernel[y, x] = (1.0 / (2 * math.pi * math.pow(sig, 2))) * np.exp( -(math.pow(x_s,2)+math.pow(y_s,2)) / (2*math.pow(sig,2)) )

        if normalized:
            kernel /= np.sum(kernel)
        return kernel

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def ssim(src, ref):
        kernel_gaus = GSSIM.iso2Dgauss()

        src_img = src
        ref_img = ref

        hi, wi, ci = src_img.shape

        k1 = 0.01
        k2 = 0.03
        L = 1.0
        N = 11
        M = src_img.shape[0] * src_img.shape[1]
        c1 = (k1 * L) ** 2
        c2 = (k2 * L) ** 2
        c3 = c2 / 2.0
        alp = 1.0
        bet = 1.0
        gam = 1.0
        w_r = 1.0/3.0
        w_g = 1.0/3.0
        w_b = 1.0/3.0

        mu_src = cv2.filter2D(src_img, -1, kernel_gaus)
        mu_ref = cv2.filter2D(ref_img, -1, kernel_gaus)

        src_pad = np.pad(src_img, ((5, 5), (5, 5), (0, 0)), 'reflect')
        ref_pad = np.pad(ref_img, ((5, 5), (5, 5), (0, 0)), 'reflect')

        sig_src = np.zeros_like(src_img)
        sig_ref = np.zeros_like(ref_img)
        cov = np.zeros_like(src_img)

        # Version 3
        # src_pad_ext.shape = (512, 512, 1, 11, 11, 3)
        src_pad_ext = np.lib.stride_tricks.sliding_window_view(src_pad, (11,11,3)).squeeze()
        ref_pad_ext = np.lib.stride_tricks.sliding_window_view(ref_pad, (11,11,3)).squeeze()

        # mu_src.shape = (512, 512, 3)
        # mu_src_win_ext.shape = (512, 512, 11, 11, 3)
        mu_src_win = np.expand_dims(mu_src, (2,3))
        mu_src_win_ext = np.tile(mu_src_win, (1, 1, 11, 11, 1))
        mu_ref_win = np.expand_dims(mu_ref, (2,3))
        mu_ref_win_ext = np.tile(mu_ref_win, (1, 1, 11, 11, 1))

        kernel_gaus_3d = np.concatenate((np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2), np.expand_dims(kernel_gaus, 2)), 2)

        kernel_gaus_3d = np.expand_dims(kernel_gaus_3d, (0,1))
        kernel_gaus_3d_ext = np.tile(kernel_gaus_3d, (hi, wi, 1, 1, 1))

        src_pad_ext_norm = src_pad_ext - mu_src_win_ext
        ref_pad_ext_norm = ref_pad_ext - mu_ref_win_ext
        sig_src = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        sig_ref = np.sum(kernel_gaus_3d_ext * ref_pad_ext_norm ** 2, axis=(2,3)) ** 0.5
        cov = np.sum(kernel_gaus_3d_ext * src_pad_ext_norm * ref_pad_ext_norm, axis=(2,3))

        l = (2 * mu_src * mu_ref + c1) / (mu_src ** 2 + mu_ref ** 2 + c1)
        c = (2 * sig_src * sig_ref + c2) / (sig_src ** 2 + sig_ref ** 2 + c2)
        s = (cov + c3) / (sig_src * sig_ref + c3)

        ssim_local = l ** alp * c ** bet * s ** gam
        mssim = np.sum(ssim_local, axis=(0,1)) / M
        mssim = mssim[0] * w_r + mssim[1] * w_g + mssim[2] * w_b

        return l, c, s

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        l, _, _ = GSSIM.ssim(src_img, ref_img)

        src_img = src.get_raw()
        ref_img = ref.get_raw()

        src_h, src_w, src_c = src_img.shape
        ref_h, ref_w, ref_c = ref_img.shape

        # apply Sobel filter for each channel
        src_edge_mag = np.zeros((src_h, src_w, 0))
        ref_edge_mag = np.zeros((ref_h, ref_w, 0))
        for c in range(3):
            src_grad_x = cv2.Sobel(src_img[:,:,c], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            src_grad_y = cv2.Sobel(src_img[:,:,c], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            src_sobel_mag = np.sqrt(src_grad_x ** 2 + src_grad_y ** 2)
            src_edge_mag = np.concatenate((src_edge_mag, np.expand_dims(src_sobel_mag, 2)), 2)

            ref_grad_x = cv2.Sobel(ref_img[:,:,c], cv2.CV_32F, 1, 0, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            ref_grad_y = cv2.Sobel(ref_img[:,:,c], cv2.CV_32F, 0, 1, ksize=3, scale=1, delta=0, borderType=cv2.BORDER_DEFAULT)
            ref_sobel_mag = np.sqrt(ref_grad_x ** 2 + ref_grad_y ** 2)
            ref_edge_mag = np.concatenate((ref_edge_mag, np.expand_dims(ref_sobel_mag, 2)), 2)

        _, c, s = GSSIM.ssim(src_edge_mag, ref_edge_mag)

        M = src_img.shape[0] * src_img.shape[1]
        alp = bet = gam = 1.0
        w_r = w_g = w_b = 1.0/3.0
        gssim_local = l ** alp * c ** bet * s ** gam
        mgssim = np.sum(gssim_local, axis=(0,1)) / M
        mgssim = mgssim[0] * w_r + mgssim[1] * w_g + mgssim[2] * w_b

        return round(mgssim, 4)

ColorTransferLib/Evaluation/MSSSIM/init.py

from . import MSSSIM

ColorTransferLib/Evaluation/MSSSIM/MSSSIM.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import torch
from torchmetrics import MultiScaleStructuralSimilarityIndexMeasure

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Multi-scale Structural Similarity for Image Quality Assessment
#   Author: Zhou Wang, Eero P. Simoncelli, Alan C. Bovik
#   Published in: IEEE Asilomar Conference on Signals, Systems and Computers
#   Year of Publication: 2003
#
# Abstract:
#   The structural similarity image quality assessment approach is based on the assumption that the human visual system 
#   is highly adapted for extracting structural information from the scene, and therefore a measure of structural 
#   similarity can provide a good approximation to perceived image quality. This paper proposes a novel multi-scale 
#   structural similarity method, which supplies more flexibility than single-scale methods in incorporating the 
#   variations of image resolution and viewing condition. Experimental comparisons demonstrate the effectiveness of the 
#   proposed method.
#
# Info:
#   Name: Multi-scale Structural Similarity
#   Shotname: MS-SSIM
#   Identifier: MSSSIM
#   Link: https://doi.org/10.1109/ACSSC.2003.1292216
#   Range: [0, 1]
#
# Implementation Details:
#   implementation from torchmetrics
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class MSSSIM:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0)
        ten_src = torch.from_numpy(src.get_raw())
        ten_ref = torch.from_numpy(ref.get_raw())

        ten_src = torch.swapaxes(ten_src, 0, 2)
        ten_src = torch.swapaxes(ten_src, 1, 2)
        ten_src = torch.unsqueeze(ten_src, 0)

        ten_ref = torch.swapaxes(ten_ref, 0, 2)
        ten_ref = torch.swapaxes(ten_ref, 1, 2)
        ten_ref = torch.unsqueeze(ten_ref, 0)

        ms_val = ms_ssim(ten_src, ten_ref)
        ms_val = float(ms_val.numpy())

        return round(ms_val, 4)

ColorTransferLib/Evaluation/NIMA/init.py

from . import NIMA

ColorTransferLib/Evaluation/NIMA/LICENSE.txt

Copyright 2018 idealo internet GmbH. All rights reserved.

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

ColorTransferLib/Evaluation/LPIPS/init.py

from . import LPIPS

ColorTransferLib/Evaluation/NIMA/NIMA.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import copy
import tensorflow as tf

from .predict import predict
from .utils.utils import calc_mean_score
from .handlers.model_builder import Nima

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: NIMA: Neural Image Assessment
#   Author: Hossein Talebi, Peyman Milanfar
#   Published in: IEEE Transactions on Image Processing
#   Year of Publication: 2018
#
# Abstract:
#   Automatically learned quality assessment for images has recently become a hot topic due to its usefulness in a wide 
#   variety of applications, such as evaluating image capture pipelines, storage techniques, and sharing media. Despite 
#   the subjective nature of this problem, most existing methods only predict the mean opinion score provided by data 
#   sets, such as AVA and TID2013. Our approach differs from others in that we predict the distribution of human opinion 
#   scores using a convolutional neural network. Our architecture also has the advantage of being significantly simpler 
#   than other methods with comparable performance. Our proposed approach relies on the success (and retraining) of 
#   proven, state-of-the-art deep object recognition networks. Our resulting network can be used to not only score 
#   images reliably and with high correlation to human perception, but also to assist with adaptation and optimization 
#   of photo editing/enhancement algorithms in a photographic pipeline. All this is done without need for a â€œgoldenâ€ 
#   reference image, consequently allowing for single-image, semantic- and perceptually-aware, no-reference quality 
#   assessment.
#
# Info:
#   Name: Neural Image Assessment
#   Shortname: NIMA
#   Identifier: NIMA
#   Link: https://doi.org/10.1109/TIP.2018.2831899
#   Source: https://github.com/idealo/image-quality-assessment
#   Range: [0, 10] with 10 = best quality
#
# Implementation Details:
#   Usage of MobileNet
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class NIMA:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        img = args[2]
        imcopy = copy.deepcopy(img)
        imcopy.resize(224, 224)
        imcopy = imcopy.get_raw()

        base_model_name = "MobileNet"
        weights_file = "Models/NIMA/MobileNet/weights_mobilenet_aesthetic_0.07.hdf5"

        # build model and load weights
        nima = Nima(base_model_name, weights=None)
        nima.build()
        nima.nima_model.load_weights(weights_file)

        # get predictions
        predictions = predict(nima.nima_model, imcopy)
        nim = calc_mean_score(predictions[0])

        tf.keras.backend.clear_session()

        return round(nim, 4)

ColorTransferLib/Evaluation/NIMA/handlers/data_generator.py

import os
import numpy as np
import tensorflow as tf
from ..utils import utils

class TestDataGenerator(tf.keras.utils.Sequence):
    '''inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator'''
    def __init__(self, samples, img_dir, batch_size, n_classes, basenet_preprocess, img_format,
                 img_load_dims=(224, 224)):
        self.samples = samples
        self.img_dir = img_dir
        self.batch_size = batch_size
        self.n_classes = n_classes
        self.basenet_preprocess = basenet_preprocess  # Keras basenet specific preprocessing function
        self.img_load_dims = img_load_dims  # dimensions that images get resized into when loaded
        self.img_format = img_format
        self.on_epoch_end()  # call ensures that samples are shuffled in first epoch if shuffle is set to True

    def __len__(self):
        return int(np.ceil(len(self.samples) / self.batch_size))  # number of batches per epoch

    def __getitem__(self, index):
        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]  # get batch indexes
        batch_samples = [self.samples[i] for i in batch_indexes]  # get batch samples
        X, y = self.__data_generator(batch_samples)
        return X, y

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.samples))

    def __data_generator(self, batch_samples):
        # initialize images and labels tensors for faster processing
        X = np.empty((len(batch_samples), *self.img_load_dims, 3))
        y = np.empty((len(batch_samples), self.n_classes))

        for i, sample in enumerate(batch_samples):
            # load and randomly augment image
            img_file = os.path.join(self.img_dir, '{}.{}'.format(sample['image_id'], self.img_format))
            img = utils.load_image(img_file, self.img_load_dims)
            if img is not None:
                X[i, ] = img

            # normalize labels
            if sample.get('label') is not None:
                y[i, ] = utils.normalize_labels(sample['label'])

        # apply basenet specific preprocessing
        # input is 4D numpy array of RGB values within [0, 255]
        X = self.basenet_preprocess(X)

        return X, y

ColorTransferLib/Evaluation/NIMA/handlers/config_loader.py

from ..utils.utils import load_json

def load_config(config_file):
    config = load_json(config_file)
    return config

ColorTransferLib/Evaluation/NIMA/handlers/model_builder.py

import importlib
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.optimizers import Adam
#from ..utils.losses import earth_movers_distance
from ColorTransferLib.Evaluation.NIMA.utils.losses import earth_movers_distance

class Nima:
    def __init__(self, base_model_name, n_classes=10, learning_rate=0.001, dropout_rate=0, loss=earth_movers_distance,
                 decay=0, weights='imagenet'):
        self.n_classes = n_classes
        self.base_model_name = base_model_name
        self.learning_rate = learning_rate
        self.dropout_rate = dropout_rate
        self.loss = loss
        self.decay = decay
        self.weights = weights
        self._get_base_module()

    def _get_base_module(self):
        # import Keras base model module
        if self.base_model_name == 'InceptionV3':
            self.base_module = importlib.import_module('tensorflow.keras.applications.inception_v3')
        elif self.base_model_name == 'InceptionResNetV2':
            self.base_module = importlib.import_module('tensorflow.keras.applications.inception_resnet_v2')
        else:
            self.base_module = importlib.import_module('tensorflow.keras.applications.'+self.base_model_name.lower())

    def build(self):
        # get base model class
        BaseCnn = getattr(self.base_module, self.base_model_name)

        # load pre-trained model
        self.base_model = BaseCnn(input_shape=(224, 224, 3), weights=self.weights, include_top=False, pooling='avg')

        # add dropout and dense layer
        x = Dropout(self.dropout_rate)(self.base_model.output)
        x = Dense(units=self.n_classes, activation='softmax')(x)

        self.nima_model = Model(self.base_model.inputs, x)

    def compile(self):
        self.nima_model.compile(optimizer=Adam(lr=self.learning_rate, decay=self.decay), loss=self.loss)

    def preprocessing_function(self):
        return self.base_module.preprocess_input

ColorTransferLib/Evaluation/NIMA/handlers/init.py

from . import config_loader, data_generator, model_builder, samples_loader

ColorTransferLib/Evaluation/NIMA/handlers/samples_loader.py

from ..utils.utils import load_json

def load_samples(samples_file):
    return load_json(samples_file)

ColorTransferLib/Evaluation/NIMA/predict.py

import os
import glob
import tensorflow as tf
import torch.nn.functional as F

gpus = tf.config.experimental.list_physical_devices('GPU')
if len(gpus) > 0:
    tf.config.experimental.set_memory_growth(gpus[0], True)

def image_file_to_json(img_path):
    img_dir = os.path.dirname(img_path)
    img_id = os.path.basename(img_path).split('.')[0]

    return img_dir, [{'image_id': img_id}]

def image_dir_to_json(img_dir, img_type='jpg'):
    img_paths = glob.glob(os.path.join(img_dir, '*.'+img_type))

    samples = []
    for img_path in img_paths:
        img_id = os.path.basename(img_path).split('.')[0]
        samples.append({'image_id': img_id})

    return samples

def predict(model, img):
    out = tf.expand_dims(tf.convert_to_tensor(img), axis=0)
    return model.predict(out, workers=1, use_multiprocessing=True, verbose=1, steps=1)

ColorTransferLib/Evaluation/NIMA/tests/test_augmentation_utils.py

import unittest
import numpy as np
from utils import utils
from unittest.mock import patch

class TestUtils(unittest.TestCase):

    @patch('numpy.random.randint')
    def test_random_crop(self, mock_np_random_randint):
        mock_np_random_randint.return_value = 1

        test_img = np.expand_dims(np.array([[0, 255], [0, 255]]), axis=2)
        crop_dims = (1, 1)
        cropped_img = utils.random_crop(test_img, crop_dims)
        self.assertEqual([255], cropped_img)

    @patch('numpy.random.random')
    def test_random_flip(self, mock_np_random_randint):
        mock_np_random_randint.return_value = 0
        temp = np.array([[0, 255], [0, 255]])
        test_img = np.dstack((temp, temp, temp))

        temp = np.array([[255, 0], [255, 0]])
        expected = np.dstack((temp, temp, temp))

        flipped_img = utils.random_horizontal_flip(test_img)
        np.testing.assert_array_equal(expected, flipped_img)

    def test_normalize_label(self):
        labels = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

        normed_label = utils.normalize_labels(labels)
        np.testing.assert_array_equal(np.array([.1, .1, .1, .1, .1, .1, .1, .1, .1, .1]), normed_label)

ColorTransferLib/Evaluation/NIMA/tests/test_data_generator.py

import os
import unittest
import numpy as np
from handlers.data_generator import TrainDataGenerator, TestDataGenerator

IMG_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'test_images')
N_CLASSES = 10
BATCH_SIZE = 2
BASENET_PREPROCESS = lambda x: x
IMG_FORMAT = 'jpg'
TEST_SAMPLES = [
    {
        'image_id': '42039',
        'label': [0, 5, 10, 28, 54, 31, 12, 3, 3, 2]
    },
    {
        'image_id': '42040',
        'label': [0, 5, 10, 28, 54, 31, 12, 3, 3, 2]
    },
    {
        'image_id': '42041',
        'label': [0, 5, 10, 28, 54, 31, 12, 3, 3, 2]
    },
    {
        'image_id': '42042',
        'label': [0, 5, 10, 28, 54, 31, 12, 3, 3, 2]
    },
    {
        'image_id': '42044',
        'label': [0, 5, 10, 28, 54, 31, 12, 3, 3, 2]
    },
]

class TestTrainDataGenerator(unittest.TestCase):

    def test_train_data_generator(self):
        dg = TrainDataGenerator(TEST_SAMPLES, IMG_DIR, BATCH_SIZE, N_CLASSES, BASENET_PREPROCESS, img_format=IMG_FORMAT,
                                shuffle=False)
        X, y = dg.__getitem__(0)

        # test image dimensions
        expected = (BATCH_SIZE, 224, 224, 3)
        self.assertEqual(X.shape, expected)

        # test label dimensions
        expected = (BATCH_SIZE, 10)
        self.assertEqual(y.shape, expected)

        # test that label is probability distribution
        expected = np.array([1, 1])
        np.testing.assert_array_almost_equal(np.sum(y, axis=1), expected)

        # test that last batch has 1 sample only
        X, y = dg.__getitem__(2)
        expected = 1
        self.assertEqual(X.shape[0], expected)

        # test number of batches
        expected = 3
        self.assertEqual(dg.__len__(), expected)

    def test_test_data_generator(self):
        dg = TestDataGenerator(TEST_SAMPLES, IMG_DIR, BATCH_SIZE, N_CLASSES, BASENET_PREPROCESS, img_format=IMG_FORMAT)
        X, y = dg.__getitem__(0)

        # test image dimensions
        expected = (BATCH_SIZE, 224, 224, 3)
        self.assertEqual(X.shape, expected)

        # test label dimensions
        expected = (BATCH_SIZE, 10)
        self.assertEqual(y.shape, expected)

        # test that label is probability distribution
        expected = np.array([1, 1])
        np.testing.assert_array_almost_equal(np.sum(y, axis=1), expected)

        # test that last batch has 1 sample only
        X, y = dg.__getitem__(2)
        expected = 1
        self.assertEqual(X.shape[0], expected)

        # test number of batches
        expected = 3
        self.assertEqual(dg.__len__(), expected)

ColorTransferLib/Evaluation/NIMA/trainer/train.py

import os
import argparse
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from sklearn.model_selection import train_test_split
from handlers.data_generator import TrainDataGenerator, TestDataGenerator
from handlers.model_builder import Nima
from handlers.samples_loader import load_samples
from handlers.config_loader import load_config
from utils.utils import ensure_dir_exists

def train(
    base_model_name,
    n_classes,
    samples,
    image_dir,
    batch_size,
    epochs_train_dense,
    epochs_train_all,
    learning_rate_dense,
    learning_rate_all,
    dropout_rate,
    job_dir,
    img_format='jpg',
    existing_weights=None,
    multiprocessing_data_load=False,
    num_workers_data_load=2,
    decay_dense=0,
    decay_all=0,
    **kwargs
):

    # build NIMA model and load existing weights if they were provided in config
    nima = Nima(
        base_model_name, n_classes, learning_rate_dense, dropout_rate, decay=decay_dense
    )
    nima.build()

    if existing_weights is not None:
        nima.nima_model.load_weights(existing_weights)

    # split samples in train and validation set, and initialize data generators
    samples_train, samples_test = train_test_split(
        samples, test_size=0.05, shuffle=True, random_state=10207
    )

    training_generator = TrainDataGenerator(
        samples_train,
        image_dir,
        batch_size,
        n_classes,
        nima.preprocessing_function(),
        img_format=img_format,
    )

    validation_generator = TestDataGenerator(
        samples_test,
        image_dir,
        batch_size,
        n_classes,
        nima.preprocessing_function(),
        img_format=img_format,
    )

    # initialize callbacks TensorBoard and ModelCheckpoint
    tensorboard = TensorBoard(
        log_dir=os.path.join(job_dir, 'logs'), update_freq='batch'
    )

    model_save_name = (
        'weights_' + base_model_name.lower() + '_{epoch:02d}_{val_loss:.3f}.hdf5'
    )
    model_file_path = os.path.join(job_dir, 'weights', model_save_name)
    model_checkpointer = ModelCheckpoint(
        filepath=model_file_path,
        monitor='val_loss',
        verbose=1,
        save_best_only=True,
        save_weights_only=True,
    )

    # start training only dense layers
    for layer in nima.base_model.layers:
        layer.trainable = False

    nima.compile()
    nima.nima_model.summary()

    nima.nima_model.fit_generator(
        generator=training_generator,
        validation_data=validation_generator,
        epochs=epochs_train_dense,
        verbose=1,
        use_multiprocessing=multiprocessing_data_load,
        workers=num_workers_data_load,
        max_queue_size=30,
        callbacks=[tensorboard, model_checkpointer],
    )

    # start training all layers
    for layer in nima.base_model.layers:
        layer.trainable = True

    nima.learning_rate = learning_rate_all
    nima.decay = decay_all
    nima.compile()
    nima.nima_model.summary()

    nima.nima_model.fit_generator(
        generator=training_generator,
        validation_data=validation_generator,
        epochs=epochs_train_dense + epochs_train_all,
        initial_epoch=epochs_train_dense,
        verbose=1,
        use_multiprocessing=multiprocessing_data_load,
        workers=num_workers_data_load,
        max_queue_size=30,
        callbacks=[tensorboard, model_checkpointer],
    )

    K.clear_session()

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-j',
        '--job-dir',
        help='train job directory with samples and config file',
        required=True,
    )
    parser.add_argument(
        '-i', '--image-dir', help='directory with image files', required=True
    )

    args = parser.parse_args()

    image_dir = args.__dict__['image_dir']
    job_dir = args.__dict__['job_dir']

    ensure_dir_exists(os.path.join(job_dir, 'weights'))
    ensure_dir_exists(os.path.join(job_dir, 'logs'))

    config_file = os.path.join(job_dir, 'config.json')
    config = load_config(config_file)

    samples_file = os.path.join(job_dir, 'samples.json')
    samples = load_samples(samples_file)

    train(samples=samples, job_dir=job_dir, image_dir=image_dir, **config)

ColorTransferLib/Evaluation/NIMA/utils/init.py

from . import utils, losses

ColorTransferLib/Evaluation/NIMA/utils/losses.py

from tensorflow.keras import backend as K

def earth_movers_distance(y_true, y_pred):
    cdf_true = K.cumsum(y_true, axis=-1)
    cdf_pred = K.cumsum(y_pred, axis=-1)
    emd = K.sqrt(K.mean(K.square(cdf_true - cdf_pred), axis=-1))
    return K.mean(emd)

ColorTransferLib/Evaluation/NIMA/utils/utils.py

import os
import json
import tensorflow as tf
import numpy as np

def load_json(file_path):
    with open(file_path, 'r') as f:
        return json.load(f)

def save_json(data, target_file):
    with open(target_file, 'w') as f:
        json.dump(data, f, indent=2, sort_keys=True)

def random_crop(img, crop_dims):
    h, w = img.shape[0], img.shape[1]
    ch, cw = crop_dims[0], crop_dims[1]
    assert h >= ch, 'image height is less than crop height'
    assert w >= cw, 'image width is less than crop width'
    x = np.random.randint(0, w - cw + 1)
    y = np.random.randint(0, h - ch + 1)
    return img[y:(y+ch), x:(x+cw), :]

def random_horizontal_flip(img):
    assert len(img.shape) == 3, 'input tensor must have 3 dimensions (height, width, channels)'
    assert img.shape[2] == 3, 'image not in channels last format'
    if np.random.random() < 0.5:
        img = img.swapaxes(1, 0)
        img = img[::-1, ...]
        img = img.swapaxes(0, 1)
    return img

def load_image(img_file, target_size):
    return np.asarray(tf.keras.preprocessing.image.load_img(img_file, target_size=target_size))

def normalize_labels(labels):
    labels_np = np.array(labels)
    return labels_np / labels_np.sum()

def calc_mean_score(score_dist):
    score_dist = normalize_labels(score_dist)
    return (score_dist*np.arange(1, 11)).sum()

def ensure_dir_exists(dir):
    if not os.path.exists(dir):
        os.makedirs(dir)

ColorTransferLib/Evaluation/NIQE/NIQE.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import pyiqa
import torch

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# ... (NIQE)
# 
#
# Source: https://github.com/idealo/image-quality-assessment
# https://github.com/chaofengc/Awesome-Image-Quality-Assessment
#
# Range []
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class NIQE:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        img = args[2]
        img = img.get_raw()

        img_ten = torch.from_numpy(img)
        img_ten = torch.swapaxes(img_ten, 1, 2)
        img_ten = torch.swapaxes(img_ten, 0, 1)
        img_ten = img_ten.unsqueeze(0)

        device = "cpu"
        iqa_metric = pyiqa.create_metric('niqe', device=device)
        score_nr = iqa_metric(img_ten)

        score_nr = float(score_nr.cpu().detach().numpy())

        return round(score_nr, 4)

ColorTransferLib/Evaluation/RMSE/RMSE.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Root Mean Square Error (RMSE)
# 
#
# Source: 
#
# Range []
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class RMSE:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        num_pix = src_img.shape[0] * src_img.shape[1]

        rmse_c = np.sqrt(np.sum((src_img - ref_img) ** 2, axis=(0,1)) / num_pix)
        rmse = np.sum(rmse_c) / 3

        return round(rmse, 4)

ColorTransferLib/Evaluation/NIQE/init.py

from . import NIQE

ColorTransferLib/Evaluation/RMSE/init.py

from . import RMSE

ColorTransferLib/Evaluation/SSIM/SSIM.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

from skimage.metrics import structural_similarity as ssim

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: Image quality assessment: from error visibility to structural similarity
#   Author: Zhou Wang, A.C. Bovik, H.R. Sheikh, E.P. Simoncelli
#   Published in: IEEE Transactions on Image Processing
#   Year of Publication: 2004
#
# Abstract:
#   Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of 
#   errors (differences) between a distorted image and a reference image using a variety of known properties of the 
#   human visual system. Under the assumption that human visual perception is highly adapted for extracting structural 
#   information from a scene, we introduce an alternative complementary framework for quality assessment based on the 
#   degradation of structural information. As a specific example of this concept, we develop a structural similarity 
#   index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective 
#   ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB 
#   implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.
#
# Info:
#   Name: Structural similarity index measure
#   Identifier: SSIM
#   Link: https://doi.org/10.1109/TIP.2003.819861
#   Range: [-1, 1]
#
# Implementation Details:
#   skimage.metrics is used
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class SSIM:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        mssim = ssim(src.get_raw(), ref.get_raw(), channel_axis=2, data_range=1.0, gaussian_weights=True, sigma=1.5, use_sample_covariance=False)

        return round(mssim, 4)

ColorTransferLib/Evaluation/PSNR/PSNR.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import math
import numpy as np

from ColorTransferLib.ImageProcessing.Image import Image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Based on the paper:
#   Title: -
#   Author: -
#   Published in: -
#   Year of Publication: -
#
# Abstract:
#   -

# Info:
#   Name: Peak Signal-to-Noise Ratio
#   Shortname: PSNR
#   Identifier: PSNR
#   Link: -
#   Range: [0, inf] with higher values indicates better quality
#
# Implementation Details:
#   if source and output are identical, the PSNR will be not considered for the evaluation
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class PSNR:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self):
        pass

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]
        src_img = src.get_raw()
        ref_img = ref.get_raw()

        num_pix = src_img.shape[0] * src_img.shape[1]

        mse_c = np.sum(np.power(np.subtract(src_img, ref_img), 2), axis=(0,1)) / num_pix
        mse = np.sum(mse_c) / 3

        psnr_v = 10 * math.log10(math.pow(1, 2) / mse)

        return round(psnr_v, 4)

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/connectMatrix.m

function cx = connectMatrix( dims , lx , inter_type , intra_type , cyclic_type )

% inter_type :
%   1 => only neighbor
%   2 => everywhere inter-scale on consecutive scales
% intra_type :
%   1 => only neighbor
%   2 => connect to everynode on same scale
% cyclic_type
%   1 => cyclic boundary rules
%   2 => non-cyclic boundaries

%% some useful numbers to access nodes at each resolution level
d = prod(dims,2);
N = sum(d);
cx = zeros(N,N);
Nmaps = size(dims,1);
offsets = zeros(Nmaps,1);
for i=2:Nmaps
    offsets(i) = d(i-1) + offsets(i-1);
end

%% connect nodes on a single resolution/level
for i=1:Nmaps    
    mapsize = d(i);
    if ( intra_type == 1 ) %% only neighbors and self
        dmatrix = simpledistance( dims(i,:) , cyclic_type );
        dmatrix = (dmatrix <= 1);
        cx( (offsets(i)+1):(offsets(i)+mapsize), (offsets(i)+1):(offsets(i)+mapsize) ) = dmatrix;
    else %% connect everyone on 
        cx( (offsets(i)+1):(offsets(i)+mapsize), (offsets(i)+1):(offsets(i)+mapsize) ) = 1;
    end
end

%%%
%% for inter-scale nodes , connect according to some rule
%% inter_type is 1  ==> connect only nodes corresponding to same location
%% inter_type is 2  ==> connect nodes corresponding to different locations
%%%

map_pairs = mycombnk( [ 1 : Nmaps ] , 2);
for i=1:size(map_pairs,1)

    mapi1 = map_pairs(i,1);
    mapi2 = map_pairs(i,2);

    %% nodes in map at level mapi1
    nodes_a = [ offsets(mapi1) + 1 : offsets(mapi1) + d(mapi1) ];

    %% nodes in map at level mapi2
    nodes_b = [ offsets(mapi2) + 1 : offsets(mapi2) + d(mapi2) ];

    %% for each pair , possibly connect
    for ii=1:d(mapi1)
        for jj=1:d(mapi2)
            %% using location matrix, determine locations of the 
            %% two nodes            
            la = lx( nodes_a(ii) , 3:(2+lx(nodes_a(ii),2)) );
            lb = lx( nodes_b(jj) , 3:(2+lx(nodes_b(jj),2)) );

            if ( inter_type == 1 ) %% only connect inter-scale nodes
                                   %% which correspond to same location
                if ( length( intersect( la , lb ) ) > 0 )
                    cx( nodes_a(ii) , nodes_b(jj) ) = 1;
                    cx( nodes_b(jj) , nodes_a(ii) ) = 1;
                end
            elseif ( inter_type == 2) %% connect all inter-scale nodes
                cx( nodes_a(ii) , nodes_b(jj) ) = 1;
                cx( nodes_b(jj) , nodes_a(ii) ) = 1;                                                               
            end
        end % end jj=1:nb
    end % end ii=1:na
end % end i=1:size(map_pairs,1)

cx(cx==0) = inf;

ColorTransferLib/Evaluation/VSI/VSI.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import os
from sys import platform

if platform == "linux" or platform == "linux2":
    # linux
    os.environ["OCTAVE_EXECUTABLE"] = "/usr/bin/octave-cli"
elif platform == "darwin":
    # OS X
    os.environ["OCTAVE_EXECUTABLE"] = "/opt/homebrew/bin/octave-cli"
elif platform == "win32":
    # Windows...
    pass

from oct2py import octave
import cv2
import numpy as np

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Structural similarity index measure (VSI)
# ...
#
# Source: VSI: A visual saliency-induced index for perceptual image quality assessment
#
# Range [??, 1]
#
# Note: recompile mex files -> see "gbvs_compile.m"
# change #include <matrix.h> to <Matrix.h>
#
# Sources:
# http://www.animaclock.com/harel/share/gbvs.php
# https://github.com/Pinoshino/gbvs
#
# Good to know:
# https://ajcr.net/Basic-guide-to-einsum/
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class VSI:
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def initOcatve():
        #octave.run(os.getcwd() + "/ColorTransferLib/Evaluation/VSI/gbvs/gbvs_install.m")

        # mex -g  mex_mgRecolourParallel_1.cpp COMPFLAGS="/openmp $COMPFLAGS"
        # Necessary to run "gbvs_install" once on a new system
        octave.addpath(octave.genpath('.'))
        octave.eval("warning('off','Octave:shadowed-function')")
        octave.eval('pkg load image')
        octave.eval('pkg load statistics')
        #octave.gbvs_install()
        #octave.eval("dir")

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def VS(img):
        outp = octave.gbvs_fast((img * 255).astype("uint8"))
        sal_map = outp["master_map_resized"]
        return sal_map

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb2lmn(img):
        M = np.array([[0.06, 0.63, 0.27], [0.3, 0.04, -0.35], [0.34, -0.6, 0.17]])
        out = np.einsum("ij, klj -> kli", M, img)
        return out

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def G(img):
        kernel_x = np.array([[3, 0, -3], [10, 0, -10], [3, 0, -3]]) / 16.0
        kernel_y = np.array([[3, 10, 3], [0, 0, 0], [-3, -10, -3]]) / 16.0

        schaar_x = cv2.filter2D(img, -1, kernel_x)
        schaar_y = cv2.filter2D(img, -1, kernel_y)

        schaar_out = np.sqrt(abs(schaar_x) ** 2 + abs(schaar_y) ** 2)
        return schaar_out

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def S_SV(VS1, VS2, C1):
        nomi = 2 * VS1 * VS2 + C1
        denomi = (abs(VS1) ** 2) + (abs(VS2) ** 2) + C1
        SSV = nomi / denomi
        return SSV

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def S_G(G1, G2, C2):
        nomi = 2 * G1 * G2 + C2
        denomi = (abs(G1) ** 2) + (abs(G2) ** 2) + C2
        SG = nomi / denomi
        return SG

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def S_C(M1, M2, N1, N2, C3):
        nomi1 = 2 * M1 * M2 + C3
        denomi1 = (abs(M1) ** 2) + (abs(M2) ** 2) + C3
        X1 = nomi1 / denomi1

        nomi2 = 2 * N1 * N2 + C3
        denomi2 = (abs(N1) ** 2) + (abs(N2) ** 2) + C3
        X2 = nomi2 / denomi2

        SC = X1 * X2
        return SC

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def S(SSV, SG, SC, a, b):
        S_val = SSV * (abs(SG) ** a) * (abs(SC) ** b)
        return S_val

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def VS_m(VS1, VS2):
        VS_m = np.maximum(VS1, VS2)
        return VS_m

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def VSI(S_val, VS_m):
        VSI_val = np.sum(S_val * VS_m) / np.sum(VS_m)
        return VSI_val

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def apply(*args):
        src = args[0]
        ref = args[2]

        VSI.initOcatve()
        C1 = 0.00001
        C2 = 0.00001
        C3 = 0.00001
        a = 0.4
        b = 0.02

        src_img = src.get_raw()
        ref_img = ref.get_raw()

        src_lmn = VSI.rgb2lmn(src_img)
        ref_lmn = VSI.rgb2lmn(ref_img)

        L1 = src_lmn[:,:,0]
        L2 = ref_lmn[:,:,0]
        M1 = src_lmn[:,:,1]
        M2 = ref_lmn[:,:,1]
        N1 = src_lmn[:,:,2]
        N2 = ref_lmn[:,:,2]

        VS1 = VSI.VS(src_img)
        VS2 = VSI.VS(ref_img)
        VS_m = VSI.VS_m(VS1, VS2)

        G1 = VSI.G(L1)
        G2 = VSI.G(L2)

        SSV = VSI.S_SV(VS1, VS2, C1)
        SG = VSI.S_G(G1, G2, C2)
        SC = VSI.S_C(M1, M2, N1, N2, C3)
        S_val = VSI.S(SSV, SG, SC, a, b)

        VSI_val = VSI.VSI(S_val, VS_m)

        return round(VSI_val, 4)

ColorTransferLib/Evaluation/SSIM/init.py

from . import SSIM

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/distanceMatrix.m

function dx = distanceMatrix( dims , lx , cyclic_type )

%% some useful numbers to access nodes at each resolution level
d = prod(dims,2);
N = sum(d);
dx = zeros(N,N);
Nmaps = size(dims,1);
offsets = zeros(Nmaps,1);
for i=2:Nmaps
    offsets(i) = d(i-1) + offsets(i-1);
end

sd = {};
for i=1:Nmaps
    sd{i} = simpledistance( dims(i,:) , cyclic_type );
end

%% form clique on all nodes in same scale
for i=1:Nmaps    
    mapsize = d(i);
    dmatrix = sd{i};
    for ii=1:mapsize
        for jj=1:mapsize
            dx( offsets(i)+ii, offsets(i)+jj ) = dmatrix(ii,jj) * d(1)/mapsize;
        end
    end    
end

map_pairs = mycombnk( [ 1 : Nmaps ] , 2);
dmatrix = sd{1};
for i=1:size(map_pairs,1)

    mapi1 = map_pairs(i,1);
    mapi2 = map_pairs(i,2);

    %% nodes in map at level mapi1
    nodes_a = [ offsets(mapi1) + 1 : offsets(mapi1) + d(mapi1) ];

    %% nodes in map at level mapi2
    nodes_b = [ offsets(mapi2) + 1 : offsets(mapi2) + d(mapi2) ];

    %% for each pair , possibly connect
    for ii=1:d(mapi1)
        for jj=1:d(mapi2)

            %% using location matrix, determine locations of the
            %% two nodes
            la = lx( nodes_a(ii) , 3:(2+lx(nodes_a(ii),2)) );
            lb = lx( nodes_b(jj) , 3:(2+lx(nodes_b(jj),2)) );

            nla = length(la);
            nlb = length(lb);

            %% convention betweeen 0-index and 1-index
            la = la + 1; 
            lb = lb + 1;

            mean_dist = 0;
	    max_dist = -inf;
            for iii=1:nla
                for jjj=1:nlb
		    dd = dmatrix( la(iii) , lb(jjj) );
                    mean_dist = mean_dist + dd;
		    max_dist = max( dd , max_dist );
                end
            end

            mean_dist = mean_dist / (nla*nlb);            
            dx( nodes_a(ii) , nodes_b(jj) ) = mean_dist; %max_dist; %mean_dist;
            dx( nodes_b(jj) , nodes_a(ii) ) = mean_dist; %max_dist; %mean_dist;

        end % end jj=1:nb
    end % end ii=1:na
end % end i=1:size(map_pairs,1)

ColorTransferLib/Evaluation/VSI/init.py

from . import VSI

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/formMapPyramid.m

%
% for each delta in deltas , adds map 
% delta binary orders smaller to A
% stacks dimensions of maps in A
%

function [ Apyr , dims ] = formMapPyramid( A , deltas )

my_eps = 1e-12;

num_deltas = length(deltas);
max_delta = max(deltas);
num_pyr = 1 + num_deltas;

dim = [ size(A) num_pyr ];
Apyr = zeros( dim );
dims = zeros(num_pyr,2);

maps = {};
maps{1}.map = A;
maps{1}.siz = size(A);
last = A;

for i=1:max_delta;
    last = mySubsample( last );
    maps{i+1}.map = last;
    maps{i+1}.siz = size(last); 
end

for i=1:max_delta+1
   map = maps{i}.map;
   map = mat2gray(map);
   if ( max(map(:)) == 0 ) map = map + my_eps; end
   maps{i}.map = map;
end

Apyr(:,:,1) = maps{1}.map;
dims(1,:) = maps{1}.siz;

if ( size(deltas,1) > size(deltas,2) ) deltas = deltas'; end
i = 1;
for delta=deltas,    
    i = i + 1;
    d = maps{1+delta}.siz;
    m = maps{1+delta}.map;    
    Apyr(1:d(1),1:d(2),i) = m;
    dims(i,:) = d;
end

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/getDims.m

function [ dims ] = getDims( orig_size , deltas )
[tmp,dims] = formMapPyramid( ones(orig_size) , deltas );

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/graphsalapply.m

function [Anorm,iters] = graphsalapply( A , frame , sigma_frac, num_iters , algtype , tol )

%
%  this function is the heart of GBVS.
%   * it takes a feature map, forms a graph over its locations, which is either a lattice of a hierachy ("multiresolution")of lattices,
%     connects the nodes with weighted edges, and computes the equilibrium distribution over states.
%   * the weight assignment rule from node i to node j depends on the 'algtype' 

%  algtype    algorith type:
%               1 : MM( i->j ) = w*AL(j)               [ mass conc ]
%               2 : MM( i->j ) = w*|AL(i)-AL(j)|       [ sal diff ]
%               3 : MM( i->j ) = w*|log(AL(i)/AL(j))|  [ sal log ]
%               4 : Anorm = A . ^ w                    [ simple mass concentration ]

%  tol controls a stopping rule on the computation of the equilibrium distribution (principal eigenvector)
%  the lower it is, the longer the algorithm runs.

if ( algtype == 4 )
  Anorm = A .^ 1.5;
  iters = 1;
  return;
end

% form a multiresolution pyramid of feature maps according to multilevels
lx = frame.lx; 
[ Apyr , dims ] = formMapPyramid( A , frame.multilevels );

% get a weight matrix between nodes based on distance matrix
sig = sigma_frac * mean(size(A));
Dw = exp( -1 * frame.D / (2 * sig^2) );

% assign a linear index to each node
AL = mexArrangeLinear( Apyr , dims );

% create the state transition matrix between nodes
P = size(lx,1);
MM = zeros( P , P );

iters = 0;

for i=1:num_iters

  % assign edge weights based on distances between nodes and algtype  
  mexAssignWeights( AL , Dw , MM , algtype );

  % make it a markov matrix (so each column sums to 1)
  mexColumnNormalize( MM );

  % find the principal eigenvector
  [AL,iteri] = principalEigenvectorRaw( MM , tol );
  iters = iters + iteri;

end

% collapse multiresolution representation back onto one scale
Vo = mexSumOverScales( AL , lx , prod(size(A)) );

% arrange the nodes back into a rectangular map
Anorm = reshape(Vo,size(A));

ColorTransferLib/Evaluation/PSNR/init.py

from . import PSNR

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/makeLocationMap.m

function lx = makeLocationMap( dims , nam , N )

Nmaps = size(dims,1);
lx = zeros(N , 2^13 + 3 );

px = {};
for i = 1 : Nmaps
    px{i}.r = partitionindex( dims(1,1) , dims(i,1) );
    px{i}.c = partitionindex( dims(1,2) , dims(i,2) );
end

for i = 1 : Nmaps
    for j = 1 : dims(i,1)
        for k = 1 : dims(i,2)

            nm = nam(j,k,i);

            xcoords = px{i}.r( 1 , find( px{i}.r(2,:) == j ) );
            ycoords = px{i}.c( 1 , find( px{i}.c(2,:) == k ) );

            lst = [];
            Nx = length(xcoords);
            Ny = length(ycoords);
            Nl = 0;

            for ii=1:Nx
                for jj=1:Ny
                    lst(end+1) = nam( xcoords(ii) , ycoords(jj) , 1 );
                    Nl = Nl + 1;
                end
            end
            lx( nm + 1 , 1:Nl+2  ) = [ nm Nl lst ];
        end
    end
end

maxL = max( lx(:,2) );
lx = lx( : , 1 : maxL + 2 );

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/initGBVS.m

%
% some constants used across different calls to gbvs()
%

function [grframe,param] = initGBVS(param, imgsize)

mymessage(param,'initializing....\n');

% logical consistency checking of parameters
if ( min(param.levels) < 2 )
    mymessage(param,'oops. cannot use level 1.. trimming levels used\n');
    param.levels = param.levels(param.levels>1);
end
if ( param.useIttiKochInsteadOfGBVS )
    param.activationType = 2;
    param.normalizationType = 3;
    param.normalizeTopChannelMaps = 1;
end

param.maxcomputelevel = max(param.levels);
if (param.activationType==2)
    param.maxcomputelevel = max( param.maxcomputelevel , max(param.ittiCenterLevels)+max(param.ittiDeltaLevels) );
end

w = imgsize(2); h = imgsize(1); scale = param.salmapmaxsize / max(w,h);
salmapsize = round( [ h w ] * scale );

% weight matrix
if ( ~param.useIttiKochInsteadOfGBVS )
  load mypath;
  ufile = sprintf('%s__m%s__%s.mat',num2str(salmapsize),num2str(param.multilevels),num2str(param.cyclic_type));
  ufile(ufile==' ') = '_';
  ufile = fullfile( pathroot , 'initcache' ,  ufile );
  if ( exist(ufile) )
    grframe = load(ufile);
    grframe = grframe.grframe;
  else
    grframe = graphsalinit( salmapsize , param.multilevels , 2, 2, param.cyclic_type );
    save(ufile,'grframe');
  end
else
  grframe = [];
end

% gabor filters
gaborParams.stddev = 2;gaborParams.elongation = 2;
gaborParams.filterSize = -1;gaborParams.filterPeriod = pi;
for i = 1 : length(param.gaborangles)
    theta = param.gaborangles(i);
    gaborFilters{i}.g0 = makeGaborFilterGBVS(gaborParams, theta, 0);
    gaborFilters{i}.g90 = makeGaborFilterGBVS(gaborParams, theta, 90);
end

param.gaborParams = gaborParams;
param.gaborFilters = gaborFilters;
param.salmapsize = salmapsize;
param.origimgsize = imgsize;

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/indexmatrix.m

function ix = indexmatrix( dims )

ix = zeros(dims);
p = prod(dims);

for i=1:p
 ix(i) = i;
end

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/graphsalinit.m

% this function creates the weight matrix for making edge weights
% and saves some other constants (like node-in-lattice index) to a 'frame'
% used when the graphs are made from saliency/feature maps.

%
% edge types (by default, instantiate fully connected graph. 
%  use inter/intra-type = 1 to connect only to nearest neighbors)
%
% inter_type :
%   1 => only same-location neighbor
%   2 => everywhere inter-scale on consecutive scales
% intra_type :
%   1 => only neighbor
%   2 => everywhere
% cyclic_type :
%   1 => cyclic boundary rules
%   2 => non-cyclic boundaries
%
%  jharel 7 / 27 / 06

function [frame] = graphsalinit( map_size , multilevels , inter_type , intra_type , cyclic_type )

dims = getDims( map_size , multilevels );
[N,nam] = namenodes(dims);
lx = makeLocationMap( dims , nam , N );
cx = connectMatrix( dims , lx , inter_type , intra_type , cyclic_type );
dx = distanceMatrix( dims , lx , cyclic_type );
D = cx .* dx;

frame.D = D;
frame.lx = lx;
frame.dims = dims;
frame.multilevels = multilevels;

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexArrangeLinear.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

// Avalues = mexArrangeLinear( A , dims )
// where A is the     NxM x K   matrix containing multi-resolution info
//       dims is      K x 2     matrix containing dimensions of each scale
//       Avalues is   P x 1 matrix containing multi-resolution info in flat array

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) {

  //Declarations
  mxArray *Aar, *dimsar;
  double *A, *dims, *Avalues;

  int i,r,c,cur_index;
  int N, M, K, P, offset_,M_orig,N_orig,mapsize, coffset;

  // get first argument A
  Aar = (mxArray*)prhs[0];
  A = mxGetPr(Aar);

  // get sigma
  dimsar = (mxArray*)prhs[1];
  dims = mxGetPr(dimsar);
  K = mxGetM(dimsar); // number of rows

  P = 0;
  for (i=0;i<K;i++) 
    P += (int)( dims[ i ] * dims[ K + i ] );

  // create output
  plhs[0] = mxCreateDoubleMatrix(P, 1, mxREAL);
  Avalues = mxGetPr(plhs[0]);

  M_orig = (int)dims[0];
  N_orig = (int)dims[K];  
  mapsize = M_orig * N_orig;
  offset_ = 0;
  cur_index = 0;
  for (i=0;i<K;i++) {
    M = (int)dims[ i ];
    N = (int)dims[ K + i ];
    for (c=0;c<N;c++) {
      coffset = offset_ + c*M_orig;
      for (r=0;r<M;r++)
	Avalues[cur_index++] = A[ coffset + r ];
    }
    offset_ += mapsize;
  }

  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexAssignWeights.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

//  mexAssignWeights( AL , D , MM , algtype )
//
//  name      dim    description
// -------------------------------------------
//  AL        Px1    values of map linearized
//  D         PxP    w=D(i,j)==D(j,i) is dist multiplier for i & j
//  MM        PxP    output space for markov matrix
//  algtype   1x1    algorith type:
//                    1 : MM( i->j ) = w*AL(j)               [ mass conc ]
//                    2 : MM( i->j ) = w*|AL(i)-AL(j)|       [ sal diff ]
//                    3 : MM( i->j ) = w*|log(AL(i)/AL(j))|  [ sal log ]
//                    4 : MM( i->j ) = w*1/|AL(i)-AL(j)|     [ sal affin ]

double myabs(double v) {
  return (v>=0) ? v : (-1*v);
}

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) {

  //Declarations
  mxArray *ALar, *Dar, *MMar, *algtypear;
  double *AL, *D, *MM, *algtype;
  double w;
  int r, c, P, Pc, Pc_r, algtype_i;

  // get AL
  ALar = (mxArray*)prhs[0];
  AL = mxGetPr(ALar);
  P = mxGetM(ALar);

  // get D
  Dar = (mxArray*)prhs[1];
  D = mxGetPr(Dar);

  // get MM
  MMar = (mxArray*)prhs[2];
  MM = mxGetPr(MMar);

  // get algtype
  algtypear = (mxArray*)prhs[3];
  algtype = mxGetPr(algtypear);
  algtype_i = (int)algtype[0];

  for (c=0;c<P;c++) { // ro
    Pc = P * c;
    for (r=0;r<P;r++) {  
      Pc_r = Pc + r;
      w = D[ Pc_r ]; // D(r,c)
      if ( algtype_i == 1 ) {
	MM[ Pc_r ] = w * AL[r];
      } else if ( algtype_i == 2 ) {	
	MM[ Pc_r ] = w * myabs( AL[r] - AL[c] );
      } else if ( algtype_i == 3 ) {	
	MM[ Pc_r ] = w * myabs( log( AL[r]/AL[c] ) );
      } else if ( algtype_i == 4 ) {
	MM[ Pc_r ] = w * 1/(myabs( AL[r] - AL[c] )+1e-12);
      }
    } // end i=0..P
  } // end j=0..P  

  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexColumnNormalize.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

// Normalizes so that each column sums to one

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) {

  //Declarations
  mxArray *Aar;
  double *A;
  double s;
  int i,j,numR,numC,myoff;

  // get first argument A
  Aar = (mxArray*)prhs[0];
  A = mxGetPr(Aar);
  numR = mxGetM(Aar);  // rows
  numC = mxGetN(Aar);  // cols

  for (j=0;j<numC;j++) {
    s = 0;    
    myoff = j*numR;
    for (i=0;i<numR;i++)
      s += A[ myoff + i ];
    for (i=0;i<numR;i++)
      A[ myoff + i ] /= s;
  }

  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexSumOverScales.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

//  Vo = mexSumOverScales( v , lx , N )
//
//  name      dim       description
// -------------------------------------------------------------------------
//  v        P x 1      values of vector linearized
//  lx       P x (2+K)  K = lx(i,2)  # of locations corresponding to i
//                      lx(i,3:3+K)  individual locations corresponding to i
//  N        1 x 1      # of locations in original size map
//  Vo       N x 1      components of v summed and collapsed according to lx

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) {

  // Declarations
  mxArray *var, *lxar, *Nar;
  double *v, *lx, *Nd, *Vo, vtmp;
  int i, j, K , N, P , P2, locum;

  // get v
  var = (mxArray*)prhs[0];
  v = mxGetPr(var);
  P = mxGetM(var);

  // get lx
  lxar = (mxArray*)prhs[1];
  lx = mxGetPr(lxar);

  // get N
  Nar = (mxArray*)prhs[2];
  Nd = mxGetPr(Nar);
  N = (int)Nd[0];

  // allocate Vo
  plhs[0] = mxCreateDoubleMatrix(N, 1, mxREAL);
  Vo = mxGetPr(plhs[0]);

  for (i=0;i<N;i++)
    Vo[i] = 0;

  P2 = 2 * P;
  for (i=0;i<P;i++) {
    K = (int)lx[ P + i ];
    vtmp = v[i] / (double)K;
    for (j=0;j<K;j++) {
      locum = (int)lx[ P2 + j*P + i ];
      Vo[ locum ] += vtmp;
    }
  }

  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/mexVectorToMap.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

//  outmap = mexVectorToMap( v , dim )
//
//  name      dim        description
// -------------------------------------------------------------------------
//  v        MN x 1      values of map in linear form
//  dim      1  x 2      =[M N] dimension of 2D map
//  outmap   M  x N      values of map in 2D map form

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) {

  // Declarations
  mxArray *var, *dimar;
  double *v, *dim, *outmap;
  int i, M, N, MN;

  // get v
  var = (mxArray*)prhs[0];
  v = mxGetPr(var);

  // get dim
  dimar = (mxArray*)prhs[1];
  dim = mxGetPr(dimar);
  M = (int)dim[0];
  N = (int)dim[1];
  MN = M * N;

  // allocate outmap
  plhs[0] = mxCreateDoubleMatrix(M, N, mxREAL);
  outmap = mxGetPr(plhs[0]);

  for (i=0;i<MN;i++)
    outmap[i] = v[i];

  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/partitionindex.m

function ix = partitionindex( N , M )

binsize = floor( N / M );
leftover = N - binsize * M;

pad = zeros( 1 , M );
pad(1) = floor(leftover / 2);
pad(M) = ceil(leftover / 2);
ix = zeros( 2 , N );

curindex = 0;
for i = 1 : M
    for j = 1 : binsize + pad(i)
        curindex = curindex + 1;
        ix( : , curindex ) = [ curindex ; i ];
    end
end

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/principalEigenvectorRaw.m

%
% computes the principal eigenvector of a [nm nm] markov matrix
%
% j harel 6/06

function [v,iter] = principalEigenvectorRaw( markovA , tol )

if ( sparseness(markovA) < .4 )
     markovA = sparse(markovA);
end

D = size(markovA,1);

df = 1;
v = ones(size(markovA,1),1)/D;
oldv = v;
oldoldv = v;
iter = 0;

while ( df > tol )
    oldv = v;
    oldoldv = oldv;
    v = markovA * v;
    df = norm(oldv-v);
    iter = iter + 1;
    s = sum(v);
    if ( s >= 0 &&  s < inf )
        continue;
    else
        v = oldoldv;
        break;
    end
end

v = v / sum(v);

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/namenodes.m

function [N,nam] = namenodes(dims)

Nmaps = size( dims , 1 );

N = 0;
for i=1:Nmaps
    N = N + prod( dims(i,:) );
end
nam = -1 * ones( [dims(1,:)  Nmaps] );
curN = 0;

for i=1:Nmaps
    for k=1:dims(i,2)
        for j=1:dims(i,1)
            nam(j,k,i) = curN;
            curN = curN + 1;
        end
    end
end

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/simpledistance.m

%
% gives you a matrix where
% d( ix(i,j) , ix(ii,jj) ) = distance^2 between (i,j) & (ii,jj)
%
% cyclic_type
%   1 => cyclic boundary rules
%   2 => non-cyclic boundaries

function d = simpledistance( dim , cyclic_type )

d = 0;

ix = indexmatrix( dim );
N = prod( dim );

d = zeros( N , N );

for i=1:dim(1)
    for j=1:dim(2)
        for ii=1:dim(1)
            for jj=1:dim(2)
                if ( d( ix(i,j) , ix(ii,jj) ) == 0 )
                    di = 0 ; dj = 0;
                    if ( cyclic_type==1 )
                        di = min( abs(i-ii) , abs( abs(i-ii) - dim(1) ) );
                        dj = min( abs(j-jj) , abs( abs(j-jj) - dim(2) ) );
                    else
                        di = i-ii;
                        dj = j-jj;
                    end
                    d( ix(i,j) , ix(ii,jj) ) = di^2 + dj^2;
                    d( ix(ii,jj) , ix(i,j) ) = di^2 + dj^2;
                end
            end
        end
    end
end

ColorTransferLib/Evaluation/VSI/gbvs/compile/cleanmex.m

!rm */*.mex*

ColorTransferLib/Evaluation/VSI/gbvs/compile/gbvs_compile.m

% cleanmex

cd util
mex('myContrast.cc');
cd ../

cd algsrc
mex('mexArrangeLinear.cc');
mex('mexAssignWeights.cc');
mex('mexColumnNormalize.cc');
mex('mexSumOverScales.cc');
mex('mexVectorToMap.cc');
cd ../

cd saltoolbox/
mex('mySubsample.cc');
mex('mexLocalMaximaGBVS.cc');
cd ../

ColorTransferLib/Evaluation/VSI/gbvs/algsrc/sparseness.m

function s = sparseness(a)
s = sum(sum(a~=0)) / prod(size(a));

ColorTransferLib/Evaluation/VSI/gbvs/compile/gbvs_compile2.m

% cleanmex

cd util
mex -maci64 myContrast.cc ;
cd ../

cd algsrc
mex -maci64 mexArrangeLinear.cc ;
mex -maci64 mexAssignWeights.cc ;
mex -maci64 mexColumnNormalize.cc ;
mex -maci64 mexSumOverScales.cc ;
mex -maci64 mexVectorToMap.cc ;
cd ../

cd saltoolbox/
mex -maci64 mySubsample.cc ;
mex -maci64 mexLocalMaximaGBVS.cc ;
cd ../

ColorTransferLib/Evaluation/VSI/gbvs/demo/flicker_motion_demo.m

% some video sequence
i = 1;
for imgi = 85 : 88
    fname{i} = sprintf('samplepics/seq/%03d.jpg',imgi);
    i = i + 1;
end
N = length(fname);

% compute the saliency maps for this sequence

param = makeGBVSParams; % get default GBVS params
param.channels = 'IF';  % but compute only 'I' instensity and 'F' flicker channels
param.levels = 3;       % reduce # of levels for speedup

motinfo = [];           % previous frame information, initialized to empty
for i = 1 : N
    [out{i} motinfo] = gbvs( fname{i}, param , motinfo );
end

% display results
figure;
for i = 1 : N
   subplot(2,N,i);    
   imshow( imread(fname{i}) );
   title( fname{i} );
   subplot(2,N,N+i);
   imshow( out{i}.master_map_resized );
end

ColorTransferLib/Evaluation/VSI/gbvs/demo/demonstration.m

% This file is a demonstration of how to call gbvs()

% make some parameters
params = makeGBVSParams;

% could change params like this
params.contrastwidth = .11;

% example of itti/koch saliency map call
params.useIttiKochInsteadOfGBVS = 1;
outitti = gbvs('samplepics/1.jpg',params);
figure;
subplot(1,2,1);
imshow(imread('samplepics/1.jpg'));
title('image');
subplot(1,2,2);
imshow(outitti.master_map_resized);
title('Itti, Koch Saliency Map');
fprintf(1,'Now waiting for user to press enter...\n');
pause;

% example of calling gbvs() with default params and then displaying result
outW = 200;
out = {};
% compute saliency maps for some images
for i = 1 : 5

  img = imread(sprintf('samplepics/%d.jpg',i));

  tic; 

    % this is how you call gbvs
    % leaving out params reset them to all default values (from
    % algsrc/makeGBVSParams.m)
    out{i} = gbvs( img );   

  toc;

  % show result in a pretty way  

  s = outW / size(img,2) ; 
  sz = size(img); sz = sz(1:2);
  sz = round( sz * s );

  img = imresize( img , sz , 'bicubic' );  
  saliency_map = imresize( out{i}.master_map , sz , 'bicubic' );
  if ( max(img(:)) > 2 ) img = double(img) / 255; end
  img_thresholded = img .* repmat( saliency_map >= prctile(saliency_map(:),75) , [ 1 1 size(img,3) ] );  

  figure;
  subplot(2,2,1);
  imshow(img);
  title('original image');

  subplot(2,2,2);
  imshow(saliency_map);
  title('GBVS map');

  subplot(2,2,3);
  imshow(img_thresholded);
  title('most salient (75%ile) parts');

  subplot(2,2,4);
  show_imgnmap(img,out{i});
  title('saliency map overlayed');

  if ( i < 5 )
    fprintf(1,'Now waiting for user to press enter...\n');
    pause;
  end

end

ColorTransferLib/Evaluation/VSI/gbvs/demo/simplest_demonstration.m

% example of how to call gbvs with default params

img = imread('samplepics/4.jpg');
out_gbvs = gbvs(img);
out_itti = ittikochmap(img);

subplot(2,3,1);
imshow(img);
title('Original Image');

subplot(2,3,2);
show_imgnmap(img,out_gbvs);
title('GBVS map overlayed');

subplot(2,3,3);
show_imgnmap(img,out_itti);
title('Itti/Koch map overlayed');

subplot(2,3,5);
imshow( out_gbvs.master_map_resized );
title('GBVS map');

subplot(2,3,6);
imshow(out_itti.master_map_resized);
title('Itti/Koch map');

ColorTransferLib/Evaluation/VSI/gbvs/gbvs_install.m

pathroot = pwd;
save -mat util/mypath.mat pathroot
addpath(genpath( pathroot ), '-begin');
savepath

ColorTransferLib/Evaluation/VSI/gbvs/gbvs_fast.m

%%
%% Use this instead of gbvs() if you want slightly less predictive maps
%% computed in a fraction of the time.
%%
%%

function out = gbvs_fast( img )

params = makeGBVSParams;
params.channels = 'DO';
params.gaborangles = [ 0 90 ];
params.levels = 3;
params.verbose = 0;
params.tol = 0.003;
params.salmapmaxsize = 24;
out = gbvs(img,params);

ColorTransferLib/Evaluation/VSI/gbvs/gbvs.m

function [out,motionInfo] = gbvs(img,param,prevMotionInfo)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                                                     %                            
% This computes the GBVS map for an image and puts it in master_map.                                  %
%                                                                                                     %
% If this image is part of a video sequence, motionInfo needs to be recycled in a                     %
% loop, and information from the previous frame/image will be used if                                 %
% "flicker" or "motion" channels are employed.                                                        %
% You need to initialize prevMotionInfo to [] for the first frame  (see demo/flicker_motion_demo.m)   %
%                                                                                                     %
%  input                                                                                              %
%    - img can be a filename, or image array (double or uint8, grayscale or rgb)                      %
%    - (optional) param contains parameters for the algorithm (see makeGBVSParams.m)                  %
%                                                                                                     %
%  output structure 'out'. fields:                                                                    %
%    - master_map is the GBVS map for img. (.._resized is the same size as img)                       %
%    - feat_maps contains the final individual feature maps, normalized                               %
%    - map_types contains a string description of each map in feat_map (resp. for each index)         %
%    - intermed_maps contains all the intermediate maps computed along the way (act. & norm.)         %
%      which are used to compute feat_maps, which is then combined into master_map                    %
%    - rawfeatmaps contains all the feature maps computed at the various scales                       %
%                                                                                                     %
%  Jonathan Harel, Last Revised Aug 2008. jonharel@gmail.com                                          %
%                                                                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

if ( strcmp(class(img),'char') == 1 ) img = imread(img); end
if ( strcmp(class(img),'uint8') == 1 ) img = double(img)/255; end
if ( (size(img,1) < 128) || (size(img,2) < 128) )
    fprintf(2,'GBVS Error: gbvs() meant to be used with images >= 128x128\n');
    out = [];
    return;
end

if ( (nargin == 1) || (~exist('param')) || isempty(param) ) param = makeGBVSParams; end
[grframe,param] = initGBVS(param,size(img));

if ( (nargin < 3) || (~exist('prevMotionInfo')) )
    prevMotionInfo = [];
end

if ( param.useIttiKochInsteadOfGBVS )
    mymessage(param,'NOTE: Computing STANDARD Itti/Koch instead of Graph-Based Visual Saliency (GBVS)\n\n');
end

%%%% 
%%%% STEP 1 : compute raw feature maps from image
%%%%

mymessage(param,'computing feature maps...\n');
if ( size(img,3) == 3 ) imgcolortype = 1; else, imgcolortype = 2; end
[rawfeatmaps motionInfo] = getFeatureMaps( img , param , prevMotionInfo );

%%%% 
%%%% STEP 2 : compute activation maps from feature maps
%%%%

mapnames = fieldnames(rawfeatmaps);
mapweights = zeros(1,length(mapnames));
map_types = {};
allmaps = {};
i = 0;
mymessage(param,'computing activation maps...\n');
for fmapi=1:length(mapnames)
    mapsobj = eval( [ 'rawfeatmaps.' mapnames{fmapi} ';'] );
    numtypes = mapsobj.info.numtypes;
    mapweights(fmapi) = mapsobj.info.weight;
    map_types{fmapi} = mapsobj.description;
    for typei = 1 : numtypes
        if ( param.activationType == 1 )
            for lev = param.levels                
                mymessage(param,'making a graph-based activation (%s) feature map.\n',mapnames{fmapi});
                i = i + 1;
                [allmaps{i}.map,tmp] = graphsalapply( mapsobj.maps.val{typei}{lev} , ...
                    grframe, param.sigma_frac_act , 1 , 2 , param.tol );
                allmaps{i}.maptype = [ fmapi typei lev ];
            end
        else
            for centerLevel = param.ittiCenterLevels
                for deltaLevel = param.ittiDeltaLevels
                    mymessage(param,'making a itti-style activation (%s) feature map using center-surround subtraction.\n',mapnames{fmapi});
                    i = i + 1;                    
                    center_ = mapsobj.maps.origval{typei}{centerLevel};
                    sz_ = size(center_);
                    surround_ = imresize( mapsobj.maps.origval{typei}{centerLevel+deltaLevel}, sz_ , 'bicubic' );                    
                    allmaps{i}.map = (center_ - surround_).^2;
                    allmaps{i}.maptype = [ fmapi centerLevel deltaLevel ];
                end
            end
        end
    end
end

%%%% 
%%%% STEP 3 : normalize activation maps
%%%%

mymessage(param,'normalizing activation maps...\n');
norm_maps = {};
for i=1:length(allmaps)
    mymessage(param,'normalizing a feature map (%d)... ', i);
    if ( param.normalizationType == 1 )
        mymessage(param,' using fast raise to power scheme\n ', i);
        algtype = 4;
        [norm_maps{i}.map,tmp] = graphsalapply( allmaps{i}.map , grframe, param.sigma_frac_norm, param.num_norm_iters, algtype , param.tol );        
    elseif ( param.normalizationType == 2 )
        mymessage(param,' using graph-based scheme\n');
        algtype = 1;
        [norm_maps{i}.map,tmp] = graphsalapply( allmaps{i}.map , grframe, param.sigma_frac_norm, param.num_norm_iters, algtype , param.tol );                
    else
        mymessage(param,' using global - mean local maxima scheme.\n');
        norm_maps{i}.map = maxNormalizeStdGBVS( mat2gray(imresize(allmaps{i}.map,param.salmapsize, 'bicubic')) );
    end
    norm_maps{i}.maptype = allmaps{i}.maptype;
end

%%%% 
%%%% STEP 4 : average across maps within each feature channel
%%%%

comb_norm_maps = {};
cmaps = {};
for i=1:length(mapnames), cmaps{i}=0; end
Nfmap = cmaps;

mymessage(param,'summing across maps within each feature channel.\n');
for j=1:length(norm_maps)
  map = norm_maps{j}.map;
  fmapi = norm_maps{j}.maptype(1);
  Nfmap{fmapi} = Nfmap{fmapi} + 1;
  cmaps{fmapi} = cmaps{fmapi} + map;
end
%%% divide each feature channel by number of maps in that channel
for fmapi = 1 : length(mapnames)
  if ( param.normalizeTopChannelMaps) 
      mymessage(param,'Performing additional top-level feature map normalization.\n');
      if ( param.normalizationType == 1 )
          algtype = 4;
          [cmaps{fmapi},tmp] = graphsalapply( cmaps{fmapi} , grframe, param.sigma_frac_norm, param.num_norm_iters, algtype , param.tol );
      elseif ( param.normalizationType == 2 )
          algtype = 1;
          [cmaps{fmapi},tmp] = graphsalapply( cmaps{fmapi} , grframe, param.sigma_frac_norm, param.num_norm_iters, algtype , param.tol );
      else
        cmaps{fmapi} = maxNormalizeStdGBVS( cmaps{fmapi} );
      end
  end
  comb_norm_maps{fmapi} = cmaps{fmapi};
end

%%%% 
%%%% STEP 5 : sum across feature channels
%%%%

mymessage(param,'summing across feature channels into master saliency map.\n');
master_idx = length(mapnames) + 1;
comb_norm_maps{master_idx} = 0;
for fmapi = 1 : length(mapnames)
  mymessage(param,'adding in %s map with weight %0.3g (max = %0.3g)\n', map_types{fmapi}, mapweights(fmapi) , max( cmaps{fmapi}(:) ) );
  comb_norm_maps{master_idx} = comb_norm_maps{master_idx} + cmaps{fmapi} * mapweights(fmapi);
end
master_map = comb_norm_maps{master_idx};
master_map = attenuateBordersGBVS(master_map,4);
master_map = mat2gray(master_map);

%%%%
%%%% STEP 6: blur for better results
%%%%
blurfrac = param.blurfrac;
if ( param.useIttiKochInsteadOfGBVS )
  blurfrac = param.ittiblurfrac;
end
if ( blurfrac > 0 )
  mymessage(param,'applying final blur with with = %0.3g\n', blurfrac);
  k = mygausskernel( max(size(master_map)) * blurfrac , 2 );
  master_map = myconv2(myconv2( master_map , k ),k');
  master_map = mat2gray(master_map);
end

if ( param.unCenterBias )  
  invCB = load('invCenterBias');
  invCB = invCB.invCenterBias;
  centerNewWeight = 0.5;
  invCB = centerNewWeight + (1-centerNewWeight) * invCB;
  invCB = imresize( invCB , size( master_map ) );
  master_map = master_map .* invCB;
  master_map = mat2gray(master_map);
end

%%%% 
%%%% save descriptive, rescaled (0-255) output for user
%%%%

feat_maps = {};
for i = 1 : length(mapnames)
  feat_maps{i} = mat2gray(comb_norm_maps{i});
end

intermed_maps = {};
for i = 1 : length(allmaps)
 allmaps{i}.map = mat2gray( allmaps{i}.map );
 norm_maps{i}.map = mat2gray( norm_maps{i}.map );
end

intermed_maps.featureActivationMaps = allmaps;
intermed_maps.normalizedActivationMaps = norm_maps;
master_map_resized = mat2gray(imresize(master_map,[size(img,1) size(img,2)]));

out = {};
out.master_map = master_map;
out.master_map_resized = master_map_resized;
out.top_level_feat_maps = feat_maps;
out.map_types = map_types;
out.intermed_maps = intermed_maps;
out.rawfeatmaps = rawfeatmaps;
out.paramsUsed = param;
if ( param.saveInputImage )
    out.inputimg = img;
end

ColorTransferLib/Evaluation/VSI/gbvs/makeGBVSParams.m

function p = makeGBVSParams()

p = {};

%%%%%%%%%%%%% general  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

p.salmapmaxsize = 32;             % size of output saliency maps (maximum dimension)
                                  % don't set this too high (e.g., >60)
                                  % if you want a saliency map at the
                                  % original image size, just used rescaled
                                  % saliency map
                                  % (out.master_map_resized in gbvs())

p.verbose = 0;                    % turn status messages on (1) / off (0)
p.verboseout = 'screen';          % = 'screen' to echo messages to screen
                                  % = 'myfile.txt' to echo messages to file                                   

p.saveInputImage = 0;             % save input image in output struct
                                  % (can be convenient, but is wasteful
                                  %  to store uncompressed image data
                                  %  around)

p.blurfrac = 0.02;                % final blur to apply to master saliency map
                                  % (in standard deviations of gaussian kernel,
                                  %  expressed as fraction of image width)
                                  % Note: use value 0 to turn off this feature.

%%%%%%%%%%%%% feature channel parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

p.channels = 'DIO';               % feature channels to use encoded as a string
                                  % these are available:
                                  %   C is for Color
                                  %   I is for Intensity
                                  %   O is for Orientation
                                  %   R is for contRast
                                  %   F is for Flicker
                                  %   M is for Motion
                                  %   D is for DKL Color (Derrington Krauskopf Lennie) ==
                                  %     much better than C channel
                                  % e.g., 'IR' would be only intensity and
                                  %       contrast, or
                                  % 'CIO' would be only color,int.,ori. (standard)
                                  % 'CIOR' uses col,int,ori, and contrast

p.colorWeight = 1;                % weights of feature channels (do not need to sum to 1). 
p.intensityWeight = 1;             
p.orientationWeight = 1;
p.contrastWeight = 1;
p.flickerWeight = 1;
p.motionWeight = 1;
p.dklcolorWeight = 1;

p.gaborangles = [ 0 45 90 135 ];  % angles of gabor filters
p.contrastwidth = .1;             % fraction of image width = length of square side over which luminance variance is 
                                  % computed for 'contrast' feature map
                                  % LARGER values will give SMOOTHER
                                  %   contrast maps

p.flickerNewFrameWt = 1;          % (should be between 0.0 and 1.0)
                                  % The flicker channel is the abs() difference
                                  % between the *previous frame estimate* and
                                  % current frame.
                                  % This parameter is the weight used
                                  % to update the previous frame estimate. 
                                  % 1 == set previous frame to current
                                  %      frame
                                  % w == set previous frame to w * present
                                  %      + (1-w) * previous estimate

p.motionAngles = [ 0 45 90 135 ]; 
                                  % directions of motion for motion channel
                                  %  --> 0 , /^ 45 , |^ 90 , ^\ 135 , etc. 
                                  % question: should use more directions?
                                  % e.g., 180, 225, 270, 315, ?

%%%%%%%%%%%%% GBVS parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

p.unCenterBias = 0;               % turned off (0) by default. Attempts to undo some emergent
                                  % center bias in GBVS (by pointwise-multiplying final saliency map by 
                                  % an inverse-bias map).

p.levels = [2 3 4];               % resolution of feature maps relative to original image (in 2^-(n-1) fractions)
                                  % (default [ 2 3 4]) .. maximum level 9 is allowed
                                  % these feature map levels will be used
                                  % if graph-based activation is used.
                                  % otherwise, the ittiCenter/Delta levels
                                  % are (see below)
                                  % minimum value allowed  = 2
                                  % maximum value allowed  = 9

p.multilevels = [];               % [1 2] corresponds to 2 additional node lattices ,
                                  % ... one at half and one at quarter size
                                  % use [] for single-resolution version of algorithm.

p.sigma_frac_act = 0.15;          % sigma parameter in activation step of GBVS (as a fraction of image width) - default .15
p.sigma_frac_norm = 0.06;         % sigma parameter in normalizaiton step of GBVS (as a fraction of image width) - default .06
p.num_norm_iters = 1;             % number of normalization iterations in GBVS - default 1

p.tol = .0001;                    % tol controls a stopping rule on the computation of the equilibrium distribution (principal eigenvector)
                                  % the higher it is, the faster the algorithm runs, but the more approximate it becomes.
                                  % it is used by algsrc/principalEigenvectorRaw.m - default .0001

p.cyclic_type = 2;                % this should *not* be changed (non-cyclic boundary rules)

%%%%%%%%%% Parameters to use Itti/Koch and/or Simpler Saliency Algorith %%%%

p.useIttiKochInsteadOfGBVS = 0;   % use value '0' for Graph-Based Visual Saliency
                                  % use value '1' for Itti Koch algorithm: 
                                  % "A Model of Saliency-Based Visual
                                  % Attention for Rapid Scene Analysis",
                                  % PAMI 1998

p.activationType = 1;             % 1 = graph-based activation (default)
                                  % 2 = center-surround activation (given
                                  %     by ittiCenter/DeltaLevels below)
                                  % ( type 2 used if useIttiKoch..= 1 )

p.normalizationType = 1;          % 1 = simplest & fastest. raises map values to a power before adding them together (default)
                                  % 2 = graph-based normalization scheme (no longer recommended)
                                  % 3 = normalization by (M-m)^2, where M =
                                  %     global maximum. m = avg. of local
                                  %     maxima
                                  % ( type 3 used if useIttiKoch..=1 )

p.normalizeTopChannelMaps = 0;    % this specifies whether to normalize the 
                                  % top-level feature map of each
                                  % channel... (in addition to normalizing
                                  % maps across scales within a channel)
                                  % 0 = don't do it (default)
                                  % 1 = do it. (used by ittiKoch scheme)

p.ittiCenterLevels = [ 2 3 ];     % the 'c' scales for 'center' maps

p.ittiDeltaLevels = [ 2 ];        %  the 'delta' in s=c+delta levels for 'surround' scales
                                  %  NOTE: for more faithful implementation , use deltaLevels = [ 2 3 ], not [ 2 ] 
                                  %  however, 3 can be problematic for images < 640 in width or height

p.ittiblurfrac = 0.03;            % apply final blur to master saliency map
                                  % (not in original Itti/Koch algo. but improves eye-movement predctions)

ColorTransferLib/Evaluation/VSI/gbvs/ittikochmap.m

function out = ittikochmap( img )

params = makeGBVSParams;
params.useIttiKochInsteadOfGBVS = 1;
params.channels = 'CIO';
params.verbose = 1;
params.unCenterBias = 0;

%
% uncomment the line below (ittiDeltaLevels = [2 3]) for more faithful implementation 
% (however, known to give crappy results for small images i.e. < 640 in height or width )
%
% params.ittiDeltaLevels = [ 2 3 ];
%

if ( strcmp(class(img),'char') == 1 ) img = imread(img); end
if ( strcmp(class(img),'uint8') == 1 ) img = double(img)/255; end

params.salmapmaxsize = round( max(size(img))/8 );

out = gbvs(img,params);

ColorTransferLib/Evaluation/VSI/gbvs/readme.txt

Graph-Based Visual Saliency (MATLAB source code)
http://www.klab.caltech.edu/~harel/share/gbvs.php

Jonathan Harel
jonharel@gmail.com
California Institute of Technology

========================================================================================

This is an installation and general help file for the saliency map MATLAB code here.

========================================================================================

What you can do with this code:

(1) Compute a "Graph-Based Visual Saliency" map for an image or image sequence (video)
    (as described in J. Harel, C. Koch, and P. Perona. "Graph-Based Visual Saliency", 
    NIPS 2006
    http://www.klab.caltech.edu/~harel/pubs/gbvs_nips.pdf)

(2) Compute the standard Itti, Koch, Niebur (PAMI 1998) saliency map.

(3) Compute modified versions of the above by altering the input parameters.

========================================================================================

Step-by-step start-up procedure:

(1) Add gbvs to your path:

    Change into the directory containing this file, and enter at the matlab prompt:

     >> gbvs_install

    If you are on a shared machine, you may get an error message such as:

      Warning: Unable to save path to file '/opt/matlab/toolbox/local/pathdef.m'
       In savepath at 162
       In gbvs_install at 5

    In that case, comment out the savepath (i.e., 'savepath' => '% savepath') 
    command in gbvs_install.m, and add this line to your startup.m file:

    run ???/gbvs_install

    where "???" is replaced by the main gbvs/ directory, which contains the
    gbvs_install function

(2) Now you are ready to compute GBVS maps:

   Demonstrations:

      >> simplest_demonstration

     see demo/demonstration.m for more complicated demo or run:
     [Note: if you get an error, see point (3) below]

      >> demonstration

   Basic Usage Example:

     >> out = gbvs( 'samplepics/1.jpg' );

   You can also compute an Itti/Koch map as follows:

     >> out = ittikochmap( 'samplepics/1.jpg' );

   Or, to call GBVS simplified to some extent (e.g. no Orientation channel) so that it runs faster, use

     >> out = gbvs_fast( 'samplepics/1.jpg');

   Now, out.master_map contains your saliency map, and out.master_map_resized is
   this saliency map interpolated (bicubic) to the resolution of the original 
   image.

   For video (not static images):
    You need to pass into gbvs() previous frame information, which is returned
    on output at every call to gbvs(). 

   See demo/flicker_motion_demo.m

   Here is the heart of it:

    motinfo = [];   % previous frame information, initialized to empty
    for i = 1 : N
      [out{i} motinfo] = gbvs( fname{i}, param , motinfo );
    end

(3) If you are not on 32 or 64 bit Windows, or on Intel-based Mac, or 32 or 64 bit Linux,
    and calling simplest_demonstration results in an error, you may have to compile
    a few .cc source code files into binary "mex" format.

    You can do that as follows. From the gbvs/ directory, in matlab, run:

     >> gbvs_compile

    If this works properly, there should be no output at all, and you're done!
    Then go back to step (2), i.e. try running the demonstration.

    Error note:
      If this is your first time compiling mex files, you may have to run:

        >> mex -setup

      and follow the instructions (typically, enter a number, to select a co-
      mpiler. then you can run "gbvs_compile"; if it doesn't work, run 
      "mex -setup" again to select a different compiler, run "gbvs_compile" 
      again, etc.)

========================================================================================

Helpful Notes:

(1) inputs of gbvs():

     * the first argument to gbvs() can be an image name or image array
     * there is an optional, second, parameters argument

(2) outputs of gbvs():

     * all put into a single structure with various descriptive fields.
     * the GBVS map: master_map
      (interpolated to the resolution of the input image: master_map_resized)
     * master saliency map for each channel: feat_maps (and their names, 
       map_types)
     * all intermediate maps to create the previous two (intermed_maps). see 
      gbvs.m for details

(3) the parameter argument:

     * initialized by makeGBVSParams.m -- read that for details.

      Some very sparse notes on fields of the parameter argument:

        sigma_frac_act    controls the spatial spread of the function modulating 
                          weights between different image locations (in image widths).
                          greater value means greater connectivity between distant
                          locations.

        tol               tolerance parameter. governs how accurately the princi-
                          pal eigenvector calculation is performed. change it to 
                          higher values to make things run faster.

        levels            the resolution of the feature maps used to compute the
                          final master map, relative to the original image size

(4) Notes on feature maps:

     * are produced by util/getFeatureMaps.m

     * by default, color, intensity, orientation  maps are computed.

       which channels are used is controlled by the parameters argument. in part-
       icular, you can choose which of these is included by editing the 
       params.channels string (see makeGBVSParams.m). you can set
       their relative weighting also in the parameters.

       If you want to introduce a new feature channel, put a new function into 
       util/featureChannels/ . Make sure to edit the channels string appropria-
       tely. Follow pattern of other channels for proper implementation.

(5) If you want to compare saliency maps to fixations (e.g., inferred from
       scanpaths recorded by an eye-tracker), use:

         >> score = rocScoreSaliencyVsFixations(salmap,X,Y,origimgsize)

        This outputs ROC Area-Under-Curve Score between a saliency map and fixat-
        ions.

         salmap       : a saliency map
         X            : vector of X locations of fixations in original image
         Y            : vector of Y locations of fixations in original image
         origimgsize  : size of original image (should have same aspect ratio as
                        saliency map)

========================================================================================

Credits:

(1) saltoolbox/ directory -- adapted from: Dirk Walther, http://www.saliencytoolbox.net

(2) Thanks to Alexander G. Huth for help with making heatmap_overlay.m readable.

========================================================================================

Revision History

first authored 8/31/2006
Revised 4/25/2008
Revised 6/5/2008
Revised 6/26/2008 
	added Itti/Koch algorithm
Revised 8/25/2008 
	added Flicker/Motion channels
Revised 11/3/2008 
	added myconv2
Revised 2/19/2010 
	added initcache to reduce initialization times
Revised 3/18/2010 
	added attenuateBordersGBVS to O_orientation call
Revised 1/17/2011 
	added attenuateBordersGBVS to master_map. 
	changed boundary condition in padImage 
	changed ittiDeltaLevels for ittiKoch to just [2] by default
	removed Intensity channel from gbvs_fast	
Revised 10/24/2011
	added unCenterBias to parameters, turned it on by default
Revised 7/24/2012
	show_imgnmap returns output. for win users: initGBVS uses fullfile.

ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/attenuateBordersGBVS.m

function result = attenuateBordersGBVS(data,borderSize)
% attentuateBorders - linearly attentuates the border of data.
%
% result = attenuateBorders(data,borderSize)
%   linearly attenuates a border region of borderSize
%   on all sides of the 2d data array

% This file is part of the SaliencyToolbox - Copyright (C) 2006
% by Dirk Walther and the California Institute of Technology.
% The Saliency Toolbox is released under the GNU General Public 
% License. See the enclosed COPYRIGHT document for details. 
% For more information about this project see: 
% http://www.saliencytoolbox.net

result = data;
dsz = size(data);

if (borderSize * 2 > dsz(1)) borderSize = floor(dsz(1) / 2); end
if (borderSize * 2 > dsz(2)) borderSize = floor(dsz(2) / 2); end
if (borderSize < 1) return; end

bs = [1:borderSize];
coeffs = bs / (borderSize + 1);

% top and bottom
rec = repmat(coeffs',1,dsz(2));
result(bs,:) = result(bs,:) .* rec;
range = dsz(1) - bs + 1;
result(range,:) = result(range,:) .* rec;

% left and right
rec = repmat(coeffs,dsz(1),1);
result(:,bs) = result(:,bs) .* rec;
range = dsz(2) - bs + 1;
result(:,range) = result(:,range) .* rec;

ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/maxNormalizeStdGBVS.m

function result = maxNormalizeStdGBVS(data)
% maxNormalizeStd - normalization based on local maxima.
% result = maxNormalizeStd(data)
%    Normalize data by multiplying it with 
%    (max(data) - avg(localMaxima))^2 as described in;
%    L. Itti, C. Koch, E. Niebur, A Model of Saliency-Based 
%    Visual Attention for Rapid Scene Analysis, IEEE PAMI, 
%    Vol. 20, No. 11, pp. 1254-1259, Nov 1998.
%
% result = maxNormalizeStd(data,minmax)
%    Specify a dynamic for the initial maximum normalization 
%    of the input data (default: [0 10]).
%
% See also maxNormalize, maxNormalizeFancy, maxNormalizeFancyFast, makeSaliencyMap.

% This file is part of the Saliency Toolbox - Copyright (C) 2005
% by Dirk Walther and the California Institute of Technology.
% The Saliency Toolbox is released under the GNU General Public 
% License. See the enclosed COPYRIGHT document for details. 
% For more information about this project see: 
% http://klab.caltech.edu/~walther/SaliencyToolbox

%
% modified by jonathan harel 2008 for GBVS code
%  .. simplified

M = 10;

data = mat2gray( data ) * M;
thresh = M / 10;
[lm_avg,lm_num,lm_sum] = mexLocalMaximaGBVS(data,thresh);

if (lm_num > 1)
  result = data * (M - lm_avg)^2;
elseif (lm_num == 1)
  result = data * M .^ 2;
else
  result = data;
end

ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/mexLocalMaximaGBVS.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

double getVal(double* img, int x, int y, int w, int h);
void getLocalMaxima(double* img, double thresh, int *lm_num, double *lm_sum, int w, int h);

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[])
{  
  // input
  double* img, *thresh;

  // output
  double lm_avg, lm_sum; int lm_num;

  double* tmp;

  img = mxGetPr( prhs[0] );
  thresh = mxGetPr( prhs[1] );

  getLocalMaxima( img, thresh[0] , &lm_num, &lm_sum , mxGetN(prhs[0]) , mxGetM(prhs[0]) );

  if (lm_sum > 0) lm_avg = (double)lm_sum / (double)lm_num;
  else lm_avg = 0.0;

  plhs[0] = mxCreateDoubleMatrix(1, 1, mxREAL); //mxReal is our data-type
  plhs[1] = mxCreateDoubleMatrix(1, 1, mxREAL); //mxReal is our data-type
  plhs[2] = mxCreateDoubleMatrix(1, 1, mxREAL); //mxReal is our data-type

  tmp = mxGetPr(plhs[0]); tmp[0] = lm_avg;
  tmp = mxGetPr(plhs[1]); tmp[0] = lm_num;
  tmp = mxGetPr(plhs[2]); tmp[0] = lm_sum;
}

double getVal(double* img, int x, int y, int w, int h) 
{
  double* ptr = img + x * h + y;  
  return *ptr;
}

void getLocalMaxima(double* img, double thresh, int *lm_num, double *lm_sum, int w, int h)
{
  int i,j;
  double val;
  // then get the mean value of the local maxima:
  *lm_sum = 0.0; *lm_num = 0;

  for (j = 1; j < h - 1; j ++)
    for (i = 1; i < w - 1; i ++)
      {
        val = getVal(img,i,j,w,h);
        if (val >= thresh &&
            val >= getVal(img,i-1, j,w,h) &&
            val >= getVal(img,i+1, j,w,h) &&
            val >= getVal(img,i, j+1,w,h) &&
            val >= getVal(img,i, j-1,w,h))  // local max
          {
            *lm_sum += val;
            (*lm_num)++;
          }
      }
  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/makeGaborFilterGBVS.m

function filter = makeGaborFilterGBVS(gaborParams, angle, phase, varargin)
% makeGaborFilter - returns a 2d Gabor filter.
%
% filter = makeGaborFilter(gaborParams, angle, phase, makeDisc)
%    Returns a two-dimensional Gabor filter with the parameter:
%    gaborParams - a struct with the following fields:
%       filterPeriod - the period of the filter in pixels,
%       elongation - the ratio of length versus width,
%       filterSize - the size of the filter in pixels,
%       stddev - the standard deviation of the Gaussian in pixels.
%    angle - the angle of orientation, in degrees,
%    phase - the phase of the filter, in degrees,
%    makeDisc - if 1, enforce a disc-shaped filter, i.e. set all values
%               outside of a circle with diameter gaborParams.filterSize to 0.
%
% filter = makeGaborFilter(gaborParams, angle, phase)
%    Returns a two-dimensional Gabor filter, assuming makeDisc = 0.
%
% See also gaborFilterMap, defaultSaliencyParams.

% This file is part of the Saliency Toolbox - Copyright (C) 2006
% by Dirk Walther and the California Institute of Technology.
% The Saliency Toolbox is released under the GNU General Public 
% License. See the enclosed COPYRIGHT document for details. 
% For more information about this project see: 
% http://www.saliencytoolbox.net

if isempty(varargin)
  makeDisc = 0;
else
  makeDisc = varargin{1};
end

% repare parameters
major_stddev = gaborParams.stddev;
minor_stddev = major_stddev * gaborParams.elongation;
max_stddev = max(major_stddev,minor_stddev);

sz = gaborParams.filterSize;
if (sz == -1)
  sz = ceil(max_stddev*sqrt(10));
else
  sz = floor(sz/2); 
end

psi = pi / 180 * phase;
rtDeg = pi / 180 * angle;

omega = 2 * pi / gaborParams.filterPeriod;
co = cos(rtDeg);
si = -sin(rtDeg);
major_sigq = 2 * major_stddev^2;
minor_sigq = 2 * minor_stddev^2;

% prepare grids for major and minor components
vec = [-sz:sz];
vlen = length(vec);
vco = vec*co;
vsi = vec*si;

major = repmat(vco',1,vlen) + repmat(vsi,vlen,1);
major2 = major.^2;
minor = repmat(vsi',1,vlen) - repmat(vco,vlen,1);
minor2 = minor.^2;

% create the actual filter
result = cos(omega * major + psi) .* ...
exp(-major2 / major_sigq ...
    -minor2 / minor_sigq);

% enforce disc shape?
if (makeDisc)
  result((major2+minor2) > (gaborParams.filterSize/2)^2) = 0;
end

% normalization
filter = result - mean(result(:));
filter = filter / sqrt(sum(filter(:).^2));

ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/mySubsample.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

void lowPass6yDecY(float* sptr, float* rptr, int w, int hs);
void lowPass6xDecX(float* sptr, float* rptr, int ws, int h);
void double2float(double *a, float* b, int N) {
  int i; for (i=0;i<N;i++) b[i] = (float)a[i]; 
};
void float2double(float* a, double* b, int N) {
  int i; for (i=0;i<N;i++) b[i] = (float)a[i]; 
};

/* the main program */
void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[])
{
  mxArray *img;
  float *decx, *decxy, *imgF;
  double *outArray, *imgD;
  int w, h, wr, hr;

  img = (mxArray*)prhs[0];
  w = mxGetN(img);
  h = mxGetM(img);

  wr = w / 2; if ( wr == 0 ) wr = 1;
  hr = h / 2; if ( hr == 0 ) hr = 1;

  if ( (w > 10) && (h > 10) ) {  

    imgF = (float*)mxMalloc(sizeof(float)*h*w);
    decx = (float*)mxMalloc(sizeof(float)*h*wr);
    decxy = (float*)mxMalloc(sizeof(float)*hr*wr);

    imgD = mxGetPr(img);
    double2float( imgD, imgF, h*w );
    lowPass6xDecX( imgF , decx, w, h );
    lowPass6yDecY( decx , decxy, wr, h );

    plhs[0] = mxCreateDoubleMatrix(hr, wr, mxREAL); 
    outArray = mxGetPr(plhs[0]);
    float2double( decxy, outArray, hr * wr );

    mxFree(imgF);
    mxFree(decx);
    mxFree(decxy);

  } else {
    plhs[0] = mxCreateDoubleMatrix(h, w, mxREAL); 
    outArray = mxGetPr(plhs[0]);
    memcpy( outArray , mxGetPr(img) , sizeof(double) * h * w );
  }

}

// ######################################################################
// kernel: 1 5 10 10 5 1
void lowPass6yDecY(float* sptr, float* rptr, int w, int hs)
{
  int x, y;
  int hr = hs / 2;
  if (hr == 0) hr = 1;

  /* if (hs <= 1)
     result = src;
     else 
  */ 
  if (hs == 2)
    for (x = 0; x < w; ++x)
      {
        // use kernel [1 1]^T / 2
        *rptr++ = (sptr[0] + sptr[1]) / 2.0;
        sptr += 2;
      }
  else if (hs == 3)
    for (x = 0; x < w; ++x)
      {
        // use kernel [1 2 1]^T / 4
        *rptr++ = (sptr[0] + sptr[1] * 2.0 + sptr[2]) / 4.0;
        sptr += 3;
      }
  else // general case with hs >= 4
    for (x = 0; x < w; ++x)
      {
        // top most point - use kernel [10 10 5 1]^T / 26
        *rptr++ = ((sptr[0] + sptr[1]) * 10.0 + 
		   sptr[2] * 5.0 + sptr[3]) / 26.0;
        //++sptr;

        // general case
        for (y = 0; y < (hs - 5); y += 2)
          {
            // use kernel [1 5 10 10 5 1]^T / 32
            *rptr++ = ((sptr[1] + sptr[4])  *  5.0 +
                       (sptr[2] + sptr[3])  * 10.0 +
                       (sptr[0] + sptr[5])) / 32.0;
            sptr += 2;
          }

        // find out how to treat the bottom most point
        if (y == (hs - 5))
          {
            // use kernel [1 5 10 10 5]^T / 31
	    *rptr++ = ((sptr[1] + sptr[4])  *  5.0 +
		       (sptr[2] + sptr[3])  * 10.0 +
		       sptr[0])            / 31.0;
            sptr += 5;
          }
        else
          {
            // use kernel [1 5 10 10]^T / 26
            *rptr++ = ( sptr[0] + sptr[1]  *  5.0 +
			(sptr[2] + sptr[3]) * 10.0) / 26.0;
            sptr += 4;
          }
      }
}

// ######################################################################
// kernel: 1 5 10 10 5 1
void lowPass6xDecX(float* sptr, float* rptr, int ws, int h)
{
  int x,y;
  const int h2 = h * 2, h3 = h * 3, h4 = h * 4, h5 = h * 5;
  int wr = ws / 2;
  if (wr == 0) wr = 1;

  /* if (ws <= 1)
     result = src;
     else */
  if (ws == 2)
    for (y = 0; y < h; ++y)
      {
        // use kernel [1 1] / 2
        *rptr++ = (sptr[0] + sptr[h]) / 2.0;
        ++sptr;
      }
  else if (ws == 3)
    for (y = 0; y < h; ++y)
      {
        // use kernel [1 2 1] / 4
        *rptr++ = (sptr[0] + sptr[h] * 2.0 + sptr[h2]) / 4.0;
        ++sptr;
      }
  else // general case for ws >= 4
    {
      // left most point - use kernel [10 10 5 1] / 26
      for (y = 0; y < h; ++y)
        {
          *rptr++ = ((sptr[0] + sptr[h]) * 10.0 + 
		     sptr[h2] * 5.0 + sptr[h3]) / 26.0;
          ++sptr;
        }
      sptr -= h;

      // general case
      for (x = 0; x < (ws - 5); x += 2)
        {
          for (y = 0; y < h; ++y)
            {
              // use kernel [1 5 10 10 5 1] / 32
              *rptr++ = ((sptr[h]  + sptr[h4])  *  5.0 +
                         (sptr[h2] + sptr[h3])  * 10.0 +
                         (sptr[0]  + sptr[h5])) / 32.0;
              ++sptr;
            }
          sptr += h;
        }

      // find out how to treat the right most point
      if (x == (ws - 5))
        for (y = 0; y < h; ++y)
          {
            // use kernel [1 5 10 10 5] / 31
            *rptr++ = ((sptr[h]  + sptr[h4])  *  5.0 +
                       (sptr[h2] + sptr[h3])  * 10.0 +
		       sptr[0]) / 31.0;
            ++sptr;
          }
      else
        for (y = 0; y < h; ++y)
          {
            // use kernel [1 5 10 10] / 26
            *rptr++ = ( sptr[0]  + sptr[h]   * 5.0 + 
			(sptr[h2] + sptr[h3]) * 10.0) / 26.0;
            ++sptr;
          }
    }
}

ColorTransferLib/Evaluation/VSI/gbvs/saltoolbox/safeDivideGBVS.m

function result = safeDivideGBVS(arg1,arg2)
% safeDivide - divides two arrays, checking for 0/0.
%
% result = safeDivide(arg1,arg2)
%    returns arg1./arg2, where 0/0 is assumed to be 0 instead of NaN.

% This file is part of the SaliencyToolbox - Copyright (C) 2006
% by Dirk Walther and the California Institute of Technology.
% The Saliency Toolbox is released under the GNU General Public 
% License. See the enclosed COPYRIGHT document for details. 
% For more information about this project see: 
% http://www.saliencytoolbox.net

ze = (arg2 == 0);
arg2(ze) = 1;
result = arg1./arg2;
result(ze) = 0;

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/C_color.m

function out = C_color( fparam, img , imgR, imgG, imgB, typeidx )

if ( nargin == 1 )

  out.weight = fparam.colorWeight;

  out.numtypes = 2;
  out.descriptions{1} = 'Blue-Yellow';
  out.descriptions{2} = 'Red-Green';

else
  if ( typeidx ) == 1
    out.map = safeDivideGBVS( abs(imgB-min(imgR,imgG)) , img );
  else
    out.map = safeDivideGBVS( abs(imgR-imgG) , img );
  end
end

ColorTransferLib/Evaluation/VSI/gbvs/util/areaROC.m

function [A,tmp] = areaROC( p )
tmp = -1;
p = getBestRows(p);
xy = sortrows([p(:,2) p(:,1)]);

x = xy(:,1);
y = xy(:,2);

x = [ 0 ; x ; 1 ];
y = [ 0 ; y ; 1 ];

A = trapz( x , y );

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/I_intensity.m

function out = I_intensity( fparam , img , imgR, imgG, imgB, typeidx )

if ( nargin == 1)  
  out.weight = fparam.intensityWeight;
  out.numtypes = 1;
  out.descriptions{1} = 'Intensity';    
else
  out.map = img;
end

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/D_dklcolor.m

function out = D_dklcolor( fparam, img , imgR, imgG, imgB, typeidx )

% CHROMATIC MECHANISMS IN LATERAL 
% GENICULATE NUCLEUS OF MACAQUE 
% BY A. M. DERRINGTON, J. KRAUSKOPF AND P. LENNIE 

% from the abstract:
%  (a) an axis along 
%  which only luminance varies, without change in chromaticity, (b) a 'constant B' axis 
%  along which chromaticity varies without changing the excitation of blue-sensitive (B) 
%  cones, (c) a 'constant R & G' axis along which chromaticity varies without change 
%  in the excitation of red-sensitive (R) or green-sensitive (G) cones

if ( nargin == 1 )
  out.weight = fparam.dklcolorWeight;
  out.numtypes = 3;
  out.descriptions{1} = 'DKL Luminosity Channel';
  out.descriptions{2} = 'DKL Color Channel 1';
  out.descriptions{3} = 'DKL Color Channel 2';
else
  rgb = repmat( imgR , [ 1 1 3 ] );
  rgb(:,:,2) = imgG;
  rgb(:,:,3) = imgB;
  dkl = rgb2dkl( rgb );

  if ( typeidx == 1 )
    out.map = dkl(:,:,1);
  elseif ( typeidx == 2 )
    out.map = dkl(:,:,2);
  elseif ( typeidx == 3 )
    out.map = dkl(:,:,3);
  end
end

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/M_motion.m

function out = M_motion( param, img , img_prev, prev_img_shift , ti )

if ( nargin == 1 )
    out.weight = param.motionWeight;
    out.numtypes = length( param.motionAngles );
    for i = 1 : length( param.motionAngles ),
        out.descriptions{i} = sprintf('Motion Direction %g',param.motionAngles(i));
    end    
else    
    out.imgShift = shiftImage( img , param.motionAngles(ti) );    
    out.map = abs( img .* prev_img_shift - img_prev .* out.imgShift );

    % this rule comes from:
    % http://ilab.usc.edu/publications/doc/Itti_etal03spienn.pdf
    % "Values smaller than 3.0 are set to zero"
    % note: this doens't seem to work ? (9/4/09)
    % out.map( out.map < 3/255 ) = 0;    
end

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/O_orientation.m

function out = O_orientation( fparam , img , imgR, imgG, imgB, typeidx )

if ( nargin == 1 )
  out.weight = fparam.orientationWeight;  
  out.numtypes = length( fparam.gaborFilters );
  for i = 1 : length( fparam.gaborFilters ),
    out.descriptions{i} = sprintf('Gabor Orientation %g',fparam.gaborangles(i));
  end
else
  gaborFilters = fparam.gaborFilters;
  j = typeidx;
  f0 = myconv2(img,gaborFilters{j}.g0);
  f90 = myconv2(img,gaborFilters{j}.g90);
  out.map = abs(f0) + abs(f90);
  out.map = attenuateBordersGBVS(out.map,13);
end

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/F_flicker.m

function out = F_flicker( param, img , img_prev, prev_img_shift , ti )

if ( nargin == 1 )
    out.weight = param.flickerWeight;
    out.numtypes = 1;
    out.descriptions{1} = 'Flicker';
else
    out.map = abs( img - img_prev );
end

ColorTransferLib/Evaluation/VSI/gbvs/util/getBestRows.m

function pnew = getBestRows( p )

% given collection of p = [ a b ; a1 b1 ; a2 b2 .. ]
% trims out instances where bi = bj , choosing row with maximum a.

bs = unique(p(:,2));
Nbs = length(bs);
pnew = zeros(Nbs,2);
for i=1:Nbs,
    pnew(i,:) = [ max(p(p(:,2)==bs(i),1)) bs(i) ];
end

ColorTransferLib/Evaluation/VSI/gbvs/util/getFeatureMaps.m

function [rawfeatmaps, motionInfo] = getFeatureMaps( img , param , prevMotionInfo )

%
% this computes feature maps for each cannnel in featureChannels/
%

load mypath;

%%%%
%%%% STEP 1 : form image pyramid and prune levels if pyramid levels get too small.
%%%%

mymessage(param,'forming image pyramid\n');

levels = [ 2 : param.maxcomputelevel ];

is_color = (size(img,3) == 3);
imgr = []; imgg = []; imgb = [];
if ( is_color ) [imgr,imgg,imgb,imgi] = mygetrgb( img );
else imgi = img; end

imgL = {};
imgL{1} = mySubsample(imgi);
imgR{1} = mySubsample(imgr); imgG{1} = mySubsample(imgg); imgB{1} = mySubsample(imgb);

for i=levels

    imgL{i} = mySubsample( imgL{i-1} );
    if ( is_color )
        imgR{i} = mySubsample( imgR{i-1} );
        imgG{i} = mySubsample( imgG{i-1} );
        imgB{i} = mySubsample( imgB{i-1} );
    else
        imgR{i} = []; imgG{i} = []; imgB{i} = [];
    end
    if ( (size(imgL{i},1) < 3) | (size(imgL{i},2) < 3 ) )
        mymessage(param,'reached minimum size at level = %d. cutting off additional levels\n', i);
        levels = [ 2 : i ];
        param.maxcomputelevel = i;
        break;
    end

end

%%% update previous frame estimate based on new frame
if ( (param.flickerNewFrameWt == 1) || (isempty(prevMotionInfo) ) )
    motionInfo.imgL = imgL;
else    
    w = param.flickerNewFrameWt;    
    for i = levels,
        %%% new frame gets weight flickerNewFrameWt
        motionInfo.imgL =  w * imgL{i} + ( 1 - w ) * prevMotionInfo.imgL{i};
    end
end

%%%
%%% STEP 2 : compute feature maps
%%%

mymessage(param,'computing feature maps...\n');

rawfeatmaps = {};

%%% get channel functions in featureChannels/directory

channel_files = dir( [pathroot '/util/featureChannels/*.m'] );

motionInfo.imgShifts = {};

for ci = 1 : length(channel_files)

    %%% parse the channel letter and name from filename
    parts = regexp( channel_files(ci).name , '^(?<letter>\w)_(?<rest>.*?)\.m$' , 'names');
    if ( isempty(parts) ), continue; end % invalid channel file name

    channelLetter = parts.letter;
    channelName = parts.rest;
    channelfunc = str2func(sprintf('%s_%s',channelLetter,channelName));
    useChannel = sum(param.channels==channelLetter) > 0;

    if ( ((channelLetter == 'C') || (channelLetter=='D')) && useChannel && (~is_color) )
        mymessage(param,'oops! cannot compute color channel on black and white image. skipping this channel\n');
        continue;
    elseif (useChannel)

        mymessage(param,'computing feature maps of type "%s" ... \n', channelName);

        obj = {};
        obj.info = channelfunc(param);
        obj.description = channelName;

        obj.maps = {};
        obj.maps.val = {};

        %%% call the channelfunc() for each desired image resolution (level in pyramid)
        %%%  and for each type index for this channel.

        for ti = 1 : obj.info.numtypes            
            obj.maps.val{ti} = {};
            mymessage(param,'..pyramid levels: ');
            for lev = levels,                
                mymessage(param,'%d (%d x %d)', lev, size(imgL{lev},1), size(imgL{lev},2));                
                if ( (channelLetter == 'F') || (channelLetter == 'M') )                   
                    if ( ~isempty(prevMotionInfo) )
                        prev_img = prevMotionInfo.imgL{lev};
                    else
                        prev_img = imgL{lev};
                    end

                    if ( ~isempty(prevMotionInfo) && isfield(prevMotionInfo,'imgShifts') && (channelLetter == 'M') )
                      prev_img_shift = prevMotionInfo.imgShifts{ti}{lev};
                    else
                      prev_img_shift = 0;
                    end

                    map = channelfunc(param,imgL{lev},prev_img,prev_img_shift,ti);                    
                    if (isfield(map,'imgShift'))
                       motionInfo.imgShifts{ti}{lev} = map.imgShift; 
                    end                    
                else
                    map = channelfunc(param,imgL{lev},imgR{lev},imgG{lev},imgB{lev},ti);
                end    
                obj.maps.origval{ti}{lev} = map.map;
                map = imresize( map.map , param.salmapsize , 'bicubic' );
                obj.maps.val{ti}{lev} = map;
            end
            mymessage(param,'\n');
        end

        %%% save output to rawfeatmaps structure
        eval( sprintf('rawfeatmaps.%s = obj;', channelName) );

    end
end

ColorTransferLib/Evaluation/VSI/gbvs/util/getIntelligentThresholds.m

function threshs = getIntelligentThresholds( vals );

threshs = unique(vals) - 1e-16;

ColorTransferLib/Evaluation/VSI/gbvs/util/heatmap_overlay.m

% img = image on which to overlay heatmap
% heatmap = the heatmap
% (optional) colorfunc .. this can be 'jet' , or 'hot' , or 'flag'

function omap = heatmap_overlay( img , heatmap, colorfun )

if ( strcmp(class(img),'char') == 1 ) img = imread(img); end
if ( strcmp(class(img),'uint8') == 1 ) img = double(img)/255; end

szh = size(heatmap);
szi = size(img);

if ( (szh(1)~=szi(1)) | (szh(2)~=szi(2)) )
  heatmap = imresize( heatmap , [ szi(1) szi(2) ] , 'bicubic' );
end

if ( size(img,3) == 1 )
  img = repmat(img,[1 1 3]);
end

if ( nargin == 2 )
    colorfun = 'jet';
end
colorfunc = eval(sprintf('%s(50)',colorfun));

heatmap = double(heatmap) / max(heatmap(:));
omap = 0.8*(1-repmat(heatmap.^0.8,[1 1 3])).*double(img)/max(double(img(:))) + repmat(heatmap.^0.8,[1 1 3]).* shiftdim(reshape( interp2(1:3,1:50,colorfunc,1:3,1+49*reshape( heatmap , [ prod(size(heatmap))  1 ] ))',[ 3 size(heatmap) ]),1);
omap = real(omap);

ColorTransferLib/Evaluation/VSI/gbvs/util/makeFixationMask.m

function mask = makeFixationMask( X , Y , origimgsize , salmapsize )

%
% this maps (X,Y) fixation coordinates to fixation mask
%
% given fixation coordinates X and Y in original image coordinates,
% produces mask of same size salmapsize where each location contains
% an integer count of the fixations lying at that location
%

if ( length(X) ~= length(Y) )
  fprintf(2,'makeFixationMask Error: number of X and Y coordinates should be the same!\n');
  mask = [];
  return;
end

scale = salmapsize(1) / origimgsize(1);

X = round(X * scale);
Y = round(Y * scale);

X(X<1) = 1;
X(X>salmapsize(2)) = salmapsize(2);
Y(Y<1) = 1;
Y(Y>salmapsize(1)) = salmapsize(1);

mask = zeros( salmapsize );
for i = 1 : length(X)
  mask( Y(i) , X(i) ) = mask( Y(i) , X(i) ) + 1;
end

ColorTransferLib/Evaluation/VSI/gbvs/util/linearmap.m

function lmap = linearmap(map)
[n,m] = size(map);
lmap = reshape(map, [1 n*m]);

ColorTransferLib/Evaluation/VSI/gbvs/util/myContrast.cc

#include <stdio.h>
#include <stdlib.h>
#include <mex.h>
#include <math.h>
#include <Matrix.h>
#include <string.h>

void mexFunction(int nlhs, mxArray *plhs[], int nrhs, const mxArray *prhs[]) {

  //---Inside mexFunction---

  //Declarations

  mxArray *xData, *sData;
  double *xValues, *outArray, *sValues;

  int i,j,k,z;
  int rowLen, colLen;

  int M;
  int Mo2;
  double sumPix;
  double sumPixsq;
  double gamma = 2.0;
  double StoDelta = 1.0;
  double weight;
  double delta = 1.0;
  double p;
  double var;
  double ni;
  int kv;
  int zv;

  // get first argument x
  xData = (mxArray*)prhs[0];
  xValues = mxGetPr(xData);
  rowLen = mxGetN(xData);
  colLen = mxGetM(xData);

  // get second argument sigma
  sData = (mxArray*)prhs[1];
  sValues = mxGetPr(sData);
  M = (int)sValues[0];
  Mo2 = (int)(M/2);

  //Allocate memory and assign output pointer
  plhs[0] = mxCreateDoubleMatrix(colLen, rowLen, mxREAL); //mxReal is our data-type

  //Get a pointer to the data space in our newly allocated memory
  outArray = mxGetPr(plhs[0]);

  //Copy matrix while multiplying each point by 2
  for(i=0;i<rowLen;i++) {
    for(j=0;j<colLen;j++) {      
      sumPix = 0;
      sumPixsq = 0;
      ni = 0;
      for (k=i-Mo2;k<=i+Mo2;k++) {
	for (z=j-Mo2;z<=j+Mo2;z++) {	  
	  kv = k;  zv = z;	  	  
	  /* if ( kv < 0 ) kv = rowLen + kv;
	  else if ( kv >= rowLen) kv = kv - rowLen;
	  if ( zv < 0 ) zv = colLen + zv;
	  else if ( zv >= colLen ) zv = zv - colLen; */	  
	  if ( kv < 0 || kv >= rowLen || zv < 0 || zv >= colLen ) continue;	  	  
	  p = xValues[(kv*colLen)+zv];
	  sumPix += p;
	  sumPixsq += p*p;
	  ni++;
	}
      }
      var = 0;
      if ( ni != 0 ) {
	sumPix /= ni;
	var = sumPixsq - ni * sumPix * sumPix;
	var /= ni;	
      }
      outArray[(i*colLen)+j] = var;
    }
  }
  return;
}

ColorTransferLib/Evaluation/VSI/gbvs/util/featureChannels/R_contrast.m

function out = R_contrast( fparam , img , imgR, imgG, imgB, typeidx )

if ( nargin == 1 )

  out.weight = fparam.contrastWeight;
  out.numtypes = 1;
  out.descriptions{1} = 'Intensity Contrast';  

else

  out.map = myContrast( img , round(size(img,1) * fparam.contrastwidth) );

end

ColorTransferLib/Evaluation/VSI/gbvs/util/mycombnk.m

% so that stats toolbox is not used
function cmbs = mycombnk( nums , k )

N = length(nums);
T = N^k;
cmbs = zeros( T , k );

for j=T:-1:1       
    n = j;        
    for jj=1:k
        b = mod(n,N);
        n = n - b;
        n = n / N;
        cmbs( j , k-jj+1 ) = nums(b+1);        
        cmbs( j , : ) = sort( cmbs(j,:) );        
    end    
    is_used = zeros( N , 1 );
    for jj=1:k
        if (is_used( cmbs(j,jj)))
            cmbs(j,:) = -1 * ones(1,k);
            break;
        else
            is_used( cmbs(j,jj) ) = 1;
        end
    end               
end

cmbs = unique(cmbs,'rows');
cmbs = cmbs(2:end,:);

ColorTransferLib/Evaluation/VSI/gbvs/util/mygausskernel.m

function k = mygausskernel( std , nstds )

maxi = round(std * nstds);
a = [ normpdf(0,0,std) zeros(1,maxi) ];

for i = 1 : maxi
    a(1+i) = normpdf( i , 0 , std );
end

k = [ a(end:-1:2) a ];
k = k / sum(k);

ColorTransferLib/Evaluation/VSI/gbvs/util/mymessage.m

function mymessage(param,s,varargin)
if (param.verbose == 1 )    
    s = [ '\t' s ];
    if ( strcmp(param.verboseout,'screen') == 1 )        
        if ( nargin == 2 )
            fprintf(s);
        else
            fprintf(s,varargin{:});
        end
    else
        fido = fopen(param.verboseout,'a');
        if ( nargin == 2 )
            fprintf(fido,s);
        else
            fprintf(fido,s,varargin{:});
        end
        fclose(fido);
    end
end        

ColorTransferLib/Evaluation/VSI/gbvs/util/mygetrgb.m

function [r,g,b,ii] = mygetrgb( img )

     r = img(:,:,1);
     g = img(:,:,2);
     b = img(:,:,3);
     ii = max(max(r,g),b);

ColorTransferLib/Evaluation/VSI/gbvs/util/padImageOld.m

function b = padImageOld( a , vpad , hpad )

if ( nargin == 2 )
  hpad = vpad;
end

if ( (size(a,1) > (2*vpad)) && (size(a,2) > (2*hpad)) )
  u = a(vpad:-1:1,:);
  b = a(end:-1:end-vpad+1,:);

  l = a(:,hpad:-1:1);
  r = a(:,end:-1:end-hpad+1);

  ul = a(vpad:-1:1,hpad:-1:1);
  ur = a(vpad:-1:1,end:-1:end-hpad+1);
  bl = a(end:-1:end-vpad+1,hpad:-1:1);
  br = a(end:-1:end-vpad+1,end:-1:end-hpad+1);
else
   u = repmat( a(1,:) , [ vpad 1 ] );
   b = repmat( a(end,:) , [ vpad 1 ] );

   l = repmat( a(:,1) , [ 1 hpad ] );
   r = repmat( a(:,end) , [ 1 hpad ] );

   ul = repmat( a(1,1) , [ vpad hpad ] );
   ur = repmat( a(1,end) , [ vpad hpad ] );
   bl = repmat( a(end,1) , [ vpad hpad ] );
   br = repmat( a(end,end) , [ vpad hpad ] );
end

b = [ ul u ur
      l  a r
      bl b br ];

ColorTransferLib/Evaluation/VSI/gbvs/util/padImage.m

function b = padImage( a , vpad , hpad )

if ( nargin == 2 )
  hpad = vpad;
end

u = repmat( a(1,:) , [ vpad 1 ] );
b = repmat( a(end,:) , [ vpad 1 ] );

l = repmat( a(:,1) , [ 1 hpad ] );
r = repmat( a(:,end) , [ 1 hpad ] );

ul = repmat( a(1,1) , [ vpad hpad ] );
ur = repmat( a(1,end) , [ vpad hpad ] );
bl = repmat( a(end,1) , [ vpad hpad ] );
br = repmat( a(end,end) , [ vpad hpad ] );

b = [ ul u ur
      l  a r
      bl b br ];

ColorTransferLib/Evaluation/VSI/gbvs/util/rankimg.m

function rimg = rankimg(img)

img = uint8(mat2gray(img)*255);

rimg = zeros(size(img));

img = img(:);

for i = 0 : 255
  rimg( img == i ) = mean( img < i );
end

ColorTransferLib/Evaluation/VSI/gbvs/util/myconv2.m

function c = myconv2(a,b)

%
% conv2 with 'same' and repeating boundary condition
%

vpad = ceil(( size(b,1) - 1 ) / 2);
hpad = ceil(( size(b,2) - 1 ) / 2);
ap = padImage( a , vpad , hpad );
cp = conv2( ap , b , 'same' );
c = cp( vpad + 1 : vpad + size(a,1) , hpad + 1 : hpad + size(a,2) );

ColorTransferLib/Evaluation/VSI/gbvs/util/rgb2dkl.m

function dkl = rgb2dkl(rgb)

sz = size(rgb);
im = shiftdim(rgb,2);
im = reshape(im,[3 prod(sz(1:2))]);
im = im';
im = rgb2dkl_v(im);
im = im';
im = reshape(im,[3 sz(1:2)]);
dkl = shiftdim(im,1);

function dkl = rgb2dkl_v(rgb)

% bunch of constants used for RGB -> DKL conversion:
lut_rgb = [ 
    0.024935, 0.0076954, 0.042291,
    0.024974, 0.0077395, 0.042346,
    0.025013, 0.0077836, 0.042401,
    0.025052, 0.0078277, 0.042456,
    0.025091, 0.0078717, 0.042511,
    0.02513, 0.0079158, 0.042566,
    0.025234, 0.007992, 0.042621,
    0.025338, 0.0080681, 0.042676,
    0.025442, 0.0081443, 0.042731,
    0.025545, 0.0082204, 0.042786,
    0.025649, 0.0082966, 0.042841,
    0.025747, 0.0084168, 0.042952,
    0.025844, 0.0085371, 0.043062,
    0.025942, 0.0086573, 0.043172,
    0.026039, 0.0087776, 0.043282,
    0.026136, 0.0088978, 0.043392,
    0.026234, 0.0090581, 0.043502,
    0.026331, 0.0092184, 0.043612,
    0.026429, 0.0093788, 0.043722,
    0.026526, 0.0095391, 0.043833,
    0.026623, 0.0096994, 0.043943,
    0.026818, 0.0099198, 0.044141,
    0.027013, 0.01014, 0.044339,
    0.027208, 0.010361, 0.044537,
    0.027403, 0.010581, 0.044736,
    0.027597, 0.010802, 0.044934,
    0.027857, 0.010994, 0.04522,
    0.028117, 0.011186, 0.045507,
    0.028377, 0.011379, 0.045793,
    0.028636, 0.011571, 0.046079,
    0.028896, 0.011764, 0.046366,
    0.029104, 0.012068, 0.046652,
    0.029312, 0.012373, 0.046938,
    0.029519, 0.012677, 0.047225,
    0.029727, 0.012982, 0.047511,
    0.029935, 0.013287, 0.047797,
    0.030273, 0.013663, 0.048326,
    0.03061, 0.01404, 0.048855,
    0.030948, 0.014417, 0.049383,
    0.031286, 0.014794, 0.049912,
    0.031623, 0.01517, 0.050441,
    0.032156, 0.015707, 0.051035,
    0.032688, 0.016244, 0.05163,
    0.033221, 0.016782, 0.052225,
    0.033753, 0.017319, 0.052819,
    0.034286, 0.017856, 0.053414,
    0.034961, 0.018693, 0.054493,
    0.035636, 0.019531, 0.055573,
    0.036312, 0.020369, 0.056652,
    0.036987, 0.021206, 0.057731,
    0.037662, 0.022044, 0.058811,
    0.038623, 0.023246, 0.060044,
    0.039584, 0.024449, 0.061278,
    0.040545, 0.025651, 0.062511,
    0.041506, 0.026854, 0.063744,
    0.042468, 0.028056, 0.064978,
    0.043857, 0.029659, 0.066806,
    0.045247, 0.031263, 0.068634,
    0.046636, 0.032866, 0.070463,
    0.048026, 0.034469, 0.072291,
    0.049416, 0.036072, 0.074119,
    0.051221, 0.038156, 0.076476,
    0.053026, 0.04024, 0.078833,
    0.054831, 0.042325, 0.081189,
    0.056636, 0.044409, 0.083546,
    0.058442, 0.046493, 0.085903,
    0.06039, 0.048737, 0.087996,
    0.062338, 0.050982, 0.090088,
    0.064286, 0.053226, 0.092181,
    0.066234, 0.055471, 0.094273,
    0.068182, 0.057715, 0.096366,
    0.070519, 0.06012, 0.098921,
    0.072857, 0.062525, 0.10148,
    0.075195, 0.06493, 0.10403,
    0.077532, 0.067335, 0.10659,
    0.07987, 0.069739, 0.10914,
    0.082208, 0.072345, 0.11176,
    0.084545, 0.07495, 0.11438,
    0.086883, 0.077555, 0.117,
    0.089221, 0.08016, 0.11963,
    0.091558, 0.082766, 0.12225,
    0.094026, 0.085611, 0.12533,
    0.096494, 0.088457, 0.12841,
    0.098961, 0.091303, 0.1315,
    0.10143, 0.094148, 0.13458,
    0.1039, 0.096994, 0.13767,
    0.10688, 0.10028, 0.14119,
    0.10987, 0.10357, 0.14471,
    0.11286, 0.10685, 0.14824,
    0.11584, 0.11014, 0.15176,
    0.11883, 0.11343, 0.15529,
    0.12208, 0.11695, 0.15903,
    0.12532, 0.12048, 0.16278,
    0.12857, 0.12401, 0.16652,
    0.13182, 0.12754, 0.17026,
    0.13506, 0.13106, 0.17401,
    0.1387, 0.13499, 0.17819,
    0.14234, 0.13892, 0.18238,
    0.14597, 0.14285, 0.18656,
    0.14961, 0.14677, 0.19075,
    0.15325, 0.1507, 0.19493,
    0.15727, 0.15519, 0.19956,
    0.1613, 0.15968, 0.20419,
    0.16532, 0.16417, 0.20881,
    0.16935, 0.16866, 0.21344,
    0.17338, 0.17315, 0.21806,
    0.17805, 0.17796, 0.22291,
    0.18273, 0.18277, 0.22775,
    0.1874, 0.18758, 0.2326,
    0.19208, 0.19238, 0.23744,
    0.19675, 0.19719, 0.24229,
    0.20156, 0.20224, 0.24758,
    0.20636, 0.20729, 0.25286,
    0.21117, 0.21234, 0.25815,
    0.21597, 0.21739, 0.26344,
    0.22078, 0.22244, 0.26872,
    0.2261, 0.22806, 0.27423,
    0.23143, 0.23367, 0.27974,
    0.23675, 0.23928, 0.28524,
    0.24208, 0.24489, 0.29075,
    0.2474, 0.2505, 0.29626,
    0.25299, 0.25651, 0.3022,
    0.25857, 0.26253, 0.30815,
    0.26416, 0.26854, 0.3141,
    0.26974, 0.27455, 0.32004,
    0.27532, 0.28056, 0.32599,
    0.28156, 0.28697, 0.33238,
    0.28779, 0.29339, 0.33877,
    0.29403, 0.2998, 0.34515,
    0.30026, 0.30621, 0.35154,
    0.30649, 0.31263, 0.35793,
    0.3126, 0.31904, 0.36388,
    0.3187, 0.32545, 0.36982,
    0.32481, 0.33186, 0.37577,
    0.33091, 0.33828, 0.38172,
    0.33701, 0.34469, 0.38767,
    0.34325, 0.3511, 0.39361,
    0.34948, 0.35752, 0.39956,
    0.35571, 0.36393, 0.40551,
    0.36195, 0.37034, 0.41145,
    0.36818, 0.37675, 0.4174,
    0.37429, 0.38317, 0.42313,
    0.38039, 0.38958, 0.42885,
    0.38649, 0.39599, 0.43458,
    0.3926, 0.4024, 0.44031,
    0.3987, 0.40882, 0.44604,
    0.40494, 0.41523, 0.45198,
    0.41117, 0.42164, 0.45793,
    0.4174, 0.42806, 0.46388,
    0.42364, 0.43447, 0.46982,
    0.42987, 0.44088, 0.47577,
    0.43623, 0.44689, 0.48128,
    0.4426, 0.45291, 0.48678,
    0.44896, 0.45892, 0.49229,
    0.45532, 0.46493, 0.4978,
    0.46169, 0.47094, 0.5033,
    0.46792, 0.47695, 0.50837,
    0.47416, 0.48297, 0.51344,
    0.48039, 0.48898, 0.5185,
    0.48662, 0.49499, 0.52357,
    0.49286, 0.501, 0.52863,
    0.49805, 0.50701, 0.53392,
    0.50325, 0.51303, 0.53921,
    0.50844, 0.51904, 0.54449,
    0.51364, 0.52505, 0.54978,
    0.51883, 0.53106, 0.55507,
    0.52442, 0.53667, 0.55969,
    0.53, 0.54228, 0.56432,
    0.53558, 0.5479, 0.56894,
    0.54117, 0.55351, 0.57357,
    0.54675, 0.55912, 0.57819,
    0.55182, 0.56433, 0.58304,
    0.55688, 0.56954, 0.58789,
    0.56195, 0.57475, 0.59273,
    0.56701, 0.57996, 0.59758,
    0.57208, 0.58517, 0.60242,
    0.57675, 0.58998, 0.60639,
    0.58143, 0.59479, 0.61035,
    0.5861, 0.5996, 0.61432,
    0.59078, 0.60441, 0.61828,
    0.59545, 0.60922, 0.62225,
    0.60065, 0.61403, 0.62709,
    0.60584, 0.61884, 0.63194,
    0.61104, 0.62365, 0.63678,
    0.61623, 0.62846, 0.64163,
    0.62143, 0.63327, 0.64648,
    0.62584, 0.63808, 0.65088,
    0.63026, 0.64289, 0.65529,
    0.63468, 0.6477, 0.65969,
    0.63909, 0.65251, 0.6641,
    0.64351, 0.65731, 0.6685,
    0.64857, 0.66132, 0.67269,
    0.65364, 0.66533, 0.67687,
    0.6587, 0.66934, 0.68106,
    0.66377, 0.67335, 0.68524,
    0.66883, 0.67735, 0.68943,
    0.67273, 0.68136, 0.69361,
    0.67662, 0.68537, 0.6978,
    0.68052, 0.68938, 0.70198,
    0.68442, 0.69339, 0.70617,
    0.68831, 0.69739, 0.71035,
    0.69221, 0.7022, 0.7141,
    0.6961, 0.70701, 0.71784,
    0.7, 0.71182, 0.72159,
    0.7039, 0.71663, 0.72533,
    0.70779, 0.72144, 0.72907,
    0.71169, 0.72505, 0.73348,
    0.71558, 0.72866, 0.73789,
    0.71948, 0.73226, 0.74229,
    0.72338, 0.73587, 0.7467,
    0.72727, 0.73948, 0.7511,
    0.73247, 0.74349, 0.75507,
    0.73766, 0.74749, 0.75903,
    0.74286, 0.7515, 0.763,
    0.74805, 0.75551, 0.76696,
    0.75325, 0.75952, 0.77093,
    0.75714, 0.76393, 0.77599,
    0.76104, 0.76834, 0.78106,
    0.76494, 0.77275, 0.78612,
    0.76883, 0.77715, 0.79119,
    0.77273, 0.78156, 0.79626,
    0.77792, 0.78677, 0.80132,
    0.78312, 0.79198, 0.80639,
    0.78831, 0.79719, 0.81145,
    0.79351, 0.8024, 0.81652,
    0.7987, 0.80762, 0.82159,
    0.80519, 0.81283, 0.82687,
    0.81169, 0.81804, 0.83216,
    0.81818, 0.82325, 0.83744,
    0.82468, 0.82846, 0.84273,
    0.83117, 0.83367, 0.84802,
    0.83636, 0.83888, 0.85286,
    0.84156, 0.84409, 0.85771,
    0.84675, 0.8493, 0.86256,
    0.85195, 0.85451, 0.8674,
    0.85714, 0.85972, 0.87225,
    0.86364, 0.86613, 0.87819,
    0.87013, 0.87255, 0.88414,
    0.87662, 0.87896, 0.89009,
    0.88312, 0.88537, 0.89604,
    0.88961, 0.89178, 0.90198,
    0.8961, 0.8986, 0.90947,
    0.9026, 0.90541, 0.91696,
    0.90909, 0.91222, 0.92445,
    0.91558, 0.91904, 0.93194,
    0.92208, 0.92585, 0.93943,
    0.92857, 0.93307, 0.94493,
    0.93506, 0.94028, 0.95044,
    0.94156, 0.94749, 0.95595,
    0.94805, 0.95471, 0.96145,
    0.95455, 0.96192, 0.96696,
    0.96364, 0.96954, 0.97357,
    0.97273, 0.97715, 0.98018,
    0.98182, 0.98477, 0.98678,
    0.99091, 0.99238, 0.99339,
    1, 1, 1 ];

lms0 = [ 34.918538957799996, 19.314796676499999, 0.585610818500000 ];

m = [ 18.32535,  44.60077,   7.46216, 4.09544,  28.20135,   6.66066, 0.02114,   0.10325,   1.05258 ];  

fac = 1.0 / (lms0(1) + lms0(2));
mm = [ sqrt(3.0)*fac, sqrt(3.0)*fac, 0.0, sqrt(lms0(1)*lms0(1)+lms0(2)*lms0(2))/lms0(1)*fac, -sqrt(lms0(1)*lms0(1)+lms0(2)*lms0(2))/lms0(2)*fac, 0.0, -fac, -fac, (lms0(1) + lms0(2)) / lms0(3) * fac ];

% do a lookup RGB -> rgb:
if ( strcmp(class(rgb),'double') == 1 )
  rgb = ceil( rgb*255 );
  rgb(rgb<1) = 1;
  rgb(rgb>256) = 256;
elseif ( strcmp(class(rgb),'uint8') == 1 )
  rgb = rgb + 1;
else
  fprintf(2,'Oops! Don''t know what kind of rgb image this is! Hoping for the best...\n');
  rgb = ceil( rgb*255 );
  rgb(rgb<1) = 1;
  rgb(rgb>256) = 256;
end

aa1 = lut_rgb( rgb(:,1) , 1 );
aa2 = lut_rgb( rgb(:,2) , 2 );
aa3 = lut_rgb( rgb(:,3) , 3 );

% now convert to LMS:
lms1 = m(1) * aa1 + m(2) * aa2 + m(3) * aa3 - lms0(1);
lms2 = m(4) * aa1 + m(5) * aa2 + m(6) * aa3 - lms0(2);
lms3 = m(7) * aa1 + m(8) * aa2 + m(9) * aa3 - lms0(3);

% finally to DKL:
dkl1 = mm(1) * lms1 + mm(2) * lms2 + mm(3) * lms3;
dkl2 = mm(4) * lms1 + mm(5) * lms2 + mm(6) * lms3;
dkl3 = mm(7) * lms1 + mm(8) * lms2 + mm(9) * lms3;

% finally to DKLn:
dkl = [ dkl1 * 0.5774 dkl2 * 2.7525 dkl3 * 0.4526 ];

ColorTransferLib/Evaluation/VSI/gbvs/util/rocScoreSaliencyVsFixations.m

function a = rocScoreSaliencyVsFixations( salmap , X , Y , origimgsize )  

%
% outputs ROC Area-Under-Curve Score between a saliency map and fixations.
%
%  salmap       : a saliency map
%  X            : vector of X locations of fixations in original image coordinates
%  Y            : vector of Y locations of fixations in original image coordinates
%  origimgsize  : size of original image (should have same aspect ratio as saliency map)
%

a = rocSal( salmap , makeFixationMask( X , Y , origimgsize , size(salmap) ) );

ColorTransferLib/Evaluation/VSI/gbvs/util/shiftImage.m

function imgShift = shiftImage( img , theta )

Worig = size(img,2);
Horig = size(img,1);

pad = 2;
imgpad = padImage(img,pad);

W = size(imgpad,2);
H = size(imgpad,1);
xi = repmat( [ 1 : W ] , [ H 1 ] );
yi = repmat( [ 1 : H ]', [ 1 W ] );

dx = cos(theta * pi / 180 );
dy =  -sin(theta * pi / 180);
imgpadshift = interp2( xi , yi , imgpad , xi + dx , yi + dy );

imgShift = imgpadshift( pad + 1 : pad + Horig , pad + 1 : pad + Worig );

ColorTransferLib/Evaluation/VSI/gbvs/util/show_imgnmap.m

function hm = show_imgnmap( img , out )
hm = heatmap_overlay( img , out.master_map_resized );
imshow( hm );

ColorTransferLib/Evaluation/init.py

from . import SSIM
from . import MSSSIM
from . import PSNR
from . import MSE
from . import HI
from . import LPIPS
from . import Corr
from . import BRISQUE
from . import CF
from . import NIMA
from . import VSI
from . import GSSIM

ColorTransferLib/MeshProcessing/init.py

from .Mesh import *
from .VolumetricVideo import *

ColorTransferLib/ImageProcessing/Image.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import numpy as np
from numba import njit
import random

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# Reads a color (RGB) or a greyscale (Grey) image and converts it to a 32 bit RGB image with a value range of [0, 1]
# Possible image color formats: ["RGB", "Grey", "CIELab", "lab"]. CIELab and lab are only available after conversion.
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Image:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # If <file_path> is None, an empty image with the given <size>=(width, height) will be created
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, file_path=None, array=None, size=(0, 0), color="RGB", normalized=False):
        if file_path is None and array is None:
            self.__img = np.zeros((size[1], size[0]), dtype=np.float32)
        elif file_path is not None and array is None:
            self.__img = cv2.imread(file_path)
            #self.__img = cv2.resize(self.__img, (self.__img.shape[1] //3, self.__img.shape[0] //3))
        elif file_path is None and array is not None:
            self.__img = array.astype(np.float32)
        else:
            raise ValueError("file_path or array has to be None")

        self.__type = "Image"
        self.__color_format = color
        self.__width = self.__img.shape[1]
        self.__height = self.__img.shape[0]
        self.__pixelnum = self.__img.shape[0] * self.__img.shape[1]

        if color == "RGB":
            self.__img = cv2.cvtColor(self.__img, cv2.COLOR_BGR2RGB).astype(np.float32)
        elif color == "BGR":
            self.__img = self.__img.astype(np.float32)
        elif color == "Grey":
            self.__img = cv2.cvtColor(self.__img, cv2.COLOR_BGR2RGB).astype(np.float32)
        else:
            raise ValueError(color + " is not a valid color format.")

        if not normalized:
            self.__img = self.__img / 255.0

        # WARNING: This is slow. Temporarily removed
        #self.__3D_color_histogram = self.__calculate_3D_color_histogram()

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # PUBLIC METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # Writes the file to the specified path.
    # ------------------------------------------------------------------------------------------------------------------
    def write(self, out_path):
        cv2.imwrite(out_path + ".png", cv2.cvtColor(self.__img, cv2.COLOR_RGB2BGR) * 255.0)

    # ------------------------------------------------------------------------------------------------------------------
    # Converts the image to the given color space.
    # ------------------------------------------------------------------------------------------------------------------
    def convert_to(self, color_format):
        if self.__color_format == color_format:
            print("INFO: Image has already the color format " + color_format + ".")

    # ------------------------------------------------------------------------------------------------------------------
    # Resizes the image without keeping the aspect ratio.
    # ------------------------------------------------------------------------------------------------------------------
    def resize(self, width=100, height=100):
        self.__width = width
        self.__height = height
        self.__img = cv2.resize(self.__img, (width, height), interpolation=cv2.INTER_AREA)
        #print(self.__width)
        #print(self.__width)

    # ------------------------------------------------------------------------------------------------------------------
    # Shows the image using OpenCV. The showed image can be scaled by providing a <resize>-values. If the
    # <stretch>-value is False, the image keeps its aspect ratio and will be extended by black borders fulfill the
    # given <resize>-values. If the resized image has to be post-processed without visualization, the <show>-value
    # has to be set to False.
    # ------------------------------------------------------------------------------------------------------------------
    def show(self, resize=(500, 500), stretch=False, show=True):
        if stretch:
            img_resized = cv2.resize(self.__img, (resize[0], resize[1]), interpolation=cv2.INTER_AREA)
        if not stretch:
            in_ratio = self.__width / self.__height
            out_ratio = resize[0] / resize[1]
            scale_factor = in_ratio / out_ratio
            if scale_factor < 1:
                top = bottom = 0
                left = right = int((self.__width / scale_factor - self.__width) / 2)
            elif scale_factor > 1:
                left = right = 0
                top = bottom = int((self.__height * scale_factor - self.__height) / 2)
            else:
                top = bottom = left = right = 0
            img_resized = cv2.copyMakeBorder(self.__img, top, bottom, left, right, cv2.BORDER_CONSTANT, None, (0,0,0))
            img_resized = cv2.resize(img_resized, (resize[0], resize[1]), interpolation=cv2.INTER_AREA)

        img_resized = cv2.cvtColor(img_resized, cv2.COLOR_RGB2BGR)

        if show:
            cv2.imshow('image', img_resized)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
        else:
            return img_resized

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # GETTER METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def get_type(self):
        return self.__type

    # ------------------------------------------------------------------------------------------------------------------
    # return the color values as vector
    # ------------------------------------------------------------------------------------------------------------------
    def get_colors(self):
        return self.__img.reshape(self.__width * self.__height, 1, 3)

    # ------------------------------------------------------------------------------------------------------------------
    # returns the image as numpy array
    # ------------------------------------------------------------------------------------------------------------------
    def get_raw(self):
        return self.__img

    # ------------------------------------------------------------------------------------------------------------------
    # returns the color histogram, mean and variance
    # ------------------------------------------------------------------------------------------------------------------
    def get_color_statistic(self, bins=256, normalized=False):
        color = self.get_colors()
        rgb_c = (color * 255.0).astype(int).reshape(color.shape[0], color.shape[2])
        histo_red = np.asarray(np.histogram(rgb_c[:,0], bins=np.arange(bins+1))[0]).reshape(bins,1)
        histo_green = np.asarray(np.histogram(rgb_c[:,1], bins=np.arange(bins+1))[0]).reshape(bins,1)
        histo_blue = np.asarray(np.histogram(rgb_c[:,2], bins=np.arange(bins+1))[0]).reshape(bins,1)

        if normalized:
            histo_red = histo_red / np.sum(histo_red)
            histo_green = histo_green / np.sum(histo_green)
            histo_blue = histo_blue / np.sum(histo_blue)

        histo = np.concatenate((histo_red, histo_green, histo_blue), axis=1)
        mean = np.mean(rgb_c, axis=0).astype(int)
        std = np.std(rgb_c, axis=0).astype(int)
        return histo, mean, std

    # ------------------------------------------------------------------------------------------------------------------
    # returns 
    # ------------------------------------------------------------------------------------------------------------------
    def get_color_distribution(self):
        color = self.get_colors()
        color = color[np.random.randint(color.shape[0], size=5000), :]
        rgb_c = (color * 255.0).astype(int).reshape(color.shape[0], color.shape[2])
        rgb_c = np.unique(rgb_c, axis=0)
        return rgb_c

    # ------------------------------------------------------------------------------------------------------------------
    # returns the 3D color histogram
    # ------------------------------------------------------------------------------------------------------------------
    def get_color_statistic_3D(self, bins=[256,256,256], normalized=False):
        color = self.get_colors()
        rgb_c = (color * 255.0).astype(int).reshape(color.shape[0], color.shape[2])
        histo = np.asarray(np.histogramdd(rgb_c, bins)[0])

        if normalized:
            sum_h = np.sum(histo)
            histo /= sum_h
        return histo

    # ------------------------------------------------------------------------------------------------------------------
    # returns the quantized 3D color histogram
    # ------------------------------------------------------------------------------------------------------------------
    def get_3D_color_histogram(self):
        return self.__3D_color_histogram

    # ------------------------------------------------------------------------------------------------------------------
    # return image width
    # ------------------------------------------------------------------------------------------------------------------
    def get_width(self):
        return self.__width

    # ------------------------------------------------------------------------------------------------------------------
    # return image height
    # ------------------------------------------------------------------------------------------------------------------
    def get_height(self):
        return self.__height

    # ------------------------------------------------------------------------------------------------------------------
    # return number of pixels
    # ------------------------------------------------------------------------------------------------------------------
    def get_pixelnum(self):
        return self.__pixelnum

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # SETTER METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # <colors> has to be provided as a vector, i.e., a flatten array
    # ------------------------------------------------------------------------------------------------------------------
    def set_colors(self, colors):
        self.__img = colors.reshape(self.__height, self.__width, 3).astype(np.float32)

    # ------------------------------------------------------------------------------------------------------------------
    # replaces the numpy array image
    # Parameters:
    # normalized = if True the array has to be normalized to range [0, 1], if false the range has to be [0, 255]
    # ------------------------------------------------------------------------------------------------------------------
    def set_raw(self, array, normalized=False):
        self.__img = array
        self.__img = self.__img.astype(np.float32)
        if not normalized:
            self.__img = self.__img / 255.0
        self.__width = self.__img.shape[1]
        self.__height = self.__img.shape[0]

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # PRIVATE METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # RGB color space is quantized in a 10x10x10 area 
    # ------------------------------------------------------------------------------------------------------------------
    def __calculate_3D_color_histogram(self):
        upd = np.clip(np.floor(self.__img * 10).astype(int), 0, 9).reshape(self.__height * self.__width, 3)
        uni = np.unique(upd, axis=0, return_counts=True)
        con = np.concatenate((uni[0], uni[1].reshape((-1, 1))), axis=1)
        return con

ColorTransferLib/ImageProcessing/Video.py

"""
Copyright 2024 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import cv2
import os
import numpy as np

from ColorTransferLib.ImageProcessing.Image import Image

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# 
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class Video:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # 
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, file_path=None, imgs=None):
        self.__type = "Video"

        if file_path is not None:
            frames = self.split_video_into_frames(file_path)
            self.__imgs = [Image(array=frame) for frame in frames]
        elif imgs is not None:
            self.__imgs = imgs

    # Function to split video into frames
    def split_video_into_frames(self, video_path):
        # Capture the video from the file
        cap = cv2.VideoCapture(video_path)

        frame_count = 0
        frames = []

        while True:
            # Read a frame from the video
            ret, frame = cap.read()

            # If the frame was read successfully, save it as an image
            if ret:
                frames.append(frame)
                frame_count += 1
            else:
                break
        # Release the video capture object
        cap.release()
        return frames

    # ------------------------------------------------------------------------------------------------------------------
    # Writes the file to the specified path.
    # ------------------------------------------------------------------------------------------------------------------
    def write(self, out_path):
        height, width = self.__imgs[0].get_width(), self.__imgs[0].get_height()
        size = (height, width)

        #Define the codec and create VideoWriter object
        fourcc = cv2.VideoWriter_fourcc(*"avc1")
        fps = 30

        # Initialize the VideoWriter
        out = cv2.VideoWriter(out_path + ".mp4", fourcc, fps, size)

        for i, frame in enumerate(self.__imgs):
            ff = (cv2.cvtColor(frame.get_raw(), cv2.COLOR_RGB2BGR) * 255.0).astype(np.uint8)

            if isinstance(ff, np.ndarray):
                out.write(ff)
            else:
                print(f"Frame {i} is not a valid numpy array and will be skipped.")

        out.release()

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # GETTER METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    def get_type(self):
        return self.__type

    # ------------------------------------------------------------------------------------------------------------------
    # return the color values as vector
    # ------------------------------------------------------------------------------------------------------------------
    def get_colors(self):
        return [img.get_colors() for img in self.__imgs]

    # ------------------------------------------------------------------------------------------------------------------
    # returns the image as numpy array
    # ------------------------------------------------------------------------------------------------------------------
    def get_raw(self):
        return [img.get_raw() for img in self.__imgs]

    # ------------------------------------------------------------------------------------------------------------------
    # returns the image 
    # ------------------------------------------------------------------------------------------------------------------
    def get_images(self):
        return self.__imgs

ColorTransferLib/MeshProcessing/Mesh.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""
import numpy as np
import open3d as o3d
import os

class Mesh:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # datatype -> [PointCloud, Mesh]
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, file_path=None, datatype=None):
        self.__type = datatype

        # vertex properties
        self.__vertices_enabled = False
        self.__vnormals_enabled = False
        self.__vcolors_enabled = False
        self.__vertex_positions = []
        self.__vertex_normals = []
        self.__vertex_colors = []
        self.__num_vertices = 0

        self.__pcd = None

        # face properties
        self.__faces_enabled = False
        self.__fnormals_enabled = False
        self.__faces = []
        self.__num_faces = 0

        # texture properties
        self.__texture_enabled = False
        self.__texture = None
        self.__texture_size = (0,0,0)
        self.__texture_uvs = []

        if datatype == "PointCloud":
            self.__init_pointcloud(file_path)
        elif datatype == "Mesh":
            self.__init_mesh(file_path)

    # ------------------------------------------------------------------------------------------------------------------
    # 
    # ------------------------------------------------------------------------------------------------------------------
    def __init_mesh(self, file_path):
        self.__pcd = o3d.io.read_triangle_mesh(file_path)

        self.__vertices_enabled = self.__pcd.has_vertices()
        self.__vcolors_enabled = self.__pcd.has_vertex_colors()
        self.__vnormals_enabled = self.__pcd.has_vertex_normals()

        self.__vertex_positions = np.asarray(self.__pcd.vertices) if self.__vertices_enabled else []
        self.__vertex_colors = np.asarray(self.__pcd.vertex_colors).astype("float32") if self.__vcolors_enabled else []
        self.__vertex_normals = np.asarray(self.__pcd.vertex_normals) if self.__vnormals_enabled else []

        self.__num_vertices = self.__vertex_positions.shape[0] if self.__vertices_enabled else 0

        self.__faces_enabled = self.__pcd.has_triangles()
        self.__fnormals_enabled = self.__pcd.has_triangle_normals()

        self.__face_positions = np.asarray(self.__pcd.triangles) if self.__faces_enabled else []
        self.__face_normals = np.asarray(self.__pcd.triangle_normals) if self.__fnormals_enabled else []

        self.__num_faces = self.__face_positions.shape[0] if self.__faces_enabled else 0

        texture_path = file_path.split(".")[0] + ".png"
        print(texture_path)
        if os.path.isfile(texture_path):
            self.__pcd.textures =  [o3d.io.read_image(texture_path).flip_vertical()]
        else:
            texture_path = file_path.split(".")[0] + ".jpg"
            if os.path.isfile(texture_path):
                self.__pcd.textures =  [o3d.io.read_image(texture_path).flip_vertical()]

        self.__texture_enabled = self.__pcd.has_textures()
        self.__texture = np.asarray(self.__pcd.textures[0]).astype("float32") / 255 if self.__texture_enabled else None

        # remove alpha channel
        if self.__texture.shape[2] == 4:
            self.__texture = self.__texture[:,:,:3]

        self.__texture_size = np.asarray(self.__texture.shape) if self.__texture_enabled else None
        self.__texture_uvs = self.__pcd.triangle_uvs

        # set all material ids to 0 because they are per default: 1
        self.__pcd.triangle_material_ids = o3d.utility.IntVector(np.asarray(self.__pcd.triangle_material_ids) * 0)

    # ------------------------------------------------------------------------------------------------------------------
    # 
    # ------------------------------------------------------------------------------------------------------------------
    def __init_pointcloud(self, file_path):
        self.__pcd = o3d.io.read_point_cloud(file_path)

        self.__vertices_enabled = self.__pcd.has_points()
        self.__vcolors_enabled = self.__pcd.has_colors()
        self.__vnormals_enabled = self.__pcd.has_normals()

        self.__vertex_positions = np.asarray(self.__pcd.points) if self.__vertices_enabled else []
        self.__vertex_colors = np.asarray(self.__pcd.colors).astype("float32") if self.__vcolors_enabled else []
        self.__vertex_normals = np.asarray(self.__pcd.normals) if self.__vnormals_enabled else []

        self.__num_vertices = self.__vertex_positions.shape[0] if self.__vertices_enabled else 0

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # PRIVATE METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # PUBLIC METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # Writes the mesh to the specified path
    # ------------------------------------------------------------------------------------------------------------------
    def write(self, path):
        if self.__type == "PointCloud":
            o3d.io.write_point_cloud(path + ".ply", self.__pcd)
        elif self.__type == "Mesh":
            # opne3d saves the textures of obj files with a "_0", "_1" etc ending, because multiple textures are
            # possible -> this ending has to be removed from the png file and within the mtl file.
            o3d.io.write_triangle_mesh(path + ".obj", self.__pcd)
            img_path = path + "_0.png"
            new_img_path = path + ".png"
            file_name = path.split("/")[-1]
            os.rename(img_path, new_img_path)

            mtl_path = path + ".mtl"
            readFile = open(mtl_path, "r")
            data = readFile.read()
            data = data.replace(file_name + "_0.png", file_name + ".png")
            writeFile = open(mtl_path, "w")
            writeFile.write(data)

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # GETTER METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    def get_mesh(self):
        return self.__pcd

    # ------------------------------------------------------------------------------------------------------------------
    # Returns if vertex colors are available
    # ------------------------------------------------------------------------------------------------------------------
    def has_vertex_colors(self):
        return self.__vcolors_enabled

    # ------------------------------------------------------------------------------------------------------------------
    # Returns if vertex normals are available
    # ------------------------------------------------------------------------------------------------------------------
    def has_vertex_normals(self):
        return self.__vnormals_enabled

    # ------------------------------------------------------------------------------------------------------------------
    # Returns if vertex normals are available
    # ------------------------------------------------------------------------------------------------------------------
    def has_vertex_colors(self):
        return self.__vcolors_enabled

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the positions of all vertices as numpy array with shape (len(vertices), 1, 3).
    # ------------------------------------------------------------------------------------------------------------------
    def get_vertex_positions(self):
        return self.__vertex_positions
        # list_pos = [vertex.get_position() for vertex in self.__vertices]
        # numpy_pos = np.asarray(list_pos, dtype=np.float32).reshape(len(list_pos), 1, 3)
        # return numpy_pos

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the colors of all vertices as numpy array with shape (len(vertices), 1, 3). Alpha channel will be
    # ignored.
    # ------------------------------------------------------------------------------------------------------------------
    def get_vertex_colors(self):
        return self.__vertex_colors  
        # list_color = [vertex.get_color()[:-1] for vertex in self.__vertices]
        # numpy_color = np.asarray(list_color, dtype=np.float32).reshape(len(list_color), 1, 3)
        # return numpy_color

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the colors of all vertices as numpy array with shape (len(vertices), 1, 3). Necessary for the
    # ColorTransferLib
    # ------------------------------------------------------------------------------------------------------------------
    def get_colors(self):
        if self.__type == "PointCloud":
            return np.expand_dims(self.__vertex_colors, 1)
        elif self.__type == "Mesh":
            tex_height = self.__texture_size[0]
            tex_width = self.__texture_size[1]
            tex_channel = self.__texture_size[2]
            return self.__texture.reshape(tex_width * tex_height, 1, tex_channel)
            # return self.__texture.astype("float32").reshape(tex_width * tex_height, 1, tex_channel)  / 255.0

    # ------------------------------------------------------------------------------------------------------------------
    # TEMPORARY because some color transfer algorithms need this
    # ------------------------------------------------------------------------------------------------------------------
    def get_raw(self):
        if self.__type == "PointCloud":
            return np.expand_dims(self.__vertex_colors, 1)
        elif self.__type == "Mesh":
            return np.resize(self.__texture,(256,256,3))
    # ------------------------------------------------------------------------------------------------------------------
    # TEMPORARY because some color transfer algorithms need this
    # ------------------------------------------------------------------------------------------------------------------
    def set_raw(self, colors, normalized=False):
        if not normalized:
            colors /= 255
        if self.__type == "PointCloud":
            self.__pcd.colors = o3d.utility.Vector3dVector(np.squeeze(colors))
        elif self.__type == "Mesh":
            # Prevents: RuntimeError: Image can only be initialized from c-style buffer.
            colors = np.asarray(colors, order="C")
            self.__texture = colors
            self.__pcd.textures = [o3d.geometry.Image((colors*255).astype("uint8"))]

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the normals of all vertices as numpy array with shape (len(vertices), 1, 3).
    # ------------------------------------------------------------------------------------------------------------------
    def get_vertex_normals(self):
        return self.__vertex_normals
        # list_normal = [vertex.get_normal() for vertex in self.__vertices]
        # numpy_normal = np.asarray(list_normal, dtype=np.float32).reshape(len(list_normal), 1, 3)
        # return numpy_normal

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the faces
    # ------------------------------------------------------------------------------------------------------------------
    def get_faces(self):
        return self.__faces

    # ------------------------------------------------------------------------------------------------------------------
    # Returns if texture is available
    # ------------------------------------------------------------------------------------------------------------------
    def has_texture(self):
        return self.__texture_enabled

    # ------------------------------------------------------------------------------------------------------------------
    # Returns texture image
    # ------------------------------------------------------------------------------------------------------------------
    def get_texture(self):
        return self.__texture

    # ------------------------------------------------------------------------------------------------------------------
    # Returns if face normals are available
    # ------------------------------------------------------------------------------------------------------------------
    def has_face_normals(self):
        return self.__fnormals_enabled

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the number of vertices
    # ------------------------------------------------------------------------------------------------------------------
    def get_num_vertices(self):
        return self.__num_vertices

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the number of faces
    # ------------------------------------------------------------------------------------------------------------------
    def get_num_faces(self):
        return self.__num_faces

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the type of object
    # ------------------------------------------------------------------------------------------------------------------
    def get_type(self):
        return self.__type

    # ------------------------------------------------------------------------------------------------------------------
    # returns the color histogram, mean and variance
    # ------------------------------------------------------------------------------------------------------------------
    def get_color_statistic(self):
        if self.__type == "Mesh":
            color = np.reshape(self.__texture, (self.__texture.shape[0]*self.__texture.shape[1], self.__texture.shape[2]))
        elif self.__type == "PointCloud":
            color = self.__vertex_colors

        rgb_c = (color * 255.0).astype(np.int)
        histo_red = np.asarray(np.histogram(rgb_c[:,0], bins=np.arange(257))[0]).reshape(256,1)
        histo_green = np.asarray(np.histogram(rgb_c[:,1], bins=np.arange(257))[0]).reshape(256,1)
        histo_blue = np.asarray(np.histogram(rgb_c[:,2], bins=np.arange(257))[0]).reshape(256,1)
        histo = np.concatenate((histo_red, histo_green, histo_blue), axis=1)
        mean = np.mean(rgb_c, axis=0).astype(np.int)
        std = np.std(rgb_c, axis=0).astype(np.int)
        return histo, mean, std

    # ------------------------------------------------------------------------------------------------------------------
    # returns 
    # ------------------------------------------------------------------------------------------------------------------
    def get_color_distribution(self):
        if self.__type == "Mesh":
            tex_height = self.__texture_size[0]
            tex_width = self.__texture_size[1]
            tex_channel = self.__texture_size[2]
            color = self.__texture.reshape(tex_height * tex_width, tex_channel)
        elif self.__type == "PointCloud":
            color = self.__vertex_colors

        color = color[np.random.randint(color.shape[0], size=5000), :]

        rgb_c = (color * 255).astype(np.int)
        rgb_c = np.unique(rgb_c, axis=0)
        return rgb_c

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # SETTER METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # <positions> have to be provided in shape (len(vertices), 3)
    # ------------------------------------------------------------------------------------------------------------------
    def set_vertex_positions(self, points):
        self.__pcd.points = o3d.utility.Vector3dVector(points)

    # ------------------------------------------------------------------------------------------------------------------
    # <colors> have to be provided in shape (len(vertices), 3)
    # ------------------------------------------------------------------------------------------------------------------
    def set_vertex_colors(self, colors):
        self.__pcd.colors = o3d.utility.Vector3dVector(colors)

    # ------------------------------------------------------------------------------------------------------------------
    # <colors> have to be provided in shape (len(vertices), 1, 3) -> Necessary for the ColorTransferLib 
    # ------------------------------------------------------------------------------------------------------------------
    def set_colors(self, colors):
        if self.__type == "PointCloud":   
            self.__pcd.colors = o3d.utility.Vector3dVector(colors.squeeze())
        elif self.__type == "Mesh":
            tex_height = self.__texture_size[0]
            tex_width = self.__texture_size[1]
            tex_channel = self.__texture_size[2]
            colors = np.asarray(colors, order="C")
            self.__pcd.textures = [o3d.geometry.Image((colors.reshape(tex_height, tex_width, tex_channel) * 255).astype("uint8"))]
            self.__texture = np.asarray(self.__pcd.textures[0]).astype("float32") / 255 if self.__texture_enabled else None

    # ------------------------------------------------------------------------------------------------------------------
    # <normals> have to be provided in shape (len(vertices), 3)
    # ------------------------------------------------------------------------------------------------------------------
    def set_vertex_normals(self, normals):
        self.__pcd.normals = o3d.utility.Vector3dVector(normals)

    # ------------------------------------------------------------------------------------------------------------------
    # ... 
    # ------------------------------------------------------------------------------------------------------------------
    def set_faces(self, faces):
        pass
        # self.__faces = faces
        # self.__faces_enabled = True
        # self.__num_faces = len(faces)

    # ------------------------------------------------------------------------------------------------------------------
    # RGB color space is quantized in a 10x10x10 area 
    # ------------------------------------------------------------------------------------------------------------------
    def get_3D_color_histogram(self):
        if self.__type == "PointCloud":   
            cols = self.__vertex_colors
        elif self.__type == "Mesh":
            tex_height = self.__texture_size[0]
            tex_width = self.__texture_size[1]
            tex_channel = self.__texture_size[2]
            cols = self.__texture.reshape(tex_width * tex_height, tex_channel)
        upd = np.clip(np.floor(cols * 10).astype(np.int8), 0, 9)
        # numpy unique is slow -> maybe optimizaiton possible
        uni = np.unique(upd, axis=0, return_counts=True)
        con = np.concatenate((uni[0], uni[1].reshape((-1, 1))), axis=1)
        return con

    # ------------------------------------------------------------------------------------------------------------------
    # create voxelgrid from given point cloud 
    # ------------------------------------------------------------------------------------------------------------------
    def get_voxel_grid(self, voxel_level):
        scale_f = voxel_level
        print(voxel_level)
        # Initialize a point cloud object
        pcd = self.__pcd
        # fit to unit cube
        sc_f_min = pcd.get_min_bound()
        sc_f_max = pcd.get_max_bound()
        sc_f = np.max(sc_f_max - sc_f_min)
        #pcd.scale(1 / sc_f, center=pcd.get_center())
        # Create a voxel grid from the point cloud with a voxel_size of 0.01
        voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd,voxel_size=scale_f * sc_f)

        #voxel_grid.scale(scale=57.0)

        voxelret = {
            "centers": [],#np.empty((0,3), np.float32),
            "colors": []#np.empty((0,3), np.float32)
        }

        voxels = voxel_grid.get_voxels()

        for vox in voxels:
            #np.append(voxelret["centers"], np.array([voxel_grid.get_voxel_center_coordinate(vox.grid_index)]), axis=0)
            #np.append(voxelret["colors"], np.array([vox.color]), axis=0)
            voxelret["centers"].append(voxel_grid.get_voxel_center_coordinate(vox.grid_index))
            voxelret["colors"].append(vox.color)

        voxelret["centers"] = np.asarray(voxelret["centers"])
        voxelret["colors"] = np.asarray(voxelret["colors"])
        voxelret["scale"] = scale_f * sc_f

        return voxelret

ColorTransferLib/ImageProcessing/ColorSpaces.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import math
from numba import cuda
from ColorTransferLib.Utils.Math import device_mul_mat3x3_vec3

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
# TODO: DESCRIPTION
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class ColorSpaces:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # KERNEL METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    @cuda.jit
    def __kernel_lab_to_rgb(input, output, M_lms2rgb, M_lab2lms1, M_lab2lms2):
        pos = cuda.grid(2)
        x = pos[1] % input.shape[1]
        y = pos[0] % input.shape[0]

        temp = device_mul_mat3x3_vec3(M_lab2lms2, device_mul_mat3x3_vec3(M_lab2lms1, input[y, x]))
        temp = (math.exp(temp[0]), math.exp(temp[1]), math.exp(temp[2]))
        temp = device_mul_mat3x3_vec3(M_lms2rgb, temp)
        output[y, x] = (min(temp[0], 1.0), min(temp[1], 1.0), min(temp[2], 1.0))

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    @cuda.jit
    def __kernel_rgb_to_lab(in_obj, output, m_rgb2lms, m_lms2lab1, m_lms2lab2):
        pos = cuda.grid(2)
        x = pos[1] % in_obj.shape[1]
        y = pos[0] % in_obj.shape[0]

        temp = device_mul_mat3x3_vec3(m_rgb2lms, in_obj[y, x])

        temp = (max(0.000000000001, temp[0]), max(0.000000000001, temp[1]), max(0.000000000001, temp[2]))
        temp = (math.log(temp[0]), math.log(temp[1]), math.log(temp[2]))
        output[y, x] = device_mul_mat3x3_vec3(m_lms2lab2, device_mul_mat3x3_vec3(m_lms2lab1, temp))

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # HOST METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb_to_lab_host(img):
        img = cuda.to_device(img)

        out = cuda.device_array(img.shape)

        device_m_rgb2lms = cuda.to_device(np.array([
                            [0.3811, 0.5783, 0.0402],
                            [0.1967, 0.7244, 0.0782],
                            [0.0241, 0.1288, 0.8444]]))

        device_m_lms2lab1 = cuda.to_device(np.array([
                            [1.0, 1.0, 1.0],
                            [1.0, 1.0, -2.0],
                            [1.0, -1.0, 0.0]]))

        device_m_lms2lab2 = cuda.to_device(np.array([
                            [1.0/math.sqrt(3.0), 0.0, 0.0],
                            [0.0, 1.0/math.sqrt(6.0), 0.0],
                            [0.0, 0.0, 1.0/math.sqrt(2.0)]]))

        threadsperblock = (32, 32)
        blockspergrid_x = int(math.ceil(out.shape[0] / threadsperblock[0]))
        blockspergrid_y = int(math.ceil(out.shape[1] / threadsperblock[1]))
        blockspergrid = (blockspergrid_x, blockspergrid_y)
        # Now start the kernel
        ColorSpaces.__kernel_rgb_to_lab[blockspergrid, threadsperblock](img,
                                                                      out,
                                                                      device_m_rgb2lms,
                                                                      device_m_lms2lab1,
                                                                      device_m_lms2lab2)

        out = out.copy_to_host()

        return out
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb_to_lab_cpu(img):
        device_m_rgb2lms = np.array([
                            [0.3811, 0.5783, 0.0402],
                            [0.1967, 0.7244, 0.0782],
                            [0.0241, 0.1288, 0.8444]])

        device_m_lms2lab1 = np.array([
                            [1.0, 1.0, 1.0],
                            [1.0, 1.0, -2.0],
                            [1.0, -1.0, 0.0]])

        device_m_lms2lab2 = np.array([
                            [1.0/math.sqrt(3.0), 0.0, 0.0],
                            [0.0, 1.0/math.sqrt(6.0), 0.0],
                            [0.0, 0.0, 1.0/math.sqrt(2.0)]])

        eigen_device_m_rgb2lms = np.tile(device_m_rgb2lms.T, (img.shape[0], 1, 1))
        eigen_device_m_lms2lab1 = np.tile(device_m_lms2lab1.T, (img.shape[0], 1, 1))
        eigen_device_m_lms2lab2 = np.tile(device_m_lms2lab2.T, (img.shape[0], 1, 1))

        result = np.einsum("ijk,ij->ik", eigen_device_m_rgb2lms,  np.squeeze(img))

        result = np.log(result + 0.000000000001)
        result = np.einsum("ijk,ij->ik", eigen_device_m_lms2lab1,  result)
        result = np.einsum("ijk,ij->ik", eigen_device_m_lms2lab2,  result)  
        result = np.expand_dims(result, 1)

        return result
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def rgb_to_lab(img):
        out = cuda.device_array(img.shape)

        device_m_rgb2lms = cuda.to_device(np.array([
                            [0.3811, 0.5783, 0.0402],
                            [0.1967, 0.7244, 0.0782],
                            [0.0241, 0.1288, 0.8444]]))

        device_m_lms2lab1 = cuda.to_device(np.array([
                            [1.0, 1.0, 1.0],
                            [1.0, 1.0, -2.0],
                            [1.0, -1.0, 0.0]]))

        device_m_lms2lab2 = cuda.to_device(np.array([
                            [1.0/math.sqrt(3.0), 0.0, 0.0],
                            [0.0, 1.0/math.sqrt(6.0), 0.0],
                            [0.0, 0.0, 1.0/math.sqrt(2.0)]]))

        threadsperblock = (32, 32)
        blockspergrid_x = int(math.ceil(out.shape[0] / threadsperblock[0]))
        blockspergrid_y = int(math.ceil(out.shape[1] / threadsperblock[1]))
        blockspergrid = (blockspergrid_x, blockspergrid_y)
        # Now start the kernel
        ColorSpaces.__kernel_rgb_to_lab[blockspergrid, threadsperblock](img,
                                                                      out,
                                                                      device_m_rgb2lms,
                                                                      device_m_lms2lab1,
                                                                      device_m_lms2lab2)

        return out

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def lab_to_rgb_cpu(img):
        device_m_lms2rgb = np.array([
                            [4.4679, -3.5873, 0.1193],
                            [-1.2186, 2.3809, -0.1624],
                            [0.0497, -0.2439, 1.2045]])

        device_m_lab2lms1 = np.array([
                            [math.sqrt(3.0)/3.0, 0.0, 0.0],
                            [0.0, math.sqrt(6.0)/6.0, 0.0],
                            [0.0, 0.0, math.sqrt(2.0)/2.0]])

        device_m_lab2lms2 = np.array([
                            [1.0, 1.0, 1.0],
                            [1.0, 1.0, -1.0],
                            [1.0, -2.0, 0.0]])

        eigen_device_m_lms2rgb = np.tile(device_m_lms2rgb.T, (img.shape[0], 1, 1))
        eigen_device_m_lab2lms1 = np.tile(device_m_lab2lms1.T, (img.shape[0], 1, 1))
        eigen_device_m_lab2lms2 = np.tile(device_m_lab2lms2.T, (img.shape[0], 1, 1))

        result = np.einsum("ijk,ij->ik", eigen_device_m_lab2lms1,  np.squeeze(img))
        result = np.einsum("ijk,ij->ik", eigen_device_m_lab2lms2,  result)
        result = np.exp(result)
        result = np.einsum("ijk,ij->ik", eigen_device_m_lms2rgb,  result)  
        result = np.expand_dims(result, 1)

        return result
    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def lab_to_rgb(img):
        out = cuda.device_array(img.shape)

        device_m_lms2rgb = cuda.to_device(np.array([
                            [4.4679, -3.5873, 0.1193],
                            [-1.2186, 2.3809, -0.1624],
                            [0.0497, -0.2439, 1.2045]]))

        device_m_lab2lms1 = cuda.to_device(np.array([
                            [math.sqrt(3.0)/3.0, 0.0, 0.0],
                            [0.0, math.sqrt(6.0)/6.0, 0.0],
                            [0.0, 0.0, math.sqrt(2.0)/2.0]]))

        device_m_lab2lms2 = cuda.to_device(np.array([
                            [1.0, 1.0, 1.0],
                            [1.0, 1.0, -1.0],
                            [1.0, -2.0, 0.0]]))

        threadsperblock = (32, 32)
        blockspergrid_x = int(math.ceil(out.shape[0] / threadsperblock[0]))
        blockspergrid_y = int(math.ceil(out.shape[1] / threadsperblock[1]))
        blockspergrid = (blockspergrid_x, blockspergrid_y)
        ColorSpaces.__kernel_lab_to_rgb[blockspergrid, threadsperblock](img,
                                                                      out,
                                                                      device_m_lms2rgb,
                                                                      device_m_lab2lms1,
                                                                      device_m_lab2lms2)

        return out

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    @staticmethod
    def lab_to_rgb_host(img):
        img = cuda.to_device(img)

        out = cuda.device_array(img.shape)

        device_m_lms2rgb = cuda.to_device(np.array([
                            [4.4679, -3.5873, 0.1193],
                            [-1.2186, 2.3809, -0.1624],
                            [0.0497, -0.2439, 1.2045]]))

        device_m_lab2lms1 = cuda.to_device(np.array([
                            [math.sqrt(3.0)/3.0, 0.0, 0.0],
                            [0.0, math.sqrt(6.0)/6.0, 0.0],
                            [0.0, 0.0, math.sqrt(2.0)/2.0]]))

        device_m_lab2lms2 = cuda.to_device(np.array([
                            [1.0, 1.0, 1.0],
                            [1.0, 1.0, -1.0],
                            [1.0, -2.0, 0.0]]))

        threadsperblock = (32, 32)
        blockspergrid_x = int(math.ceil(out.shape[0] / threadsperblock[0]))
        blockspergrid_y = int(math.ceil(out.shape[1] / threadsperblock[1]))
        blockspergrid = (blockspergrid_x, blockspergrid_y)
        ColorSpaces.__kernel_lab_to_rgb[blockspergrid, threadsperblock](img,
                                                                      out,
                                                                      device_m_lms2rgb,
                                                                      device_m_lab2lms1,
                                                                      device_m_lab2lms2)

        out = out.copy_to_host()

        return out

ColorTransferLib/Options/BCC.json

[
  {
    "name": "colorspace",
    "default": "lalphabeta",
    "type": "string",
    "values": ["rgb", "lalphabeta"],
    "tooltip": "Decides in which color space the algorithm is applied.",
    "changeable": true
  }
]

ColorTransferLib/Evaluation/VSI/gbvs/util/rocSal.m

function a = rocSal( salmap , mask )

%     ROC area agreement between saliency map (salmap) and fixations (mask) 
% == good measure of HOW WELL salmap 'predicts' fixations
%
% - mask is the same size as salmap and
%   contains number of fixations at each
%   map location ( 0,1,2,..etc. )
%
% - gives the ROC score of the following binary classification problem 
%
%   the set of trials is this
%      {each (fixation,location)} UNION
%      {each location on the map without a fixation}
%
%   a true positive occurs
%      when a (fixation,location) pair is above threshold
%   a true negative occurs
%      when a (no fixation,location) pair is below threshold
%   a false negative occurs
%      when a (fixation,location) pair is below threshold
%   a false positive occurs
%      when a (no fixation,location) pair is above threshold
%
%   ROC curve plots TPR against FPR
%    where TPR = TP / (TP+FN) = TP / (number of ground-truth trues)
%          FPR = FP / (FP+TN) = FP / (number of ground-truth falses)
%
%  so if out of 10 fixations, 9 occur at one location, which is the 
%  only above threshold location, the TPR is 90%, and the FPR is 0%
%

% limit to 256 unique values
salmap = mat2gray(salmap);
if ( strcmp(class(salmap),'double') )
    % salmap = uint8(salmap * 255);
    salmap = uint8(salmap * 50);
end

t = getIntelligentThresholds( salmap(:) );
Nt = length(t);
p = zeros(Nt,2);

% number of (fixation,location) trials
Ntrues = sum(mask(:)); 

% number of (no fixation,location) trials
falses = (mask==0);
Nfalses = sum( falses(:) );
if ( Nfalses == 0 ) Nfalses = 1e-6; end

for ti = 1 : Nt   

  T = t(ti);    
  shouldbefix = salmap >= T;    

  TPm = mask .* shouldbefix;
  TP = sum( TPm(:) );
  tpr = TP / Ntrues;

  FPm = (mask==0) .* shouldbefix;
  FP = sum( FPm(:) );  
  fpr = FP / Nfalses;

  p(ti,:) = [ tpr fpr ];

end

a = areaROC(p);

ColorTransferLib/ImageProcessing/init.py

from .ColorSpaces import *
from .Image import *

ColorTransferLib/MeshProcessing/VolumetricVideo.py

"""
Copyright 2024 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""
import numpy as np
import open3d as o3d
import os
from ColorTransferLib.MeshProcessing.Mesh import Mesh

class VolumetricVideo:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # datatype -> [PointCloud, Mesh]
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, folder_path=None, file_name=None, meshes=None):
        self.__type = "VolumetricVideo"
        self.__numMeshes = 0
        self.__meshes = []
        self.__file_name = file_name

        if folder_path==None and meshes!=None:
            self.__meshes = meshes
            self.__numMeshes = len(meshes)
            return

        # Verbinde files_path und file_name zu einem Pfad
        full_path = os.path.join(folder_path, file_name)

        for i in range(1800):
            i_str = str(i).zfill(5)
            file_path = f"{full_path}_{i_str}.obj"

            if os.path.exists(file_path):
                self.__numMeshes += 1
                print(f"Loading {file_path}")
                mesh = Mesh(file_path=file_path, datatype="Mesh")
                self.__meshes.append(mesh)
            else:
                break

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # PUBLIC METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    # ------------------------------------------------------------------------------------------------------------------
    # Writes the mesh to the specified path
    # ------------------------------------------------------------------------------------------------------------------
    def write(self, path):
        new_file_name = path.split("/")[-1]
        new_file_name = new_file_name.replace('$volumetric$', '')

        out_path = os.path.join(path, new_file_name)

        for i in range(self.__numMeshes):
            i_str = str(i).zfill(5)
            file_path = f"{out_path}_{i_str}"

            # opne3d saves the textures of obj files with a "_0", "_1" etc ending, because multiple textures are
            # possible -> this ending has to be removed from the png file and within the mtl file.
            o3d.io.write_triangle_mesh(file_path + ".obj", self.__meshes[i].get_mesh())
            img_path = file_path + "_0.png"
            new_img_path = file_path + ".png"
            file_name = file_path.split("/")[-1]
            os.rename(img_path, new_img_path)

            mtl_path = file_path + ".mtl"
            readFile = open(mtl_path, "r")
            data = readFile.read()
            data = data.replace(file_name + "_0.png", file_name + ".png")
            writeFile = open(mtl_path, "w")
            writeFile.write(data)

    def set_file_name(self, file_name):
        self.__file_name = file_name

    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # GETTER METHODS
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------

    def get_type(self):
        return self.__type

    def get_file_name(self):
        return self.__file_name

    # ------------------------------------------------------------------------------------------------------------------
    # Returns the colors of all vertices as numpy array with shape (len(vertices), 1, 3). Necessary for the
    # ColorTransferLib
    # ------------------------------------------------------------------------------------------------------------------
    def get_colors(self):
        return [mesh.get_colors() for mesh in self.__meshes]

    # ------------------------------------------------------------------------------------------------------------------
    # returns the image 
    # ------------------------------------------------------------------------------------------------------------------
    def get_meshes(self):
        return self.__meshes

ColorTransferLib/Options/CAM.json

[
  {
    "name": "smooth",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "show_masks",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "select_matches",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "Set to True to select color correspondences in palettes.",
    "changeable": true
  },
  {
    "name": "eps",
    "default": 1e-6,
    "type": "float",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "sigma",
    "default": 0.25,
    "type": "float",
    "values": [],
    "tooltip": "No description yet! Info: 0.25 and 0.3 work well in most cases.",
    "changeable": true
  },
  {
    "name": "palette_size",
    "default": 5,
    "type": "int",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "add_black_white",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "style_loss_weight",
    "default": 10000,
    "type": "int",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "content_loss_weight",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "color_distance",
    "default": "chroma_L2",
    "type": "string",
    "values": ["chroma_L2", "L2"],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "style_feature_distance",
    "default": "L2",
    "type": "string",
    "values": ["L2", "COSINE"],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "content_feature_distance",
    "default": "L2",
    "type": "string",
    "values": ["L2", "COSINE"],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "optimizer",
    "default": "LBFGS",
    "type": "string",
    "values": ["LBFGS", "Adam", "Adagrad"],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "lr",
    "default": 0.5,
    "type": "float",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "iterations",
    "default": 100,
    "type": "int",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "enable_img_scale",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "Desired size of the output image.",
    "changeable": true
  },
  {
    "name": "img_size",
    "default": 384,
    "type": "int",
    "values": [],
    "tooltip": "Desired size of the output image.",
    "changeable": true
  },
  {
    "name": "gaussian_kernel",
    "default": null,
    "type": "int",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": false
  },
  {
    "name": "device",
    "default": "cuda",
    "type": "string",
    "values": ["cuda", "cpu"],
    "tooltip": "No description yet!",
    "changeable": true
  },
  {
    "name": "loader",
    "default": null,
    "type": "string",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": false
  },
  {
    "name": "unloader",
    "default": null,
    "type": "string",
    "values": [],
    "tooltip": "No description yet!",
    "changeable": false
  },
  {
    "name": "content_layers_default",
    "default": ["conv_4", "conv_5"],
    "type": "array",
    "values": [],
    "tooltip": "Desired depth layers to compute content losses.",
    "changeable": false
  },
  {
    "name": "color_aware_layers_default",
    "default": ["conv_1", "conv_2", "conv_3", "conv_4", "conv_5"],
    "type": "array",
    "values": [],
    "tooltip": "Desired depth layers to compute style losses.",
    "changeable": false
  }
]

ColorTransferLib/Options/EB3.json

[
  {
    "name": "version",
    "default": "MGD",
    "type": "string",
    "values": ["IGD", "MGD"],
    "tooltip": "Either Independent Gaussian Distributions or Multivariate Gaussian Distribution.",
    "changeable": true
  }, {
    "name": "pca",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "Decides whether the normals are rotated using PCA",
    "changeable": true
  }
]

ColorTransferLib/Options/DPT.json

[
  {
    "name": "content_image_path",
    "default": "data/images/2020_Lee_Example-18_Source.png",
    "type": "string",
    "values": [],
    "tooltip": "Path to the content image.",
    "changeable": false
  },
  {
    "name": "style_image_path",
    "default": "data/images/2020_Lee_Example-18_Reference.png",
    "type": "string",
    "values": [],
    "tooltip": "Path to the style image.",
    "changeable": false
  },
  {
    "name": "content_seg_path",
    "default": "data/images/2020_Lee_Example-18_Source_seg.png",
    "type": "string",
    "values": [],
    "tooltip": "Path to the style segmentation.",
    "changeable": false
  },
  {
    "name": "style_seg_path",
    "default": "data/images/2020_Lee_Example-18_Reference_seg.png",
    "type": "string",
    "values": [],
    "tooltip": "Path to the style segmentation.",
    "changeable": false
  },
  {
    "name": "init_image_path",
    "default": "",
    "type": "string",
    "values": [],
    "tooltip": "Path to the init image.",
    "changeable": false
  },
  {
    "name": "output_image",
    "default": "best_stylized.png",
    "type": "string",
    "values": [],
    "tooltip": "Path to output the stylized image.",
    "changeable": false
  },
  {
    "name": "serial",
    "default": "./",
    "type": "string",
    "values": [],
    "tooltip": "Path to save the serial out_iter_X.png",
    "changeable": false
  },
  {
    "name": "vgg19_path",
    "default": "Models/DPT/vgg19.npy",
    "type": "string",
    "values": [],
    "tooltip": "Path to save the serial out_iter_X.png",
    "changeable": false
  },
  {
    "name": "max_iter",
    "default": 100,
    "type": "int",
    "values": [],
    "tooltip": "Training Optimizer Options: Maximum image iteration.",
    "changeable": true
  },
  {
    "name": "learning_rate",
    "default": 1.0,
    "type": "float",
    "values": [],
    "tooltip": "Training Optimizer Options: Learning rate for adam optimizer.",
    "changeable": true
  },
  {
    "name": "print_iter",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "Training Optimizer Options: Print loss per iterations.",
    "changeable": false
  },
  {
    "name": "save_iter",
    "default": 100,
    "type": "int",
    "values": [],
    "tooltip": "Save temporary result per iterations.",
    "changeable": false
  },
  {
    "name": "lbfgs",
    "default": true,
    "type": "int",
    "values": [true, false],
    "tooltip": "True=lbfgs, False=Adam",
    "changeable": true
  },
  {
    "name": "content_weight",
    "default": 5e0,
    "type": "float",
    "values": [],
    "tooltip": "Weight of content loss.",
    "changeable": true
  },
  {
    "name": "style_weight",
    "default": 1e2,
    "type": "float",
    "values": [],
    "tooltip": "Weight of style loss.",
    "changeable": true
  },
  {
    "name": "tv_weight",
    "default": 1e-3,
    "type": "float",
    "values": [],
    "tooltip": "Weight of total variational loss.",
    "changeable": true
  },
  {
    "name": "affine_weight",
    "default": 1e4,
    "type": "float",
    "values": [],
    "tooltip": "Weight of affine loss.",
    "changeable": true
  },
  {
    "name": "style_option",
    "default": 0,
    "type": "int",
    "values": [0, 1, 2],
    "tooltip": "0=non-Matting, 1=only Matting, 2=first non-Matting, then Matting",
    "changeable": true
  },
  {
    "name": "apply_smooth",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "If apply local affine smooth.",
    "changeable": true
  },
  {
    "name": "f_radius",
    "default": 15,
    "type": "int",
    "values": [],
    "tooltip": "Sooth argument.",
    "changeable": true
  },
  {
    "name": "f_edge",
    "default": 1e-1,
    "type": "float",
    "values": [],
    "tooltip": "Sooth argument.",
    "changeable": true
  }
]

ColorTransferLib/Options/GPC.json

[
  {
    "name": "colorspace",
    "default": "lalphabeta",
    "type": "string",
    "values": ["rgb", "lalphabeta"],
    "tooltip": "Decides in which color space the algorithm is applied.",
    "changeable": true
  }
]

ColorTransferLib/Options/GLO.json

[
  {
    "name": "colorspace",
    "default": "rgb",
    "type": "string",
    "values": ["rgb", "lalphabeta"],
    "tooltip": "Decides in which color space the algorithm is applied.",
    "changeable": true
  }
]

ColorTransferLib/Options/HIS.json

[
  {
    "name": "dataroot",
    "default": "test",
    "type": "string",
    "values": ["test"],
    "tooltip": "Path to images (should have subfolders trainA, trainB, valA, valB, etc)",
    "changeable": false
  },
  {
    "name": "batchSize",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "Input batch size.",
    "changeable": true
  },
  {
    "name": "loadSize",
    "default": 512,
    "type": "int",
    "values": [],
    "tooltip": "Scale images to this size.",
    "changeable": true
  },
  {
    "name": "fineSize",
    "default": 512,
    "type": "int",
    "values": [],
    "tooltip": "Then crop to this size.",
    "changeable": true
  },
  {
    "name": "ngf",
    "default": 64,
    "type": "int",
    "values": [],
    "tooltip": "Num. of gen filters in first conv layer.",
    "changeable": true
  },
  {
    "name": "ndf",
    "default": 64,
    "type": "int",
    "values": [],
    "tooltip": "Num. of discrim filters in first conv layer.",
    "changeable": true
  },
  {
    "name": "which_model_netD",
    "default": "basic",
    "type": "string",
    "values": ["basic"],
    "tooltip": "Selects model to use for netD.",
    "changeable": true
  },
  {
    "name": "which_model_netG",
    "default": "resnet_9blocks",
    "type": "string",
    "values": ["resnet_9blocks"],
    "tooltip": "Selects model to use for netG.",
    "changeable": true
  },
  {
    "name": "n_layers_D",
    "default": 3,
    "type": "int",
    "values": [],
    "tooltip": "Only used if which_model_netD==n_layers.",
    "changeable": true
  },
  {
    "name": "gpu_ids",
    "default": [0],
    "type": "array",
    "values": [],
    "tooltip": "Gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU.",
    "changeable": false
  },
  {
    "name": "name",
    "default": "experiment_name",
    "type": "string",
    "values": ["experiment_name"],
    "tooltip": "Name of the experiment. It decides where to store samples and models.",
    "changeable": false
  },
  {
    "name": "model",
    "default": "cycle_gan",
    "type": "string",
    "values": ["cycle_gan","pix2pix","test"],
    "tooltip": "Chooses which model to use. cycle_gan, pix2pix, test.",
    "changeable": true
  },
  {
    "name": "which_direction",
    "default": "AtoB",
    "type": "string",
    "values": ["AtoB","BtoA"],
    "tooltip": "AtoB or BtoA.",
    "changeable": false
  },
  {
    "name": "nThreads",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "Num. threads for loading data.",
    "changeable": true
  },
  {
    "name": "checkpoints_dir",
    "default": "checkpoints",
    "type": "string",
    "values": [],
    "tooltip": "Models are saved here.",
    "changeable": true
  },
  {
    "name": "network",
    "default": "iccv_submitted",
    "type": "string",
    "values": ["iccv_submitted"],
    "tooltip": "iccv_submitted.",
    "changeable": false
  },
  {
    "name": "network_H",
    "default": "basic",
    "type": "string",
    "values": ["basic"],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "norm",
    "default": "instance",
    "type": "string",
    "values": ["instance"],
    "tooltip": "Instance normalization or batch normalization",
    "changeable": true
  },
  {
    "name": "serial_batches",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "If true, takes images in order to make batches, otherwise takes them randomly",
    "changeable": true
  },
  {
    "name": "display_winsize",
    "default": 512,
    "type": "int",
    "values": [],
    "tooltip": "Display window size",
    "changeable": false
  },
  {
    "name": "display_id",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "Window id of the web display.",
    "changeable": false
  },
  {
    "name": "display_env",
    "default": "main",
    "type": "string",
    "values": [],
    "tooltip": "Environment name of the web display.",
    "changeable": false
  },
  {
    "name": "display_port",
    "default": 6005,
    "type": "int",
    "values": [],
    "tooltip": "Visdom port of the web display.",
    "changeable": false
  },
  {
    "name": "no_dropout",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "No dropout for the generator.",
    "changeable": true
  },
  {
    "name": "max_dataset_size",
    "default": 	3.402823466e38,
    "type": "float",
    "values": [],
    "tooltip": "Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.",
    "changeable": true
  },
  {
    "name": "resize_or_crop",
    "default": 	"resize_and_crop",
    "type": "string",
    "values": ["resize_and_crop", "crop", "scale_width", "scale_width_and_crop"],
    "tooltip": "Scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]",
    "changeable": true
  },
  {
    "name": "no_flip",
    "default": 	true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "If specified, do not flip the images for data augmentation",
    "changeable": true
  },
  {
    "name": "init_type",
    "default": 	"normal",
    "type": "string",
    "values": ["normal", "xavier", "kaiming", "orthogonal"],
    "tooltip": "Network initialization [normal|xavier|kaiming|orthogonal]",
    "changeable": true
  },
  {
    "name": "img_type",
    "default": 	"lab",
    "type": "string",
    "values": ["lab"],
    "tooltip": "Environment name of the web display.",
    "changeable": false
  },
  {
    "name": "pair_ratio",
    "default":	0.0,
    "type": "float",
    "values": [],
    "tooltip": "Ratio of Pair data.",
    "changeable": false
  },
  {
    "name": "mode",
    "default":	"gsgt",
    "type": "string",
    "values": ["gsgt", "gsrt", "rsrt"],
    "tooltip": "...",
    "changeable": true
  },
  {
    "name": "test_dir",
    "default":	"1",
    "type": "string",
    "values": ["1", "2", "3", "4", "5"],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "is_psnr",
    "default":	false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": true
  },
  {
    "name": "is_SR",
    "default":	false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": true
  },
  {
    "name": "ntest",
    "default":	3.402823466e38,
    "type": "float",
    "values": [],
    "tooltip": "Num of test examples.",
    "changeable": false
  },
  {
    "name": "results_dir",
    "default":	"results",
    "type": "string",
    "values": ["results"],
    "tooltip": "Saves results here.",
    "changeable": false
  },
  {
    "name": "aspect_ratio",
    "default":	1.0,
    "type": "float",
    "values": [],
    "tooltip": "Aspect ratio of result images.",
    "changeable": true
  },
  {
    "name": "phase",
    "default":	"test",
    "type": "string",
    "values": ["test"],
    "tooltip": "Train, val, test, etc.",
    "changeable": false
  },
  {
    "name": "which_epoch",
    "default":	"latest",
    "type": "string",
    "values": ["latest"],
    "tooltip": "Which epoch to load? set to latest to use latest cached model.",
    "changeable": true
  },
  {
    "name": "how_many",
    "default":	600,
    "type": "int",
    "values": [],
    "tooltip": "How many test images to run.",
    "changeable": true
  },
  {
    "name": "video_folder",
    "default":	"bear",
    "type": "string",
    "values": ["bear"],
    "tooltip": "folder name ...",
    "changeable": false
  },
  {
    "name": "ab_bin",
    "default":	64,
    "type": "int",
    "values": [],
    "tooltip": "ab_bin",
    "changeable": true
  },
  {
    "name": "l_bin",
    "default": 8,
    "type": "int",
    "values": [],
    "tooltip": "l_bin",
    "changeable": true
  },
  {
    "name": "isTrain",
    "default":	false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "isTrain",
    "changeable": false
  }
]

ColorTransferLib/Options/CCS.json

[
  {
    "name": "colorspace",
    "default": "lalphabeta",
    "type": "string",
    "values": ["rgb", "lalphabeta"],
    "tooltip": "Decides in which color space the algorithm is applied.",
    "changeable": true
  }
]

ColorTransferLib/Options/MKL.json

[
  {
    "name": "colorspace",
    "default": "lalphabeta",
    "type": "string",
    "values": ["rgb", "lalphabeta"],
    "tooltip": "Decides in which color space the algorithm is applied.",
    "changeable": true
  }
]

ColorTransferLib/Options/PDF.json

[
  {
    "name": "iterations",
    "default": 20,
    "type": "int",
    "values": [],
    "tooltip": "Number of iterations the algorithm is applied.",
    "changeable": true
  }
]

ColorTransferLib/Options/NST.json

[
  {
      "name": "verbose",
      "default": true,
      "type": "bool",
      "values": [true, false],
      "tooltip": "Boolean flag indicating if statements should be printed to the console.",
      "changeable": false
  },
  {
      "name": "img_name",
      "default": "result",
      "type": "string",
      "values": [],
      "tooltip": "Filename of the output image.",
      "changeable": false
  },
  {
      "name": "style_imgs",
      "default": "reference.jpg",
      "type": "string",
      "values": [],
      "tooltip": "Filenames of the style images.",
      "changeable": false
  },
  {
      "name": "style_imgs_weights",
      "default": [1.0],
      "type": "array",
      "values": [],
      "tooltip": "Interpolation weights of each of the style images.",
      "changeable": false
  },
  {
      "name": "content_img",
      "default": "source.jpg",
      "type": "string",
      "values": [],
      "tooltip": "Filename of the content image.",
      "changeable": false
  },
  {
      "name": "style_imgs_dir",
      "default": "./styles",
      "type": "string",
      "values": [],
      "tooltip": "Directory path to the style images.",
      "changeable": false
  },
  {
      "name": "content_img_dir",
      "default": "./image_input",
      "type": "string",
      "values": [],
      "tooltip": "Directory path to the content image.",
      "changeable": false
  },
  {
      "name": "init_img_type",
      "default": "content",
      "type": "string",
      "values": ["random", "content", "style"],
      "tooltip": "Image used to initialize the network.",
      "changeable": true
  },
  {
      "name": "max_size",
      "default": 2048,
      "type": "int",
      "values": [],
      "tooltip": "Maximum width or height of the input images.",
      "changeable": true
  },
  {
      "name": "content_weight",
      "default": 5e0,
      "type": "float",
      "values": [],
      "tooltip": "Weight for the content loss function. Default: 5e0",
      "changeable": true
  },
  {
      "name": "style_weight",
      "default": 1e4,
      "type": "float",
      "values": [],
      "tooltip": "Weight for the style loss function. Default: 1e4",
      "changeable": true
  },
  {
      "name": "tv_weight",
      "default": 1e-3,
      "type": "float",
      "values": [],
      "tooltip": "Weight for the total variational loss function. Set small (e.g. 1e-3).",
      "changeable": true
  },
  {
      "name": "temporal_weight",
      "default": 2e2,
      "type": "float",
      "values": [],
      "tooltip": "Weight for the temporal loss function.",
      "changeable": true
  },
  {
      "name": "content_loss_function",
      "default": 1,
      "type": "int",
      "values": [1, 2, 3],
      "tooltip": "Different constants for the content layer loss function.",
      "changeable": true
  },
  {
      "name": "content_layers",
      "default": ["conv4_2"],
      "type": "array",
      "values": [],
      "tooltip": "VGG19 layers used for the content image.",
      "changeable": false
  },
  {
      "name": "style_layers",
      "default": ["relu1_1", "relu2_1", "relu3_1", "relu4_1", "relu5_1"],
      "type": "array",
      "values": [],
      "tooltip": "VGG19 layers used for the style image.",
      "changeable": false
  },
  {
      "name": "content_layer_weights",
      "default": [1.0],
      "type": "array",
      "values": [],
      "tooltip": "Contributions (weights) of each content layer to loss.",
      "changeable": false
  },
  {
      "name": "style_layer_weights",
      "default": [0.2, 0.2, 0.2, 0.2, 0.2],
      "type": "array",
      "values": [],
      "tooltip": "Contributions (weights) of each style layer to loss.",
      "changeable": false
  },
  {
      "name": "original_colors",
      "default": false,
      "type": "bool",
      "values": [true, false],
      "tooltip": "Transfer the style but not the colors.",
      "changeable": true
  },
  {
      "name": "color_convert_type",
      "default": "yuv",
      "type": "string",
      "values": ["yuv", "ycrcb", "luv", "lab"],
      "tooltip": "Color space for conversion to original colors.",
      "changeable": true
  },
  {
      "name": "color_convert_time",
      "default": "after",
      "type": "string",
      "values": ["after", "before"],
      "tooltip": "Time (before or after) to convert to original colors.",
      "changeable": true
  },
  {
      "name": "style_mask",
      "default": false,
      "type": "bool",
      "values": [true, false],
      "tooltip": "Transfer the style to masked regions.",
      "changeable": false
  },
  {
      "name": "style_mask_imgs",
      "default": "",
      "type": "string",
      "values": [],
      "tooltip": "Filenames of the style mask images.",
      "changeable": false
  },
  {
      "name": "noise_ratio",
      "default": 1.0,
      "type": "float",
      "values": [],
      "tooltip": "Interpolation value between the content image and noise image if the network is initialized with random.",
      "changeable": true
  },
  {
      "name": "seed",
      "default": 0,
      "type": "int",
      "values": [],
      "tooltip": "Seed for the random number generator.",
      "changeable": true
  },
  {
      "name": "model_weights",
      "default": "Models/NST/imagenet-vgg-verydeep-19.mat",
      "type": "string",
      "values": [],
      "tooltip": "Weights and biases of the VGG-19 network.",
      "changeable": false
  },
  {
      "name": "pooling_type",
      "default": "avg",
      "type": "string",
      "values": ["avg", "max"],
      "tooltip": "Type of pooling in convolutional neural network.",
      "changeable": true
  },
  {
      "name": "device",
      "default": "/gpu:0",
      "type": "string",
      "values": ["/gpu:0", "/cpu:0"],
      "tooltip": "GPU or CPU mode.  GPU mode requires NVIDIA CUDA.",
      "changeable": true
  },
  {
      "name": "img_output_dir",
      "default": "./image_output",
      "type": "string",
      "values": [],
      "tooltip": "Relative or absolute directory path to output image and data.",
      "changeable": false
  },
  {
      "name": "optimizer",
      "default": "adam",
      "type": "string",
      "values": ["adam"],
      "tooltip": "Loss minimization optimizer. L-BFGS gives better results. Adam uses less memory. Info: lbfgs is not working on tensorflow2",
      "changeable": false
  },
  {
      "name": "learning_rate",
      "default": 1e0,
      "type": "float",
      "values": [],
      "tooltip": "Learning rate parameter for the Adam optimizer.",
      "changeable": true
  },
  {
      "name": "max_iterations",
      "default": 100,
      "type": "int",
      "values": [],
      "tooltip": "Max number of iterations for the Adam or L-BFGS optimizer.",
      "changeable": true
  },
  {
      "name": "print_iterations",
      "default": 100,
      "type": "int",
      "values": [],
      "tooltip": "Number of iterations between optimizer print statements.",
      "changeable": true
  }
]

ColorTransferLib/Options/PSN.json

[
  {
    "name": "iterations",
    "default": 100,
    "type": "int",
    "values": [],
    "tooltip": "Number of iterations for style transfer.",
    "changeable": true
  },
  {
    "name": "geotransfer",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "Decides if the geometry is always transferred.",
    "changeable": true
  }
]

ColorTransferLib/Utils/BaseOptions.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
# ----------------------------------------------------------------------------------------------------------------------
class BaseOptions:
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    # CONSTRUCTOR
    # ------------------------------------------------------------------------------------------------------------------
    # ------------------------------------------------------------------------------------------------------------------
    def __init__(self, options=[]):
        self.__options = options
        self.set_options(options)
        #self.__options = self.__init_default_options()
        #self.set_options(self.__options)

        #if len(options) > 0:
        #    self.set_options(options)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def get_keys(self):
        keys = []
        for op in self.__options:
            keys.append(op["name"])
        return keys

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def set_option(self, option, value):
        setattr(self, option, value)

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def set_options(self, options):
        for op in options:
            setattr(self, op["name"], op["default"])

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    def get_options(self):
        return self.__options

    # ------------------------------------------------------------------------------------------------------------------
    #
    # ------------------------------------------------------------------------------------------------------------------
    #def __init_default_options(self):
    #    options = []
    #    return options

ColorTransferLib/Options/TPS.json

[
  {
    "name": "cluster_num",
    "default": 50,
    "type": "int",
    "values": [],
    "tooltip": "Number of cluster per gaussian mixture model.",
    "changeable": true
  },
  {
    "name": "cluster_method",
    "default": "KMeans",
    "type": "str",
    "values": ["KMeans", "MVQ"],
    "tooltip": "Method for calculating clusters. MVQ currently not usable.",
    "changeable": true
  },
  {
    "name": "colorspace",
    "default": "RGB",
    "type": "str",
    "values": ["RGB", "CIELab"],
    "tooltip": "Colorspace.",
    "changeable": true
  }
]

ColorTransferLib/Options/RHG.json

[
  {
    "name": "data",
    "default": "./dataset/",
    "type": "string",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "models_dir",
    "default": "./Models/RHG",
    "type": "string",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "histGAN_models_dir",
    "default": "./Models/RHG",
    "type": "string",
    "values": [],
    "tooltip": "directory of pre-trained HistoGAN model",
    "changeable": false
  },
  {
    "name": "histoGAN_model_name",
    "default": "",
    "type": "string",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "target_hist",
    "default": "",
    "type": "string",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "input_image",
    "default": "",
    "type": "string",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "face_extraction",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "name",
    "default": "Universal_rehistoGAN_v0",
    "type": "string",
    "values": ["Faces_rehistoGAN_v0", "Faces_rehistoGAN_v1", "Faces_rehistoGAN_v2", "Faces_rehistoGAN_v3", "Universal_rehistoGAN_v0", "Universal_rehistoGAN_v1", "Universal_rehistoGAN_v2"],
    "tooltip": "...",
    "changeable": true
  },
  {
    "name": "sampling",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "for testing mode, if no target histogram is given, use sampling process",
    "changeable": false
  },
  {
    "name": "target_number",
    "default": 50,
    "type": "int",
    "values": [],
    "tooltip": "number of recolored images; ignore if you specify a target histogram",
    "changeable": false
  },
  {
    "name": "new",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "load_from",
    "default": -1,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "image_size",
    "default": 256,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": true
  },
  {
    "name": "network_capacity",
    "default": 18,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": true
  },
  {
    "name": "transparent",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "batch_size",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "gradient_accumulate_every",
    "default": 8,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "num_train_steps",
    "default": 200000,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "learning_rate",
    "default": 2e-4,
    "type": "float",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "num_workers",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "save_every",
    "default": 10000,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "trunc_psi",
    "default": 0.75,
    "type": "float",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "fp16",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "fq_layers",
    "default": [],
    "type": "array",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "fq_dict_size",
    "default": 256,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "attn_layers",
    "default": [],
    "type": "array",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "gpu",
    "default": 0,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "hist_bin",
    "default": 64,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "hist_insz",
    "default": 150,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "hist_method",
    "default": "inverse-quadratic",
    "type": "string",
    "values": ["inverse-quadratic"],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "hist_resizing",
    "default": "sampling",
    "type": "string",
    "values": ["sampling"],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "hist_sigma",
    "default": 0.02,
    "type": "float",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "generate",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "alpha",
    "default": 32.0,
    "type": "float",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "beta",
    "default": 1.5,
    "type": "float",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "gamma",
    "default": 2.0,
    "type": "float",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "change_hyperparameters",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "change_hyperparameters_after",
    "default": 100000,
    "type": "int",
    "values": [],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "rec_loss",
    "default": "laplacian",
    "type": "string",
    "values": ["laplacian", "sobel"],
    "tooltip": "reconstruction loss (sobel or laplacian)",
    "changeable": false
  },
  {
    "name": "internal_hist",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "Internal histogram injection. This was an ablation on a different design; not what we did in the official ReHistoGAN",
    "changeable": false
  },
  {
    "name": "skip_conn_to_GAN",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "See Figures 4 and 6 in the paper.",
    "changeable": false
  },
  {
    "name": "fixed_gan_weights",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "To fix weights of the HistoGANs head.",
    "changeable": false
  },
  {
    "name": "load_histoGAN_weights",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "To load weights of HistoGANs head.",
    "changeable": false
  },
  {
    "name": "initialize_gan",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "variance_loss",
    "default": true,
    "type": "bool",
    "values": [true, false],
    "tooltip": "...",
    "changeable": false
  },
  {
    "name": "upsampling_output",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "TESTING PHASE: Applies guided upsampling. It is recommended if input image > 256x256.",
    "changeable": false
  },
  {
    "name": "upsampling_method",
    "default": "pyramid",
    "type": "string",
    "values": ["pyramid"],
    "tooltip": "TESTING PHASE: BGU or pyramid.",
    "changeable": false
  },
  {
    "name": "pyramid_levels",
    "default": 6,
    "type": "int",
    "values": [],
    "tooltip": "TESTING PHASE: when --upsampling_output True and --upsampling_method is pyramid, this controls the number of levels in the Laplacian pymraid.",
    "changeable": false
  },
  {
    "name": "swapping_levels",
    "default": 1,
    "type": "int",
    "values": [],
    "tooltip": "TESTING PHASE: when --upsampling_output True and --upsampling_method is pyramid, this controls the number of levels to swap.",
    "changeable": false
  },
  {
    "name": "level_blending",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "TESTING PHASE: when --upsampling_output True and --upsampling_method is pyramid, this allows to blend between pyramid levels.",
    "changeable": false
  },
  {
    "name": "post_recoloring",
    "default": false,
    "type": "bool",
    "values": [true, false],
    "tooltip": "TESTING PHASE: Applies post-recoloring to reduce artifacts. It is recommended if initial results have some color bleeding/artifacts.",
    "changeable": false
  },
  {
    "name": "results_dir",
    "default": "./results_ReHistoGAN",
    "type": "string",
    "values": [],
    "tooltip": "",
    "changeable": false
  }
]

ColorTransferLib/Utils/Helper.py

import os
import importlib

# ----------------------------------------------------------------------------------------------------------------------
#
# ----------------------------------------------------------------------------------------------------------------------
def check_compatibility(src, ref, compatibility_list):
    # return element
    output = {
        "status_code": 0,
        "response": "",
        "object": None
    }

    if src.get_type() not in compatibility_list["src"]:
        output["status_code"] = -1
        output["response"] = "No support for the following source type: " + src.get_type()

    if ref.get_type() not in compatibility_list["ref"]:
        output["status_code"] = -1
        output["response"] = "No support for the following reference type: " + ref.get_type()

    return output

# ----------------------------------------------------------------------------------------------------------------------
# read all available algorithms from the Algorithms folder and import them
# ----------------------------------------------------------------------------------------------------------------------
def get_methods():
    available_methods = os.listdir(os.path.dirname(os.path.abspath(__file__)) + "/../Algorithms")
    #toremove = ["__init__.pyc", "__init__.py", "__pycache__", "FCM", "GMM","RGH","DPT","PSN","BCC","NST","TPS","EB3","PDF","GLO","HIS","FUZ","CAM"]
    toremove = ["__init__.pyc", "__init__.py", "__pycache__", ".DS_Store", "FCM", "GMM"]
    available_methods = [m for m in available_methods if m not in toremove]
    return available_methods

# ----------------------------------------------------------------------------------------------------------------------
# read all available metrics from the Evaluation folders
# ----------------------------------------------------------------------------------------------------------------------
def get_metrics():
    available_metrics = os.listdir(os.path.dirname(os.path.abspath(__file__)) + "/../Evaluation")
    toremove = ["__init__.pyc", "__init__.py", "__pycache__", ".DS_Store"]
    available_metrics = [m for m in available_metrics if m not in toremove]
    return available_metrics

ColorTransferLib/Options/FUZ.json

[
  {
    "name": "cluster_num",
    "default": 3,
    "type": "int",
    "values": [],
    "tooltip": "Number of clusters",
    "changeable": true
  },
  {
    "name": "fuzzier",
    "default": 2.0,
    "type": "float",
    "values": [],
    "tooltip": "Controls how fuzzy the cluster boundary should be",
    "changeable": true
  },
  {
    "name": "max_iterations",
    "default": 100,
    "type": "int",
    "values": [],
    "tooltip": "Maximum number of iterations for updating clusters.",
    "changeable": true
  },
  {
    "name": "error",
    "default": 1e-04,
    "type": "float",
    "values": [],
    "tooltip": "Terminations error.",
    "changeable": true
  }
]

ColorTransferLib/Utils/init.py

from .Math import *
from .Helper import *
from .BaseOptions import *

ColorTransferLib/init.py

from .ColorTransfer import ColorTransfer, ColorTransferEvaluation

from . import Algorithms
from . import ImageProcessing
from . import MeshProcessing
from . import Utils
from . import Evaluation

ColorTransferLib/Utils/Math.py

"""
Copyright 2022 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

import numpy as np
import math
from numba import cuda
import random

# ------------------------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------------------------
# DEVICE METHODS
# ------------------------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------------------------
@cuda.jit(device=True)
def device_mul_mat3x3_vec3(mat3x3, vec3):
    m00 = mat3x3[0][0] * vec3[0] + mat3x3[0][1] * vec3[1] + mat3x3[0][2] * vec3[2]
    m01 = mat3x3[1][0] * vec3[0] + mat3x3[1][1] * vec3[1] + mat3x3[1][2] * vec3[2]
    m02 = mat3x3[2][0] * vec3[0] + mat3x3[2][1] * vec3[1] + mat3x3[2][2] * vec3[2]

    return m00, m01, m02

# ----------------------------------------------------------------------------------------------------------------------
# Find the rotation matrix associated with counterclockwise rotation about the given axis by theta radians.
#
# Args:
#    axis (list): rotation axis of the form [x, y, z]
#    theta (float): rotational angle in radians
#
# Returns:
#    array. Rotation matrix.
#
# Source:
# - https://www.andre-gaschler.com/rotationconverter/
# - https://en.wikipedia.org/wiki/Rotation_matrix
# ----------------------------------------------------------------------------------------------------------------------
def get_3x3rotation_matrix(axis, theta):
    axis = np.asarray(axis)
    # transform <axis> to unit vector
    axis = axis/math.sqrt(np.dot(axis, axis))

    t = 1.0 - math.cos(theta)
    c = math.cos(theta)
    s = math.sin(theta)
    n1 = axis[0]
    n2 = axis[1]
    n3 = axis[2]

    return np.array([[n1*n1*t+c,    n1*n2*t-n3*s, n1*n3*t+n2*s],
                     [n2*n1*t+n3*s, n2*n2*t+c,    n2*n3*t-n1*s],
                     [n3*n1*t-n2*s, n3*n2*t+n1*s, n3*n3*t+c]])

# ----------------------------------------------------------------------------------------------------------------------
# Creates a random 3x3 rotation matrix based on get_3x3rotation_matrix(axis, theta)
# ----------------------------------------------------------------------------------------------------------------------
def get_random_3x3rotation_matrix():
    x = random.uniform(-1.0, 1.0)
    y = random.uniform(-1.0, 1.0)
    z = random.uniform(-1.0, 1.0)
    # check if the the vector is not a zero vector
    if x == 0.0 and y == 0.0 and z == 0.0:
        raise ValueError("Can't create rotation matrix with zero vector.")
    random_vec = [x, y, z]
    random_angle = random.uniform(0.0, 2*math.pi)
    return get_3x3rotation_matrix(random_vec, random_angle)

README.md

ColorTransferLib

The ColorTransferLib is a library focused on color transfer, featuring a range of published algorithms. Some algorithms have been re-implemented, while others are integrated from public repositories. The primary objective of this project is to compile all existing color and style transfer methods into one library with a standardized API. This aids the research community in both development and comparison of algorithms. Currently, the library supports 15 color and style transfer methods for images (PNG-Format), 3D point clouds (PLY-Format), and textured triangle meshes (OBJ-Format with corresponding MTL and PNG). Additionally, it includes 20 metrics for evaluating color transfer results. A detailed list of all algorithms is available below.

API

For seamless integration, adhere to the API specifications of the new color transfer algorithm, depicted in the Figure below.

Each class demands three inputs: Source, Reference, and Options. The Source and Reference should be of the Image or Mesh class type, with the latter encompassing 3D point clouds and textured triangle meshes. The Options input consists of dictionaries, stored as a JSON file in the Options folder. For a sample option, see Listings 1. Every option details an adjustable parameter for the algorithm.

Save each new color transfer class in the ColorTransferLib Repository under the Algorithms folder. This ensures its automatic integration into the user interface. The class should have two essential functions: get_info() and apply(...). The get_info() function yields vital details about the algorithm (refer to Listing 2). It also provides data type details, facilitating the identification of compatible objects for the algorithm. The apply(...) function ingests the inputs and embodies the core logic for color transfer.

The output should resemble a dictionary format, as outlined in Listing 3. A status code of 0 signifies a valid algorithm output, while -1 indicates invalidity. The process time denotes the algorithm's execution duration, useful for subsequent evaluations. The 'object' key in the dictionary holds the result, which should match the class type of the Source input.

Installation
Requirements

(1) Download the Models-Folder from here and place it in the project's root directory.

(2) Install the following packages:

sudo apt-get install liboctave-dev
# allows writing of mp4 with h246 codec
sudo apt-get install ffmpeg

(3) Install the following octave package:

# activate octave environment
octave
# install packages
octave:1> pkg install -forge image
octave:2> pkg install -forge statistics

(4) Run the gbvs_install.m to make the evaluation metric VSI runnable:

user@pc:~/<Project Path>/ColorTransferLib/Evaluation/VIS/gbvs$ ocatve
octave:1> gbvs_install.m

Install via PyPI
pip install colortransferlib
# manual installation to allow h246 codec
pip install opencv-python==4.9.0.80 --no-binary opencv-python --force-reinstall

Install from source
pip install -r requirements/requirements.txt
python setup.py bdist_wheel
pip install --force-reinstall ../ColorTransferLib/dist/ColorTransferLib-0.0.4-py3-none-any.whl 
# manual installation to allow h246 codec
pip install opencv-python==4.9.0.80 --no-binary opencv-python 

Usage
from ColorTransferLib.ColorTransfer import ColorTransfer
from ColorTransferLib.ImageProcessing.Image import Image

src = Image(file_path='/media/source.png')
ref = Image(file_path='/media/reference.png') 

algo = "GLO"
ct = ColorTransfer(src, ref, algo)
out = ct.apply()

# No output file extension has to be given
if out["status_code"] == 0:
    out["object"].write("/media/out")
else:
    print("Error: " + out["response"])

Available Color Transfer Methods:

The following color and style transfer methods are integrated in the library. Some of them are reimplemented based on the algorithm's description in the the published papers and others are adopted from existing repositories and adpated to fit the API. The original implementation of the latter methods can be found next to the publication's name. The superscripts (2D, 3D) indicated wether the algorithm is applicable to 2D structures like images and mesh textures or to 3D structures like point clouds. The subscript (CT, ST) describs wether the algorithm is a color or a style transfer algorithm.

Year	ID	Publication
2001	$GLO^{2D,3D}_{CT}$	Color Transfer between Images
2003	$BCC^{2D,3D}_{CT}$	A Framework for Transfer Colors Based on the Basic Color Categories
2005	$PDF^{2D,3D}_{CT}$	N-dimensional probability density function transfer and its application to color transfer
2006	$CCS^{2D,3D}_{CT}$	Color transfer in correlated color space
2007	$MKL^{2D,3D}_{CT}$	The Linear Monge-Kantorovitch Linear Colour Mapping for Example-Based Colour Transfer
2009	$GPC^{2D}_{CT}$	Gradient-Preserving Color Transfer
2010	$FUZ^{2D,3D}_{CT}$	An efficient fuzzy clustering-based color transfer method
2015	$NST^{2D}_{ST}$	A Neural Algorithm of Artistic Style - Original Implementation
2017	$DPT^{2D}_{ST}$	Deep Photo Style Transfer - Original Implementation
2019	$TPS^{2D}_{CT}$	L2 Divergence for robust colour transfer - Original Implementation
2020	$HIS^{2D}_{CT}$	Deep Color Transfer using Histogram Analogy - Original Implementation
2020	$PSN^{3D}_{ST}$	PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color - Original Implementation
2020	$EB3^{3D}_{CT}$	Example-Based Colour Transfer for 3D Point Clouds
2021	$CAM^{2D}_{ST}$	CAMS: Color-Aware Multi-Style Transfer - Original Implementation
2021	$RHG^{2D}_{CT}$	HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms
Available Objective Evaluation Metrics

Three classes of evaluation metrics are considered here. Metrics that evaluate the color consistency with the reference image (indicated with $^r$), metrics that evaluate the structural similarity with the source image (indicated with $^s$) and metrics that evaluates the overall quality of the output (indicated with $^o$).

Year	ID	Name	Publication
/	$PSNR^s_{rgb}$	Peak Signal-to-Noise Ratio	/
/	$HI^r_{rgb}$	Histogram Intersection	/
/	$Corr^r_{rgb}$	Correlation	/
/	$BA^r_{rgb}$	Bhattacharyya Distance	/
/	$MSE^s_{rgb}$	Mean-Squared Error	/
/	$RMSE^s_{rgb}$	Root-Mean-Squared Error	/
2003	$CF^o_{rgyb}$	Colorfulness	Measuring Colourfulness in Natural Images
2003	$MSSSIM^s_{rgb}$	Multi-Scale Structural Similarity Index	Multiscale structural similarity for image quality assessment
2004	$SSIM^s_{rgb}$	Structural Similarity Index	Image quality assessment: from error visibility to structural similarity
2006	$GSSIM^s_{rgb}$	Gradient-based Structural Similarity Index	Gradient-Based Structural Similarity for Image Quality Assessment
2010	$IVSSIM^s_{rgb}$	4-component Structural Similarity Index	Content-partitioned structural similarity index for image quality assessment
2011	$IVEGSSIM^s_{rgb}$	4-component enhanced Gradient-based Structural Similarity Index	An image similarity measure using enhanced human visual system characteristics
2011	$FSIM^s_{c,yiq}$	Feature Similarity Index	FSIM: A Feature Similarity Index for Image Quality Assessment
2012	$BRISQUE^o_{rgb}$	Blind/Referenceless Image Spatial Quality Evaluator	No-Reference Image Quality Assessment in the Spatial Domain
2013	$NIQE^o_{rgb}$	Naturalness Image Quality Evaluator	Making a â€œCompletely Blindâ€ Image Quality Analyzer
2014	$VSI^s_{rgb}$	Visual Saliency-based Index	VSI: A Visual Saliency-Induced Index for Perceptual Image Quality Assessment
2016	$CTQM^{sro}_{lab}$	Color Transfer Quality Metric	Novel multi-color transfer algorithms and quality measure
2018	$LPIPS^s_{rgb}$	Learned Perceptual Image Patch Similarity	The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
2018	$NIMA^o_{rgb}$	Neural Image Assessment	NIMA: Neural Image Assessment
2019	$CSS^{sr}_{rgb}$	Color and Structure Similarity	Selective color transfer with multi-source images
Citation

If you utilize this code in your research, kindly provide a citation:

@inproceeding{potechius2023,
  title={A software test bed for sharing and evaluating color transfer algorithms for images and 3D objects},
  author={Herbert Potechius, Thomas Sikora, Gunasekaran Raja, Sebastian Knorr},
  year={2023},
  booktitle={European Conference on Visual Media Production (CVMP)},
  doi={10.1145/3626495.3626509}
}

main.py

"""
Copyright 2023 by Herbert Potechius,
Ernst-Abbe-Hochschule Jena - University of Applied Sciences - Department of Electrical Engineering and Information
Technology - Immersive Media and AR/VR Research Group.
All rights reserved.
This file is released under the "MIT License Agreement".
Please see the LICENSE file that should have been included as part of this package.
"""

from ColorTransferLib.MeshProcessing.Mesh import Mesh
from ColorTransferLib.MeshProcessing.VolumetricVideo import VolumetricVideo
from ColorTransferLib.ImageProcessing.Image import Image
from ColorTransferLib.ImageProcessing.Video import Video
from ColorTransferLib.ColorTransfer import ColorTransfer, ColorTransferEvaluation

import cv2

# ------------------------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------------------------
#
# ------------------------------------------------------------------------------------------------------------------
# ------------------------------------------------------------------------------------------------------------------
if __name__ == '__main__':  

    #src = Video(file_path='/media/potechius/External/data/Videos/sample-5s.mp4')

    src = VolumetricVideo(folder_path='/media/potechius/External/data/VolumetricVideos/$volumetric$Human', file_name='Human')

    #src.write("/media/potechius/External/data/VolumetricVideos/out")
    #exit()

    #src.write("/media/potechius/External/data/Videos/out.mp4")

    #frames = src.get_raw()
    # for i, frame in enumerate(frames):
    #     print(frame.dtype)
    #     cv2.imshow(f'Frame {i}', frame)
    #     # Wait for 'interval' seconds
    #     if cv2.waitKey(int(1000)) & 0xFF == ord('q'):
    #         break

    # cv2.destroyAllWindows()

    #exit()

    # 2D Color/Style Transfer Example
    #src = Image(file_path='/media/potechius/External/data/Images/Wanderer_above_the_Sea_of_Fog.png')
    ref = Image(file_path='/media/potechius/External/data/Images/The_Scream.png')  
    #out = Image(file_path='/media/potechius/External/data/Images/out.png')  

    #cte = ColorTransferEvaluation(src, ref, out)
    #eva = cte.apply("VSI")
    #print(eva)

    #exit()

    #src = Mesh(file_path='/home/potechius/Downloads/3D/src.ply', datatype="PointCloud")
    #ref = Mesh(file_path='/home/potechius/Downloads/3D/ref.ply', datatype="PointCloud")  

    #src = Mesh(file_path='/home/potechius/Downloads/3D_mesh/Apple.obj', datatype="Mesh")
    #ref = Mesh(file_path='/home/potechius/Downloads/3D_mesh/Pillow.obj', datatype="Mesh")  

    algo = "GLO"
    ct = ColorTransfer(src, ref, algo)
    out = ct.apply()

    if out["status_code"] == 0:
        out["object"].write("/media/potechius/External/data/VolumetricVideos/$volumetric$Test")
    else:
        print("Error: " + out["response"])
    print("Done")

    # Evaluation Example
    # TODO

init.py

requirements/requirements.txt

brisque==0.0.15
cmake==3.27.7
Cython==3.0.5
dlib==19.24.2
faiss-gpu==1.7.2
fuzzy-c-means==1.7.0
gdown==4.7.1
h5py==3.10.0
imblearn==0.0
linear_attention_transformer==0.19.1
libsvm==3.23.0.4
lpips==0.1.4
matplotlib==3.8.1
numpy==1.26.4
numba==0.58.1
oct2py==5.6.0
open3d==0.17.0
PilloW==10.0.1
phasepack==1.5
pyamg==5.0.1
pyfftw==0.13.1
pyhull==2015.2.1
pyiqa==0.1.8
pyntcloud==0.3.1
pysaliency==0.2.21
retry==0.9.2
scikit-image==0.22.0
scikit-learn==1.3.2
scipy==1.11.3
seaborn==0.13.0
setuptools==65.5.1
tensorflow==2.16.1
tensorflow-probability==0.22.1
tf_slim==1.1.0
torch==2.1.0
torchfile==0.1.0
torchmetrics==1.2.0
torchvision==0.16.0
torch-optimizer==0.3.0
tqdm==4.66.1
typing-extensions==4.7
vector-quantize-pytorch==1.10.4
zipfile36==0.1.3

setup.py

import setuptools

from pathlib import Path
this_directory = Path(__file__).parent
long_description = (this_directory / "README.md").read_text()

# Reading the content of the requirements.txt
with open('requirements/requirements.txt') as f:
    requirements = f.read().splitlines()

setuptools.setup(
    name="ColorTransferLib",
    version="1.0.0-4",
    author="Herbert Potechius",
    author_email="potechius.herbert@gmail.com",
    description="This library provides color and tyle transfer algorithms which were published in scientific papers. Additionall a set of IQA metrics are available.",
    packages=setuptools.find_packages(),
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: POSIX :: Linux",
    ],
    package_data={"": ['Options/*.json',
                       'Config/*.json',
                       'Algorithms/TPS/L2RegistrationForCT/*',
                       'Algorithms/TPS/L2RegistrationForCT/*/*',
                       'Algorithms/TPS/L2RegistrationForCT/*/*/*', 
                       'Evaluation/VIS/gbvs/*', 
                       'Evaluation/VIS/gbvs/*/*', 
                       'Evaluation/VIS/gbvs/*/*/*']},
    include_package_data=True,
    python_requires='>=3.8',
    long_description=long_description,
    long_description_content_type='text/markdown',
    install_requires= requirements
)

# run seperately: "pip install opencv-python==4.9.0.80 --no-binary opencv-python"

requirements/constraints.txt

opencv-python --no-binary opencv-python

ColorTransferLib/Evaluation/VSI/gbvs/util/show_imgnmap2.m

function show_imgnmap2( img , smap )
smap = mat2gray( imresize(smap,[size(img,1) size(img,2)]) );
imshow(heatmap_overlay( img , smap ));

